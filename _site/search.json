[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Group_Project_G6/Proposal/Group6_project_proposal.html",
    "href": "Group_Project_G6/Proposal/Group6_project_proposal.html",
    "title": "Group 6 VA Project Proposal",
    "section": "",
    "text": "With the given choices based on the VAST Challenge 2024 and CHINAVIS 2024 Data Visualisation Competition, the selected topic for this group project is Challenge I from the latter, titled「Data Analysis Inspires Wisdom」Time-Series Multivariate Education Data Visual Analytics Challenge. This topic was chosen due to the Project Group members’ contextual familiarity with the topic, as well as its alignment to the members’ learning interests in leveraging on key visual analytics tools, to explore and design data-driven solutions for seemingly complex problems.\nThe selected project is centred on the learning patterns of learners in a programming course conducted by NorthClass Education Training Institute. Fundamentally, the project’s raison d’etre is to provide actionable insights towards the institution’s endeavor to diagnose and analyze learners’ knowledge mastery levels, monitor the trends in their learning behaviors, identify and dissect potential factors that contribute to learning difficulties, and hence to derive feasible suggestions to adjust teaching strategies and course design.\nTo this end, data was collected from a select group of learners over a specified set of programming tasks over a particular learning period to facilitate the design and implementation of a Visual Analytics solution as a key outcome in this project.\n\n\n\nFrom the Challenge, the key problem statement was to perform a comprehensive analysis of multiple datasets that describe various aspects of the learner’s profile, learning patterns and status, to derive key insights to enhance teaching strategies and course design.\nConsequently the key requirements based on the 5 stipulated tasks in the challenge were as follows.\n\nTask 1: To provide a quantitative assessment of the learners’ knowledge mastery and identify weak links in their knowledge system, based on the multi-dimensional attributes such as answer scores and answer status in the learners’ log records of the learners’ question-answering behaviors.\n\nThis would entail an analysis of the learners’ aggregate performance in their programming tasks (a.k.a. questions in the dataset), including measures of central tendency, or any notable patterns that can glean insights towards knowledge mastery and weaknesses from the given datasets\n\nTask 2: To design and present learners’ profiles, based on learners’ personalized learning behavior patterns (including peak answering hours, preferred question types, correct answering rates, etc.), and various other attributes and characteristics\n\nThis would entail an analysis and profiling of of learners’ behaviours and attributes based on statistically significant patterns that are observed across the variables in the given datasets\n\nTask 3: To analyse and provide a visual representation of the relationship between learning modes and knowledge acquisition (learners’ ability to absorb, integrate, and apply knowledge)\n\nThis would entail an analysis to uncover the various learning modes and the patterns and/or correlations that it may have with the learners’ performance in the questions from the given datasets\n\nTask 4: To analyse the difficulty level of questions and learner’s level of knowledge (which should mostly align) and also to identify ‘inappropriate questions’ that may not align to this relationship.\n\nThis would entail an analysis and comparison between the learners’ performance in each question and the learners’ performance ranking, and hence to identify any potential outliers that violate the alignment of question difficulty and learner’s level of knowledge\n\nTask 5: To offer valuable recommendations to optimize question bank content settings and enhance the quality of teaching and learning, based on the outcomes of the aforementioned analysis of Tasks 1 to 4 above.\n\nThis would entail deriving actionable recommendations based on the insights gleaned in the visual analytics of the datasets from tasks 1 to 4 above"
  },
  {
    "objectID": "Group_Project_G6/Proposal/Group6_project_proposal.html#project-overview",
    "href": "Group_Project_G6/Proposal/Group6_project_proposal.html#project-overview",
    "title": "Group 6 VA Project Proposal",
    "section": "",
    "text": "With the given choices based on the VAST Challenge 2024 and CHINAVIS 2024 Data Visualisation Competition, the selected topic for this group project is Challenge I from the latter, titled「Data Analysis Inspires Wisdom」Time-Series Multivariate Education Data Visual Analytics Challenge. This topic was chosen due to the Project Group members’ contextual familiarity with the topic, as well as its alignment to the members’ learning interests in leveraging on key visual analytics tools, to explore and design data-driven solutions for seemingly complex problems.\nThe selected project is centred on the learning patterns of learners in a programming course conducted by NorthClass Education Training Institute. Fundamentally, the project’s raison d’etre is to provide actionable insights towards the institution’s endeavor to diagnose and analyze learners’ knowledge mastery levels, monitor the trends in their learning behaviors, identify and dissect potential factors that contribute to learning difficulties, and hence to derive feasible suggestions to adjust teaching strategies and course design.\nTo this end, data was collected from a select group of learners over a specified set of programming tasks over a particular learning period to facilitate the design and implementation of a Visual Analytics solution as a key outcome in this project.\n\n\n\nFrom the Challenge, the key problem statement was to perform a comprehensive analysis of multiple datasets that describe various aspects of the learner’s profile, learning patterns and status, to derive key insights to enhance teaching strategies and course design.\nConsequently the key requirements based on the 5 stipulated tasks in the challenge were as follows.\n\nTask 1: To provide a quantitative assessment of the learners’ knowledge mastery and identify weak links in their knowledge system, based on the multi-dimensional attributes such as answer scores and answer status in the learners’ log records of the learners’ question-answering behaviors.\n\nThis would entail an analysis of the learners’ aggregate performance in their programming tasks (a.k.a. questions in the dataset), including measures of central tendency, or any notable patterns that can glean insights towards knowledge mastery and weaknesses from the given datasets\n\nTask 2: To design and present learners’ profiles, based on learners’ personalized learning behavior patterns (including peak answering hours, preferred question types, correct answering rates, etc.), and various other attributes and characteristics\n\nThis would entail an analysis and profiling of of learners’ behaviours and attributes based on statistically significant patterns that are observed across the variables in the given datasets\n\nTask 3: To analyse and provide a visual representation of the relationship between learning modes and knowledge acquisition (learners’ ability to absorb, integrate, and apply knowledge)\n\nThis would entail an analysis to uncover the various learning modes and the patterns and/or correlations that it may have with the learners’ performance in the questions from the given datasets\n\nTask 4: To analyse the difficulty level of questions and learner’s level of knowledge (which should mostly align) and also to identify ‘inappropriate questions’ that may not align to this relationship.\n\nThis would entail an analysis and comparison between the learners’ performance in each question and the learners’ performance ranking, and hence to identify any potential outliers that violate the alignment of question difficulty and learner’s level of knowledge\n\nTask 5: To offer valuable recommendations to optimize question bank content settings and enhance the quality of teaching and learning, based on the outcomes of the aforementioned analysis of Tasks 1 to 4 above.\n\nThis would entail deriving actionable recommendations based on the insights gleaned in the visual analytics of the datasets from tasks 1 to 4 above"
  },
  {
    "objectID": "Group_Project_G6/Proposal/Group6_project_proposal.html#the-dataset",
    "href": "Group_Project_G6/Proposal/Group6_project_proposal.html#the-dataset",
    "title": "Group 6 VA Project Proposal",
    "section": "The Dataset",
    "text": "The Dataset\nThe provided materials for the challenge include 3 datasets described below, as well as a separate document providing a more detailed description of the data and variables\n\nDataset 1: Student Information - This comprises of 5 Cols, 1364 Rows, providing individualised demographic variables of the learners (a.k.a students) within the scope this project\nDataset 2: Learning Subject Title Information - This comprises of 5 Cols, 44 Rows, providing variables of the questions from the programming tasks which are collated in the scope of this project\nDataset 3: Class Submission Records - This comprises of multiple datasets, each with 10 Cols and various number of rows, providing supposedly the participating learners’ answering variables to the questions collated in the scope of this project"
  },
  {
    "objectID": "Group_Project_G6/Proposal/Group6_project_proposal.html#approach",
    "href": "Group_Project_G6/Proposal/Group6_project_proposal.html#approach",
    "title": "Group 6 VA Project Proposal",
    "section": "Approach",
    "text": "Approach\nThe broad approach for the project is shown in the figure below."
  },
  {
    "objectID": "Group_Project_G6/Proposal/Group6_project_proposal.html#methodology",
    "href": "Group_Project_G6/Proposal/Group6_project_proposal.html#methodology",
    "title": "Group 6 VA Project Proposal",
    "section": "Methodology",
    "text": "Methodology\nOur methodology systematically integrates data collection, data processing, analysis, pattern mining, modeling, and recommendations to create a comprehensive Visual Analytics solution for improving teaching strategies and course designs at NorthClass Institute, elaborated below.\n\nKey Data Preparation\nData preprocessing and cleaning is required at the initial phase to ensure usability and consistency of the data records for subsequent data analysis. Based on initial assessment of the datasets, the Key Data preparation steps required are as follows.\n\nRemove duplicates, handle missing values through imputation or deletion, and correct inconsistencies.\nCorrecting data type and addressing out-liers.\nCreate new relevant attributes if needed.\nMerging of Multiple datasets in Dataset 3 into a single continuous dataframe, and subsequently joining Datasets 1 and 2 with 3.\n\n\n\nKey DataViz\nTo address the aforesaid requirements, the following methodologies will be considered for the data visualisation (DataViz).\n\nFor task 1, the analysis of answering behaviors involves identifying key performance metrics such as average scores, answer correctness, and time correlations to evaluate learners’ understanding and application of knowledge. Descriptive statistics will be used to compute mean, median, and mode of scores for different topics and question types. Histograms, violin and box plots could be used to visualize score distributions. Performance metrics could be calculated such as accuracy rate, average attempts per question, and time efficiency. These statistical analysis and visualisations will be done in tandem with the classification of questions, to finally visualise and quantify mastery and weaknesses in the various knowledge areas.\nFor task 2, learning behavior patterns involve analyzing habitual actions. Time-series analysis will be used to identify peak answering times. And by studying the frequency and performance of different question types with bar charts, the preferred question type can also be identified. To profile and segment the learner groups, clustering analysis algorithms like K-means or hierarchical clustering can be used based on features engineered from the aforementioned behavior patterns and preferences.\nFor task 3, in modeling the relationship between knowledge acquisition and learning modes, feature selection may build upon analysis from task 1 for knowledge acquisition and task 2 for learning modes. Correlation and Association analysis can then be used to ascertain the statistical significance of the relationships if any.\nFor task 4, to identify question difficulty and learner knowledge levels, questions and learners can be ranked and categorized based on the historical performance data, and thresholds can be defined to sort and bin the data into levels. Cross-comparison can thus be done to filter and analyze data to detect inappropriate questions that are too difficult based on learner performance. An alternative group of inappropriate questions can also be suggested through visualising questions that ‘weak’ learners scored better and/or questions that ‘strong’ learners scored worse.\nLast but not least for task 5, based on the insights gleaned, actionable recommendations will be derived, (such as optimizing the question bank, customizing knowledge delivery based on learning patterns and preferences) aiming to enhance the overall quality of teaching and learning.\n\nAccordingly, a summary of the envisioned DataViz for this project will be as as follows.\n\nGeneral Analysis (for Task 1):\n\nBar plot for mean, median, and mode of scores for different topics and question types.\nBar plot for classifying answers into correct and incorrect categories.\nScatter plot for time vs. Correctness\nFacet plots to visualise multiple plots in a single space\n\nScore distribution and performance metrics (for Tasks 1 & 4): \n\nHistogram for score distribution, bar plot for accuracy rate\nFacet, Ridgeline or Raindrop plots for performance metrics and to compare distribution for various categories\n\nPeaking hours analysis (for Task 2):\n\nTime line chart: Displays the number of activities at different time points in a day to help identify peak hours.\nHeat map: Displays the distribution of daily or weekly activities to help discover high-frequency activities in a specific time period.\n\nClustering result display (for Task 2 & 3):\n\nRadar chart: Displays multiple dimensional features of each cluster.\nBox plot: Displays the distribution of each cluster to understand the differences of clusters.\n\nAssociation and Correlation Analysis (for Task 3 & 4):\n\nScatter plot with best-fit line/curve and shaded area for significance level using ggscaterstats for single numerical independent variable and dependent variable\nMultiple Violin plots using ggbetweenstats for multiple categorical independent variables and numerical independent variable"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html",
    "title": "Hands-on_Ex1",
    "section": "",
    "text": "In this hands-on exercise, the basic principles and essential components of ggplot2 are visited. At the same time, hands-on experience is gained through using these components to plot statistical graphics based on the principles of Layered Grammar of Graphics."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#overview",
    "title": "Hands-on_Ex1",
    "section": "",
    "text": "In this hands-on exercise, the basic principles and essential components of ggplot2 are visited. At the same time, hands-on experience is gained through using these components to plot statistical graphics based on the principles of Layered Grammar of Graphics."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#getting-started",
    "title": "Hands-on_Ex1",
    "section": "Getting Started",
    "text": "Getting Started\n\nInstall & Load Libraries\nThe code chunk below installs and launches the tidyverse package into R environment\n\npacman:: p_load(tidyverse)\n\n\n\nImporting the data\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package, which is part of the tidyverse package.\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNote that starting the code with |# eval: false display the code chunk without running the code"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#ggplot-introduction",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#ggplot-introduction",
    "title": "Hands-on_Ex1",
    "section": "GGPlot Introduction",
    "text": "GGPlot Introduction\nGGplot2 is an R package for declaratively creating data-driven graphics based on The Grammar of Graphics\nIt is also part of the tidyverse family specially designed for visual exploration and communication.\nFor more detail, visit ggplot2 link.\n\nR graphics Vs GGPlot\nA comparison of Rgraphics against GG Plot is illustrated in the following code chunk\n\nRgraphics (Histogram)GGPlot\n\n\n\nhist(exam_data$MATHS)\n\n\n\n\n\n\n\nggplot(data=exam_data, aes(x = MATHS)) +\n  geom_histogram(bins=10, \n                 boundary = 100,\n                 color=\"black\", \n                 fill=\"grey\") +\n  ggtitle(\"Distribution of Maths scores\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#grammar-of-graphics",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#grammar-of-graphics",
    "title": "Hands-on_Ex1",
    "section": "Grammar of Graphics",
    "text": "Grammar of Graphics\nBefore getting started using ggplot2, it is important to understand the principles of Grammer of Graphics.\nGrammar of Graphics is a general scheme for data visualization which breaks up graphs into semantic components such as scales and layers. It was introduced by Leland Wilkinson in 1999 Grammar of Graphics, Springer. The grammar of graphics is an answer to a question:\nWhat is a statistical graphic?\nIn the nutshell, Grammar of Graphics defines the rules of structuring mathematical and aesthetic elements into a meaningful graph.\nThere are two principles in Grammar of Graphics, they are:\n\nGraphics = distinct layers of grammatical elements\nMeaningful plots through aesthetic mapping\n\nA good grammar of graphics will provide insight into the composition of complicated graphics, and reveal unexpected connections between seemingly different graphics (Cox 1978). It also provides a strong foundation for understanding a diverse range of graphics. Furthermore, it may provide guidance on what a well-formed or correct graphic looks like, but there could still be many grammatically correct but nonsensical graphics.\n\nLayered Grammar of Graphics\nggplot2 is an implementation of Leland Wilkinson’s Grammar of Graphics. There are 7 layers of grammar, aka Essential Grammatical Elements, described as follows.\n\nData: The dataset being plotted.\nAesthetics: take attributes of the data and use them to influence visual characteristics, such as position, colours, size, shape, or transparency.\nGeometrics: The visual elements used for our data, such as point, bar or line.\nFacets: splits the data into subsets to create multiple variations of the same graph (paneling, multiple plots).\nStatistics: statistical transformations that summarise data (e.g. mean, confidence intervals).\nCoordinate systems: define the plane on which data are mapped on the graphic.\nThemes: modify all non-data components of a plot, such as main title, sub-title, y-aixs title, or legend background.\n\n\n\nGGPlot2 Essential Grammatical Elements: Data\nWith ggplot() a blank canvas for the data visualisation can be first intialised, as shown in the following code chunk.\n\nggplot(data=exam_data)\n\n\n\n\n\n\nGGPlot2 Essential Grammatical Elements: Aesthetics mappings\nAesthetic mappings utilises attributes of the data and and use them to influence visual characteristics, such as position, colour, size, shape, or transparency. Each visual characteristic can thus encode an aspect of the data and be used to convey information.\nAll aesthetics of a plot are specified in the aes() function call (each geom layer can have its own aes specification)\nThe code chunk below illustrates the use of aes() to add a simple aesthetic element (the x axis and label) into the plot.\n\nggplot(data=exam_data, \n       aes(x= MATHS))\n\n\n\n\n\n\nGGPlot2 Essential Grammatical Elements: Geometrics\nGeometric objects are the centerpiece graphical marks placed on a plot. Examples include:\n\ngeom_point for drawing individual points (e.g., a scatter plot)\ngeom_line for drawing lines (e.g., for a line charts)\ngeom_smooth for drawing smoothed lines (e.g., for simple trends or approximations)\ngeom_bar for drawing bars (e.g., for bar charts)\ngeom_histogram for drawing binned values (e.g. a histogram)\ngeom_polygon for drawing arbitrary shapes\ngeom_map for drawing polygons in the shape of a map! (You can access the data to use for these maps by using the map_data() function).\n\n\nGeometric object: Bar charts\nThe code chunk below plots a bar chart by using geom_bar().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar()\n\n\n\n\n\n\nGeometric object: Dotplot\nIn a dot plot, the width of a dot corresponds to the bin width (or maximum width, depending on the binning algorithm), and dots are stacked, with each dot representing one observation.\nIn the code chunk below, geom_dotplot() of ggplot2 is used to plot a dot plot.\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot(dotsize = 0.5)\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\nRather than auto-assignment, the bin width could also be specified.\nSince each dot represents 1 observation, the y-axis is not meaningful and becomes misleading instead.\nTo address the above, the code chunk specifies the bin width, and also removes the y-axis using scale_y_continuous().\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot(binwidth=2.5,         \n               dotsize = 0.5) +      \n  scale_y_continuous(NULL,           \n                     breaks = NULL) \n\n\n\n\n\n\nGeometric object: Histograms\nIn the code chunk below, geom_histogram() is used to create a simple histogram by using values in MATHS field of exam_data.\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nGeometric object modification: geom()\nIn the code chunk below,\n\nbins argument is used to change the number of bins to 20,\nfill argument is used to shade the histogram with light blue color, and\ncolor argument is used to change the outline colour of the bars in black\n\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20,            \n                 color=\"black\",      \n                 fill=\"light blue\")  \n\n\n\n\n\n\nGeometric object modification: aes()\nThe code chunk below changes the interior colour of the histogram (i.e. fill) by using sub-group of aesthetic().\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           fill = GENDER)) +\n  geom_histogram(bins=20, \n                 color=\"grey30\")\n\n\n\n\n\n\nGeometric object: Kernel Density Estimate (KDE)\ngeom-density() computes and plots kernel density estimate, which is a smoothed version of the histogram.\nIt is a useful alternative to the histogram for continuous data that comes from an underlying smooth distribution.\nThe code below plots the distribution of Maths scores in a kernel density estimate plot.\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_density()           \n\n\n\n\nThe code chunk below plots two kernel density lines by using colour or fill arguments of aes()\n\nggplot(data=exam_data, \n       aes(x = MATHS, \n           colour = GENDER)) +\n  geom_density()\n\n\n\n\n\n\nGeometric object: Boxplot\ngeom_boxplot() displays continuous value list. It visualises five summary statistics (the median, two hinges and two whiskers), and all “outlying” points individually. The code chunk below plots boxplots by using geom_boxplot().\n\nggplot(data=exam_data, \n       aes(y = MATHS,       \n           x= GENDER)) +    \n  geom_boxplot()            \n\n\n\n\nNotches are used in box plots to help visually assess whether the medians of distributions differ. If the notches do not overlap, this is evidence that the medians are different.\nThe code chunk below plots the distribution of Maths scores by gender in notched plot instead of boxplot.\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot(notch=TRUE)\n\n\n\n\n\n\nGeometric object: Violin Plot\ngeom_violin is designed for creating violin plot. Violin plots are a way of comparing multiple data distributions. With ordinary density curves, it is difficult to compare more than just a few distributions because the lines visually interfere with each other. With a violin plot, it’s easier to compare several distributions since they’re placed side by side.\nThe code below plot the distribution of Maths score by gender in violin plot.\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_violin()\n\n\n\n\n\n\nGeometric object: Points\ngeom_point() is especially useful for creating scatterplot.\nThe code chunk below plots a scatterplot showing the Maths and English grades of pupils by using geom_point().\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point()            \n\n\n\n\n\n\nGeometric object: combination\nThe code chunk below plots the data points on the boxplots by using both geom_boxplot() and geom_point().\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot() +                    \n  geom_point(position=\"jitter\", \n             size = 0.5)        \n\n\n\n\n\n\n\nGGPlot2 Essential Grammatical Elements: Statistics\nThe Statistics functions statistically transform data, usually as some form of summary. For example:\n\nfrequency of values of a variable (bar graph)\n\na mean\na confidence limit\n\nThere are two ways to use these functions:\n\nadd a stat_() function and override the default geom, or\nadd a geom_() function and override the default stat.\n\n\n\nStat function: stat_summary()\nThe boxplots below are incomplete because the positions of the means were not shown.\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot()\n\n\n\n\nMean values can be depicted by a red dot, using stat_summary() and overriding the default geom(), to address the above with the following code chunk.\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  stat_summary(geom = \"point\",       \n               fun =\"mean\",         \n               colour =\"red\",        \n               size=4)\n\n\n\n\n\n\nStat function: using geom()\nThe code chunk below adding mean values by using geom_() function and overriding the default stat.\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\",        \n             fun.y=\"mean\",           \n             colour =\"red\",          \n             size=4)          \n\nWarning in geom_point(stat = \"summary\", fun.y = \"mean\", colour = \"red\", :\nIgnoring unknown parameters: `fun.y`\n\n\nNo summary function supplied, defaulting to `mean_se()`\n\n\n\n\n\n\n\nStat function: geom_smooth() to plot best fit curve\nThe interpretability of the scatterplot showing the relationship of Maths and English grades of pupils can be improved by adding a best fit curve.\nIn the code chunk below, geom_smooth() is used to plot a best fit curve on the scatterplot.\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(size=0.5)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThe default method used is loess.\nThe default smoothing method can be overridden as shown below.\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nGGPlot2 Essential Grammatical Elements: Facets\nFacetting generates small multiples (sometimes also called trellis plot), each displaying a different subset of the data. They are an alternative to aesthetics for displaying additional discrete variables. ggplot2 supports two types of factes, namely: facet_grid() and facet_wrap.\n\nFacet functions: facet_wrap()\nfacet_wrap wraps a 1d sequence of panels into 2d. This is generally a better use of screen space than facet_grid because most displays are roughly rectangular.\nThe code chunk below plots a trellis plot using facet-wrap().\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20) +\n    facet_wrap(~ CLASS)\n\n\n\n\n\n\nFacet functions: facet_grid()\nfacet_grid() forms a matrix of panels defined by row and column facetting variables. It is most useful when you have two discrete variables, and all combinations of the variables exist in the data.\nThe code chunk below plots a trellis plot using facet_grid().\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20) +\n    facet_grid(~ CLASS)\n\n\n\n\n\n\n\nGGPlot2 Essential Grammatical Elements: Coordinate systems\nThe Coordinates functions map the position of objects onto the plane of the plot. There are a number of different possible coordinate systems to use, they are:\n\ncoord_cartesian(): the default cartesian coordinate systems, where you specify x and y values (e.g. allows you to zoom in or out).\ncoord_flip(): a cartesian system with the x and y flipped.\ncoord_fixed(): a cartesian system with a “fixed” aspect ratio (e.g. 1.78 for a “widescreen” plot).\ncoord_quickmap(): a coordinate system that approximates a good aspect ratio for maps.\n\n\nCoordinates function: coord_flip()\nBy the default, the bar chart of ggplot2 is in vertical form.\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar()\n\n\n\n\nThe code chunk below flips the horizontal bar chart into vertical bar chart by using coord_flip().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\n\n\nCoordinates function: coord_cartesian()\nThe scatterplot on the right is slightly misleading because the y-aixs and x-axis range are not equal.\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, size=0.5)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe code chunk below fixed both the y-axis and x-axis range from 0-100.\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nGGPlot2 Essential Grammatical Elements: Themes\nThemes control elements of the graph not related to the data. For example:\n\nbackground colour\nsize of fonts\ngridlines\ncolour of labels\nBuilt-in themes include: - theme_gray() (default) - theme_bw() - theme_classic()\n\nA list of theme can be found at this link. Each theme element can be conceived of as either a line (e.g. x-axis), a rectangle (e.g. graph background), or text (e.g. axis title).\nThe code chunk below plot a horizontal bar chart using theme_gray().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_gray()\n\n\n\n\nA horizontal bar chart plotted using theme_classic().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_classic()\n\n\n\n\nA horizontal bar chart plotted using theme_minimal().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_minimal()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#references",
    "title": "Hands-on_Ex1",
    "section": "References",
    "text": "References\n\nHadley Wickham (2023) ggplot2: Elegant Graphics for Data Analysis. Online 3rd edition.\nWinston Chang (2013) R Graphics Cookbook 2nd edition. Online version.\nHealy, Kieran (2019) Data Visualization: A practical introduction. Online version\nLearning ggplot2 on Paper – Components\nLearning ggplot2 on Paper – Layer\nLearning ggplot2 on Paper – Scale"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html",
    "title": "Hands-on Ex2",
    "section": "",
    "text": "In this hands-on exercise, several ggplot2 extensions for creating more elegant and effective statistical graphics will be introduced\ncontrol the placement of annotation on a graph by using functions provided in ggrepel package, create professional publication quality figure by using functions provided in ggthemes and hrbrthemes packages, plot composite figure by combining ggplot2 graphs by using patchwork package.\n\n\nThe code chunk below installs and launches the tidyverse package into R environment\n\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: an R package provides some extra themes, geoms, and scales for ‘ggplot2’.\nhrbrthemes: an R package provides typography-centric themes and theme components for ggplot2.\npatchwork: an R package for preparing composite figure created using ggplot2.\n\n\npacman:: p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse)\n\n\n\n\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package, which is part of the tidyverse package.\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\nThere are a total of 7 attributes in the exam_data tibble data frame. 4 of them are categorical data type and the other 3 are in continuous data type.\n\ncategorical attributes are: ID, CLASS, GENDER and RACE.\ncontinuous attributes are: MATHS, ENGLISH and SCIENCE."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#overview",
    "title": "Hands-on Ex2",
    "section": "",
    "text": "In this hands-on exercise, several ggplot2 extensions for creating more elegant and effective statistical graphics will be introduced\ncontrol the placement of annotation on a graph by using functions provided in ggrepel package, create professional publication quality figure by using functions provided in ggthemes and hrbrthemes packages, plot composite figure by combining ggplot2 graphs by using patchwork package.\n\n\nThe code chunk below installs and launches the tidyverse package into R environment\n\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: an R package provides some extra themes, geoms, and scales for ‘ggplot2’.\nhrbrthemes: an R package provides typography-centric themes and theme components for ggplot2.\npatchwork: an R package for preparing composite figure created using ggplot2.\n\n\npacman:: p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse)\n\n\n\n\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package, which is part of the tidyverse package.\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\nThere are a total of 7 attributes in the exam_data tibble data frame. 4 of them are categorical data type and the other 3 are in continuous data type.\n\ncategorical attributes are: ID, CLASS, GENDER and RACE.\ncontinuous attributes are: MATHS, ENGLISH and SCIENCE."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#ggrepel-annotation",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#ggrepel-annotation",
    "title": "Hands-on Ex2",
    "section": "GGRepel Annotation",
    "text": "GGRepel Annotation\nOne of the challenge in plotting statistical graph is annotation, especially with large number of data points.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label(aes(label = ID), \n             hjust = .5, \n             vjust = -.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\nGGRepel is an extension of ggplot2 package which provides geoms for ggplot2 to repel overlapping text as in our examples on the right.\ngeom_text() simply replaces geom_text_repel() and geom_label_repel replaces geom_label()\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label_repel(aes(label = ID), \n                   fontface = \"bold\") +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#exploring-ggplot2-themes",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#exploring-ggplot2-themes",
    "title": "Hands-on Ex2",
    "section": "Exploring GGplot2 Themes",
    "text": "Exploring GGplot2 Themes\nggplot2 comes with eight built-in themes, they are: theme_gray(), theme_bw(), theme_classic(), theme_dark(), theme_light(), theme_linedraw(), theme_minimal(), and theme_void().\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_gray() +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\nRefer to this link for more info about ggplot2 Themes\n\nGGthemes Package\nGGthemes provides ‘ggplot2’ themes that replicate the look of plots by Edward Tufte, Stephen Few, Fivethirtyeight, The Economist, ‘Stata’, ‘Excel’, and The Wall Street Journal, among others.\nIn the example below, The Economist theme is used.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_economist()\n\n\n\n\n\nHRBthemes Package\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum()\n\n\n\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum(axis_title_size = 18,\n              base_size = 15,\n              grid = \"Y\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#plotting-multiple-graphs",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#plotting-multiple-graphs",
    "title": "Hands-on Ex2",
    "section": "Plotting Multiple Graphs",
    "text": "Plotting Multiple Graphs\n\nThe plotThe code\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\")\n\n\n\n\n\n\n\np1 &lt;- ggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\")\n\n\n\n\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\np2 &lt;- ggplot(data=exam_data, \n             aes(x = ENGLISH)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of English scores\")\n\n\n\n\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\np3 &lt;- ggplot(data=exam_data, \n             aes(x= MATHS, \n                 y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\n\nComposite Graphics\n\nCombining 2 ggplot2 graphs\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\np1 + p2 \n\n\n\n\n\nCombining 3 ggplot2 graphs\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n(p1 / p2) | p3\n\n\n\n\n\nComposite figure with tag\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = 'I')\n\n\n\n\n\nComposite figure with insert\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1)\n\n\n\n\n\nComposite figure by using patchwork and ggtheme\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\npatchwork &lt;- (p1 / p2) | p3\npatchwork & theme_economist()\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPlease note."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3a.html",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3a.html",
    "title": "Hands-on Ex3a",
    "section": "",
    "text": "ggiraph for making ‘ggplot’ graphics interactive.\nplotly, R library for plotting interactive statistical graphs.\nDT provides an R interface to the JavaScript library DataTables that create interactive table on html page.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\npatchwork for combining multiple ggplot2 graphs into one figure.\n\n\npacman::p_load(ggiraph, plotly, \n               patchwork, DT, tidyverse) \n\n\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\nggiraph is an htmlwidget and a ggplot2 extension. It allows ggplot graphics to be interactive.\nInteractive is made with ggplot geometries that can understand three arguments:\nTooltip: a column of data-sets that contain tooltips to be displayed when the mouse is over elements. Onclick: a column of data-sets that contain a JavaScript function to be executed when elements are clicked. Data_id: a column of data-sets that contain an id to be associated with elements. If it used within a shiny application, elements associated with an id (data_id) can be selected and manipulated on client and server sides. Refer to this article for more detail explanation.\n\n\nBelow shows a typical code chunk to plot an interactive statistical graph by using ggiraph package. Notice that the code chunk consists of two parts. First, an ggplot object will be created. Next, girafe() of ggiraph will be used to create an interactive svg object.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)\n\n\n\n\n\ntwo steps are involved. First, an interactive version of ggplot2 geom (i.e. geom_dotplot_interactive()) will be used to create the basic graph. Then, girafe() will be used to generate an svg object to be displayed on an html page.\nBy hovering the mouse pointer on an data point of interest, the student’s ID will be displayed.\n\n\n\ntooltip can be customised further to include a list object as shown in the code chunk below.\n\nexam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS)) \n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)\n\n\n\n\n\nThe first three lines of codes in the code chunk create a new field called tooltip. At the same time, it populates text in ID and CLASS fields into the newly created field. Next, this newly created field is used as tooltip field as shown in the code of line 7.\nBy hovering the mouse pointer on an data point of interest, the student’s ID and Class will be displayed\n\n\n\nCode chunk below uses opts_tooltip() of ggiraph to customize tooltip rendering by add css declarations\n\ntooltip_css &lt;- \"background-color:white; #&lt;&lt;\nfont-style:bold; color:black;\" #&lt;&lt;\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID),                   \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #&lt;&lt;\n    opts_tooltip(    #&lt;&lt;\n      css = tooltip_css)) #&lt;&lt;\n)              \n\n\n\n\n\nNotice that the background colour of the tooltip is black and the font colour is white and bold.\nRefer to Customizing girafe objects to learn more about how to customise ggiraph objects.\n\n\n\nCode chunk below shows an advanced way to customise tooltip. In this example, a function is used to compute 90% confident interval of the mean. The derived statistics are then displayed in the tooltip.\n\ntooltip &lt;- function(y, ymax, accuracy = .01) {\n  mean &lt;- scales::number(y, accuracy = accuracy)\n  sem &lt;- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)\n}\n\ngg_point &lt;- ggplot(data=exam_data, \n                   aes(x = RACE),\n) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light blue\"\n  ) +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, size = 0.2\n  )\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\n\nCode chunk below shows the second interactive feature of ggiraph, namely data_id.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)                \n\n\n\n\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\nNote that the default value of the hover css is hover_css = “fill:orange;”.\n\n\n\nIn the code chunk below, css codes are used to change the highlighting effect.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                              \n\n\n\n\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\nNote: Different from previous example, in this example the ccs customisation request are encoded directly.\n\n\n\ncombine tooltip and hover effect on the interactive statistical graph as shown in the code chunk below.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, \n        data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)           \n\n\n\n\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over. At the same time, the tooltip will show the CLASS.\n\n\n\nonclick argument of ggiraph provides hotlink interactivity on the web as in the code chunk below.\n\nexam_data$onclick &lt;- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618)\n\n\n\n\n\nInteractivity: Web document link with a data object will be displayed on the web browser upon mouse click.\nNote that click actions must be a string column in the dataset containing valid javascript instructions\n\n\n\nCoordinated multiple views methods has been implemented in the data visualisation below\nIn order to build a coordinated multiple views as shown in the example below, the following programming strategy will be used:\nAppropriate interactive functions of ggiraph will be used to create the multiple views. patchwork function of patchwork package will be used inside girafe function to create the interactive coordinated multiple views.\n\np1 &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np2 &lt;- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p1 + p2), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       ) \n\n\n\n\n\nNotice that when a data point of one of the dotplot is selected, the corresponding data point ID on the second data visualisation will be highlighted too.\nThe data_id aesthetic is critical to link observations between plots and the tooltip aesthetic is optional but nice to have when mouse over a point.\n\n\n\n\nPlotly’s R graphing library create interactive web graphics from ggplot2 graphs and/or a custom interface to the (MIT-licensed) JavaScript library plotly.js inspired by the grammar of graphics. Different from other plotly platform, plot.R is free and open source.\nThere are two ways to create interactive graph by using plotly, they are:\nby using plot_ly(), and by using ggplotly()\n\n\nThe tabset below shows an example a basic interactive plot created by using plot_ly().\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)\n\n\n\n\n\n\n\nIn the code chunk below, color argument is mapped to a qualitative visual variable (i.e. RACE)\n\nThe plotThe code\n\n\n\n\n\n\n\n\nInteractive:\nClick on the colour symbol at the legend.\n\n\n\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~RACE)\n\n\n\n\n\n\n\nThe code chunk below plots an interactive scatter plot by using ggplotly().\n\nThe plotThe code\n\n\n\n\n\n\n\n\nNotice that the only extra line you need to include in the code chunk is ggplotly().\n\n\n\np &lt;- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p)\n\n\n\n\n\n\n\nThe creation of a coordinated linked plot by using plotly involves three steps:\nhighlight_key() of plotly package is used as shared data. two scatterplots will be created by using ggplot2 functions. lastly, subplot() of plotly package is used to place them next to each other side-by-side.\n\nThe plotThe code\n\n\n\n\n\n\n\n\nClick on a data point of one of the scatterplot and see how the corresponding point on the other scatterplot is selected.\n\n\n\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)d &lt;- highlight_key(exam_data)\np1 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nsubplot(ggplotly(p1),\n        ggplotly(p2))\n\n\n\n\nThing to learn from the code chunk:\nhighlight_key() simply creates an object of class crosstalk::SharedData. Visit this link to learn more about crosstalk\n\n\n\n\nCrosstalk is an add-on to the htmlwidgets package. It extends htmlwidgets with a set of classes, functions, and conventions for implementing cross-widget interactions (currently, linked brushing and filtering).\n\n\nA wrapper of the JavaScript Library DataTables\nData objects in R can be rendered as HTML tables using the JavaScript library ‘DataTables’ (typically via R Markdown or Shiny).\n\nDT::datatable(exam_data, class= \"compact\")\n\n\n\n\n\n\n\n\n\nCode chunk below is used to implement the coordinated brushing shown above.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nd &lt;- highlight_key(exam_data) \np &lt;- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg &lt;- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)      \n\n\n\n\nThings to learn from the code chunk:\nhighlight() is a function of plotly package. It sets a variety of options for brushing (i.e., highlighting) multiple plots. These options are primarily designed for linking multiple plotly graphs, and may not behave as expected when linking plotly to another htmlwidget package via crosstalk. In some cases, other htmlwidgets will respect these options, such as persistent selection in leaflet.\nbscols() is a helper function of crosstalk package. It makes it easy to put HTML elements side by side. It can be called directly from the console but is especially designed to work in an R Markdown document. Warning: This will bring in all of Bootstrap!."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3a.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3a.html#getting-started",
    "title": "Hands-on Ex3a",
    "section": "",
    "text": "ggiraph for making ‘ggplot’ graphics interactive.\nplotly, R library for plotting interactive statistical graphs.\nDT provides an R interface to the JavaScript library DataTables that create interactive table on html page.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\npatchwork for combining multiple ggplot2 graphs into one figure.\n\n\npacman::p_load(ggiraph, plotly, \n               patchwork, DT, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3a.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3a.html#importing-data",
    "title": "Hands-on Ex3a",
    "section": "",
    "text": "exam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3a.html#interactive-data-visualisation-ggiraph-methods",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3a.html#interactive-data-visualisation-ggiraph-methods",
    "title": "Hands-on Ex3a",
    "section": "",
    "text": "ggiraph is an htmlwidget and a ggplot2 extension. It allows ggplot graphics to be interactive.\nInteractive is made with ggplot geometries that can understand three arguments:\nTooltip: a column of data-sets that contain tooltips to be displayed when the mouse is over elements. Onclick: a column of data-sets that contain a JavaScript function to be executed when elements are clicked. Data_id: a column of data-sets that contain an id to be associated with elements. If it used within a shiny application, elements associated with an id (data_id) can be selected and manipulated on client and server sides. Refer to this article for more detail explanation.\n\n\nBelow shows a typical code chunk to plot an interactive statistical graph by using ggiraph package. Notice that the code chunk consists of two parts. First, an ggplot object will be created. Next, girafe() of ggiraph will be used to create an interactive svg object.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)\n\n\n\n\n\ntwo steps are involved. First, an interactive version of ggplot2 geom (i.e. geom_dotplot_interactive()) will be used to create the basic graph. Then, girafe() will be used to generate an svg object to be displayed on an html page.\nBy hovering the mouse pointer on an data point of interest, the student’s ID will be displayed.\n\n\n\ntooltip can be customised further to include a list object as shown in the code chunk below.\n\nexam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS)) \n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)\n\n\n\n\n\nThe first three lines of codes in the code chunk create a new field called tooltip. At the same time, it populates text in ID and CLASS fields into the newly created field. Next, this newly created field is used as tooltip field as shown in the code of line 7.\nBy hovering the mouse pointer on an data point of interest, the student’s ID and Class will be displayed\n\n\n\nCode chunk below uses opts_tooltip() of ggiraph to customize tooltip rendering by add css declarations\n\ntooltip_css &lt;- \"background-color:white; #&lt;&lt;\nfont-style:bold; color:black;\" #&lt;&lt;\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID),                   \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #&lt;&lt;\n    opts_tooltip(    #&lt;&lt;\n      css = tooltip_css)) #&lt;&lt;\n)              \n\n\n\n\n\nNotice that the background colour of the tooltip is black and the font colour is white and bold.\nRefer to Customizing girafe objects to learn more about how to customise ggiraph objects.\n\n\n\nCode chunk below shows an advanced way to customise tooltip. In this example, a function is used to compute 90% confident interval of the mean. The derived statistics are then displayed in the tooltip.\n\ntooltip &lt;- function(y, ymax, accuracy = .01) {\n  mean &lt;- scales::number(y, accuracy = accuracy)\n  sem &lt;- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)\n}\n\ngg_point &lt;- ggplot(data=exam_data, \n                   aes(x = RACE),\n) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light blue\"\n  ) +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, size = 0.2\n  )\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\n\nCode chunk below shows the second interactive feature of ggiraph, namely data_id.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)                \n\n\n\n\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\nNote that the default value of the hover css is hover_css = “fill:orange;”.\n\n\n\nIn the code chunk below, css codes are used to change the highlighting effect.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                              \n\n\n\n\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\nNote: Different from previous example, in this example the ccs customisation request are encoded directly.\n\n\n\ncombine tooltip and hover effect on the interactive statistical graph as shown in the code chunk below.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, \n        data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)           \n\n\n\n\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over. At the same time, the tooltip will show the CLASS.\n\n\n\nonclick argument of ggiraph provides hotlink interactivity on the web as in the code chunk below.\n\nexam_data$onclick &lt;- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618)\n\n\n\n\n\nInteractivity: Web document link with a data object will be displayed on the web browser upon mouse click.\nNote that click actions must be a string column in the dataset containing valid javascript instructions\n\n\n\nCoordinated multiple views methods has been implemented in the data visualisation below\nIn order to build a coordinated multiple views as shown in the example below, the following programming strategy will be used:\nAppropriate interactive functions of ggiraph will be used to create the multiple views. patchwork function of patchwork package will be used inside girafe function to create the interactive coordinated multiple views.\n\np1 &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np2 &lt;- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p1 + p2), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       ) \n\n\n\n\n\nNotice that when a data point of one of the dotplot is selected, the corresponding data point ID on the second data visualisation will be highlighted too.\nThe data_id aesthetic is critical to link observations between plots and the tooltip aesthetic is optional but nice to have when mouse over a point."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3a.html#interactive-data-visualisation---plotly-methods",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3a.html#interactive-data-visualisation---plotly-methods",
    "title": "Hands-on Ex3a",
    "section": "",
    "text": "Plotly’s R graphing library create interactive web graphics from ggplot2 graphs and/or a custom interface to the (MIT-licensed) JavaScript library plotly.js inspired by the grammar of graphics. Different from other plotly platform, plot.R is free and open source.\nThere are two ways to create interactive graph by using plotly, they are:\nby using plot_ly(), and by using ggplotly()\n\n\nThe tabset below shows an example a basic interactive plot created by using plot_ly().\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)\n\n\n\n\n\n\n\nIn the code chunk below, color argument is mapped to a qualitative visual variable (i.e. RACE)\n\nThe plotThe code\n\n\n\n\n\n\n\n\nInteractive:\nClick on the colour symbol at the legend.\n\n\n\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~RACE)\n\n\n\n\n\n\n\nThe code chunk below plots an interactive scatter plot by using ggplotly().\n\nThe plotThe code\n\n\n\n\n\n\n\n\nNotice that the only extra line you need to include in the code chunk is ggplotly().\n\n\n\np &lt;- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p)\n\n\n\n\n\n\n\nThe creation of a coordinated linked plot by using plotly involves three steps:\nhighlight_key() of plotly package is used as shared data. two scatterplots will be created by using ggplot2 functions. lastly, subplot() of plotly package is used to place them next to each other side-by-side.\n\nThe plotThe code\n\n\n\n\n\n\n\n\nClick on a data point of one of the scatterplot and see how the corresponding point on the other scatterplot is selected.\n\n\n\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)d &lt;- highlight_key(exam_data)\np1 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nsubplot(ggplotly(p1),\n        ggplotly(p2))\n\n\n\n\nThing to learn from the code chunk:\nhighlight_key() simply creates an object of class crosstalk::SharedData. Visit this link to learn more about crosstalk"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3a.html#interactive-data-visualisation---crosstalk-methods",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3a.html#interactive-data-visualisation---crosstalk-methods",
    "title": "Hands-on Ex3a",
    "section": "",
    "text": "Crosstalk is an add-on to the htmlwidgets package. It extends htmlwidgets with a set of classes, functions, and conventions for implementing cross-widget interactions (currently, linked brushing and filtering).\n\n\nA wrapper of the JavaScript Library DataTables\nData objects in R can be rendered as HTML tables using the JavaScript library ‘DataTables’ (typically via R Markdown or Shiny).\n\nDT::datatable(exam_data, class= \"compact\")\n\n\n\n\n\n\n\n\n\nCode chunk below is used to implement the coordinated brushing shown above.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nd &lt;- highlight_key(exam_data) \np &lt;- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg &lt;- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)      \n\n\n\n\nThings to learn from the code chunk:\nhighlight() is a function of plotly package. It sets a variety of options for brushing (i.e., highlighting) multiple plots. These options are primarily designed for linking multiple plotly graphs, and may not behave as expected when linking plotly to another htmlwidget package via crosstalk. In some cases, other htmlwidgets will respect these options, such as persistent selection in leaflet.\nbscols() is a helper function of crosstalk package. It makes it easy to put HTML elements side by side. It can be called directly from the console but is especially designed to work in an R Markdown document. Warning: This will bring in all of Bootstrap!."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3b.html",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3b.html",
    "title": "Hands-on Ex3b",
    "section": "",
    "text": "When telling a visually-driven data story, animated graphics tends to attract the interest of the audience and make deeper impression than static graphics. In this hands-on exercise, you will learn how to create animated data visualisation by using gganimate and plotly r packages. At the same time, you will also learn how to (i) reshape data by using tidyr package, and (ii) process, wrangle and transform data by using dplyr package.\n\n\nWhen creating animations, the plot does not actually move. Instead, many individual plots are built and then stitched together as movie frames, just like an old-school flip book or cartoon. Each frame is a different plot when conveying motion, which is built using some relevant subset of the aggregate data. The subset drives the flow of the animation when stitched back together.\n\n\n\nBefore we dive into the steps for creating an animated statistical graph, it’s important to understand some of the key concepts and terminology related to this type of visualization.\nFrame: In an animated line graph, each frame represents a different point in time or a different category. When the frame changes, the data points on the graph are updated to reflect the new data.\nAnimation Attributes: The animation attributes are the settings that control how the animation behaves. For example, you can specify the duration of each frame, the easing function used to transition between frames, and whether to start the animation from the current frame or from the beginning.\nTip Before you start making animated graphs, you should first ask yourself: Does it makes sense to go through the effort? If you are conducting an exploratory data analysis, a animated graphic may not be worth the time investment. However, if you are giving a presentation, a few well-placed animated graphics can help an audience connect with your topic remarkably better than static counterparts.\n\n\n\n\n\n\nFirst, write a code chunk to check, install and load the following R packages:\n\nplotly, R library for plotting interactive statistical graphs.\ngganimate, an ggplot extension for creating animated statistical graphs.\ngifski converts video frames to GIF animations using pngquant’s fancy features for efficient cross-frame palettes and temporal dithering. It produces animated GIFs that use thousands of colors per frame.\ngapminder: An excerpt of the data available at Gapminder.org. We just want to use its country_colors scheme.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\n\n\npacman::p_load(readxl, gifski, gapminder,\n               plotly, gganimate, tidyverse)\n\n\n\n\nIn this hands-on exercise, the Data worksheet from GlobalPopulation Excel workbook will be used.\nWrite a code chunk to import Data worksheet from GlobalPopulation Excel workbook by using appropriate R package from tidyverse family.\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_each_(funs(factor(.)), col) %&gt;%\n  mutate(Year = as.integer(Year))\n\nThings to learn from the code chunk above read_xls() of readxl package is used to import the Excel worksheet. mutate_each_() of dplyr package is used to convert all character data type into factor. mutate of dplyr package is used to convert data values of Year field into integer. Unfortunately, mutate_each_() was deprecated in dplyr 0.7.0. and funs() was deprecated in dplyr 0.8.0. In view of this, we will re-write the code by using mutate_at() as shown in the code chunk below.\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_at(col, as.factor) %&gt;%\n  mutate(Year = as.integer(Year))\n\nInstead of using mutate_at(), across() can be used to derive the same outputs.\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate(across(col, as.factor)) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\n\n\n\ngganimate extends the grammar of graphics as implemented by ggplot2 to include the description of animation. It does this by providing a range of new grammar classes that can be added to the plot object in order to customise how it should change with time.\n\ntransition_*() defines how the data should be spread out and how it relates to itself across time.\nview_*() defines how the positional scales should change along the animation.\nshadow_*() defines how data from other points in time should be presented in the given point in time.\nenter_()/exit_() defines how new data should appear and how old data should disappear during the course of the animation.\nease_aes() defines how different aesthetics should be eased during transitions.\n\n\n\nIn the code chunk below, the basic ggplot2 functions are used to create a static bubble plot.\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') \n\n\n\n\n\n\n\nIn the code chunk below,\ntransition_time() of gganimate is used to create transition through distinct states in time (i.e. Year). ease_aes() is used to control easing of aesthetics. The default is linear. Other methods are: quadratic, cubic, quartic, quintic, sine, circular, exponential, elastic, back, and bounce.\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +       \n  ease_aes('linear')          \n\n\n\n\n\n\n\n\nIn Plotly R package, both ggplotly() and plot_ly() support key frame animations through the frame argument/aesthetic. They also support an ids argument/aesthetic to ensure smooth transitions between objects with the same id (which helps facilitate object constancy).\n\n\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young')\n\nggplotly(gg)    \n\n\n\n\nThings to learn from the code chunk above Appropriate ggplot2 functions are used to create a static bubble plot. The output is then saved as an R object called gg. ggplotly() is then used to convert the R graphic object into an animated svg object.\nNotice that although show.legend = FALSE argument was used, the legend still appears on the plot. To overcome this problem, theme(legend.position=‘none’) should be used as shown in the plot and code chunk below.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young') + \n  theme(legend.position='none')\n\nggplotly(gg)\n\n\n\n\n\n\n\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\nbp &lt;- globalPop %&gt;%\n  plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent,\n          sizes = c(2, 100),\n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          ) %&gt;%\n  layout(showlegend = FALSE)\nbp"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3b.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3b.html#overview",
    "title": "Hands-on Ex3b",
    "section": "",
    "text": "When telling a visually-driven data story, animated graphics tends to attract the interest of the audience and make deeper impression than static graphics. In this hands-on exercise, you will learn how to create animated data visualisation by using gganimate and plotly r packages. At the same time, you will also learn how to (i) reshape data by using tidyr package, and (ii) process, wrangle and transform data by using dplyr package.\n\n\nWhen creating animations, the plot does not actually move. Instead, many individual plots are built and then stitched together as movie frames, just like an old-school flip book or cartoon. Each frame is a different plot when conveying motion, which is built using some relevant subset of the aggregate data. The subset drives the flow of the animation when stitched back together.\n\n\n\nBefore we dive into the steps for creating an animated statistical graph, it’s important to understand some of the key concepts and terminology related to this type of visualization.\nFrame: In an animated line graph, each frame represents a different point in time or a different category. When the frame changes, the data points on the graph are updated to reflect the new data.\nAnimation Attributes: The animation attributes are the settings that control how the animation behaves. For example, you can specify the duration of each frame, the easing function used to transition between frames, and whether to start the animation from the current frame or from the beginning.\nTip Before you start making animated graphs, you should first ask yourself: Does it makes sense to go through the effort? If you are conducting an exploratory data analysis, a animated graphic may not be worth the time investment. However, if you are giving a presentation, a few well-placed animated graphics can help an audience connect with your topic remarkably better than static counterparts."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3b.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3b.html#getting-started",
    "title": "Hands-on Ex3b",
    "section": "",
    "text": "First, write a code chunk to check, install and load the following R packages:\n\nplotly, R library for plotting interactive statistical graphs.\ngganimate, an ggplot extension for creating animated statistical graphs.\ngifski converts video frames to GIF animations using pngquant’s fancy features for efficient cross-frame palettes and temporal dithering. It produces animated GIFs that use thousands of colors per frame.\ngapminder: An excerpt of the data available at Gapminder.org. We just want to use its country_colors scheme.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\n\n\npacman::p_load(readxl, gifski, gapminder,\n               plotly, gganimate, tidyverse)\n\n\n\n\nIn this hands-on exercise, the Data worksheet from GlobalPopulation Excel workbook will be used.\nWrite a code chunk to import Data worksheet from GlobalPopulation Excel workbook by using appropriate R package from tidyverse family.\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_each_(funs(factor(.)), col) %&gt;%\n  mutate(Year = as.integer(Year))\n\nThings to learn from the code chunk above read_xls() of readxl package is used to import the Excel worksheet. mutate_each_() of dplyr package is used to convert all character data type into factor. mutate of dplyr package is used to convert data values of Year field into integer. Unfortunately, mutate_each_() was deprecated in dplyr 0.7.0. and funs() was deprecated in dplyr 0.8.0. In view of this, we will re-write the code by using mutate_at() as shown in the code chunk below.\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_at(col, as.factor) %&gt;%\n  mutate(Year = as.integer(Year))\n\nInstead of using mutate_at(), across() can be used to derive the same outputs.\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate(across(col, as.factor)) %&gt;%\n  mutate(Year = as.integer(Year))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3b.html#animated-data-visualisation-gganimate-methods",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3b.html#animated-data-visualisation-gganimate-methods",
    "title": "Hands-on Ex3b",
    "section": "",
    "text": "gganimate extends the grammar of graphics as implemented by ggplot2 to include the description of animation. It does this by providing a range of new grammar classes that can be added to the plot object in order to customise how it should change with time.\n\ntransition_*() defines how the data should be spread out and how it relates to itself across time.\nview_*() defines how the positional scales should change along the animation.\nshadow_*() defines how data from other points in time should be presented in the given point in time.\nenter_()/exit_() defines how new data should appear and how old data should disappear during the course of the animation.\nease_aes() defines how different aesthetics should be eased during transitions.\n\n\n\nIn the code chunk below, the basic ggplot2 functions are used to create a static bubble plot.\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') \n\n\n\n\n\n\n\nIn the code chunk below,\ntransition_time() of gganimate is used to create transition through distinct states in time (i.e. Year). ease_aes() is used to control easing of aesthetics. The default is linear. Other methods are: quadratic, cubic, quartic, quintic, sine, circular, exponential, elastic, back, and bounce.\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +       \n  ease_aes('linear')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3b.html#animated-data-visualisation-plotly",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3b.html#animated-data-visualisation-plotly",
    "title": "Hands-on Ex3b",
    "section": "",
    "text": "In Plotly R package, both ggplotly() and plot_ly() support key frame animations through the frame argument/aesthetic. They also support an ids argument/aesthetic to ensure smooth transitions between objects with the same id (which helps facilitate object constancy).\n\n\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young')\n\nggplotly(gg)    \n\n\n\n\nThings to learn from the code chunk above Appropriate ggplot2 functions are used to create a static bubble plot. The output is then saved as an R object called gg. ggplotly() is then used to convert the R graphic object into an animated svg object.\nNotice that although show.legend = FALSE argument was used, the legend still appears on the plot. To overcome this problem, theme(legend.position=‘none’) should be used as shown in the plot and code chunk below.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young') + \n  theme(legend.position='none')\n\nggplotly(gg)\n\n\n\n\n\n\n\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\nbp &lt;- globalPop %&gt;%\n  plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent,\n          sizes = c(2, 100),\n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          ) %&gt;%\n  layout(showlegend = FALSE)\nbp"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4a.html",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4a.html",
    "title": "Hands-on Ex4a",
    "section": "",
    "text": "ggstatsplot is an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\n\nTo provide alternative statistical inference methods by default.\nTo follow best practices for statistical reporting. For all statistical tests reported\n\n\n\nThe code chunk below installs and launches the tidyverse, ggdist, ggridges, colourspace & ggthemes packages into R environment\n\npacman:: p_load(ggstatsplot, tidyverse)\n\n\n\n\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package, which is part of the tidyverse package.\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThat’s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10. It can be defined mathematically as\n\nThe Schwarz criterion/BIC is one of the easiest ways to calculate rough approximation of the Bayes Factor.\nA Bayes Factor can be any positive number. One of the most common interpretations is this one—first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013:\n\n\n\n\nIn the code chunk below, ggbetweenstats() is used to build a visual for two-sample mean test of Maths scores by gender.\n\nggbetweenstats(\n  data = exam,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\n\nIn the code chunk below, ggbetweenstats() is used to build a visual for One-way ANOVA test on English score by race.\n\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n“ns” → only non-significant\n“s” → only significant\n“all” → everything\n\n\n\n\n\n\n\n\n\n\nIn the code chunk below, ggscatterstats() is used to build a visual for Significant Test of Correlation between Maths scores and English scores.\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\nIn the code chunk below, the Maths scores is binned into a 4-class variable by using cut().\n\nexam1 &lt;- exam %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\n\nIn this code chunk below ggbarstats() is used to build a visual for Significant Test of Association\n\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)\n\n\n\n\n\n\n\n\nIn this section, you will learn how to visualise model diagnostic and model parameters by using parameters package.\nToyota Corolla case study will be used. The purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables.\n\n\nThe code chunk below installs and launches the tidyverse, ggdist, ggridges, colourspace & ggthemes packages into R environment\n\npacman::p_load(readxl, performance, parameters, see, qqplotr)\n\n\n\n\nIn the code chunk below, read_xls() of readxl package is used to import the data worksheet of ToyotaCorolla.xls workbook into R.\n\ncar_resale &lt;- read_xls(\"data/ToyotaCorolla.xls\", \"data\")\ncar_resale\n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …\n\n\nNotice that the output object car_resale is a tibble data frame.\n\n\n\nThe code chunk below is used to calibrate a multiple linear regression model by using lm() of Base Stats of R.\n\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\n\nIn the code chunk, check_collinearity() of performance package.\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\nIn the code chunk, check_normality() of performance package.\n\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\ncheck_n &lt;- check_normality(model1)\n\nplot(check_n)\n\n\n\n\n\n\n\nIn the code chunk, check_heteroscedasticity() of performance package.\n\ncheck_h &lt;- check_heteroscedasticity(model1)\n\nplot(check_h)\n\n\n\n\n\n\n\nWe can also perform the complete by using check_model().\n\ncheck_model(model1)\n\n\n\n\n\n\n\nIn the code below, plot() of see package and parameters() of parameters package is used to visualise the parameters of a regression model.\n\nplot(parameters(model1))\n\n\n\n\n\n\n\nIn the code below, ggcoefstats() of ggstatsplot package to visualise the parameters of a regression model.\n\nggcoefstats(model1, \n            output = \"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4a.html#visual-statistical-analysis-with-ggstatsplot",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4a.html#visual-statistical-analysis-with-ggstatsplot",
    "title": "Hands-on Ex4a",
    "section": "",
    "text": "ggstatsplot is an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\n\nTo provide alternative statistical inference methods by default.\nTo follow best practices for statistical reporting. For all statistical tests reported\n\n\n\nThe code chunk below installs and launches the tidyverse, ggdist, ggridges, colourspace & ggthemes packages into R environment\n\npacman:: p_load(ggstatsplot, tidyverse)\n\n\n\n\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package, which is part of the tidyverse package.\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThat’s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10. It can be defined mathematically as\n\nThe Schwarz criterion/BIC is one of the easiest ways to calculate rough approximation of the Bayes Factor.\nA Bayes Factor can be any positive number. One of the most common interpretations is this one—first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013:\n\n\n\n\nIn the code chunk below, ggbetweenstats() is used to build a visual for two-sample mean test of Maths scores by gender.\n\nggbetweenstats(\n  data = exam,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\n\nIn the code chunk below, ggbetweenstats() is used to build a visual for One-way ANOVA test on English score by race.\n\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n“ns” → only non-significant\n“s” → only significant\n“all” → everything\n\n\n\n\n\n\n\n\n\n\nIn the code chunk below, ggscatterstats() is used to build a visual for Significant Test of Correlation between Maths scores and English scores.\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\nIn the code chunk below, the Maths scores is binned into a 4-class variable by using cut().\n\nexam1 &lt;- exam %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\n\nIn this code chunk below ggbarstats() is used to build a visual for Significant Test of Association\n\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4a.html#visualising-models-with-parameters-package",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4a.html#visualising-models-with-parameters-package",
    "title": "Hands-on Ex4a",
    "section": "",
    "text": "In this section, you will learn how to visualise model diagnostic and model parameters by using parameters package.\nToyota Corolla case study will be used. The purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables.\n\n\nThe code chunk below installs and launches the tidyverse, ggdist, ggridges, colourspace & ggthemes packages into R environment\n\npacman::p_load(readxl, performance, parameters, see, qqplotr)\n\n\n\n\nIn the code chunk below, read_xls() of readxl package is used to import the data worksheet of ToyotaCorolla.xls workbook into R.\n\ncar_resale &lt;- read_xls(\"data/ToyotaCorolla.xls\", \"data\")\ncar_resale\n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …\n\n\nNotice that the output object car_resale is a tibble data frame.\n\n\n\nThe code chunk below is used to calibrate a multiple linear regression model by using lm() of Base Stats of R.\n\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\n\nIn the code chunk, check_collinearity() of performance package.\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\nIn the code chunk, check_normality() of performance package.\n\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\ncheck_n &lt;- check_normality(model1)\n\nplot(check_n)\n\n\n\n\n\n\n\nIn the code chunk, check_heteroscedasticity() of performance package.\n\ncheck_h &lt;- check_heteroscedasticity(model1)\n\nplot(check_h)\n\n\n\n\n\n\n\nWe can also perform the complete by using check_model().\n\ncheck_model(model1)\n\n\n\n\n\n\n\nIn the code below, plot() of see package and parameters() of parameters package is used to visualise the parameters of a regression model.\n\nplot(parameters(model1))\n\n\n\n\n\n\n\nIn the code below, ggcoefstats() of ggstatsplot package to visualise the parameters of a regression model.\n\nggcoefstats(model1, \n            output = \"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4b.html",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4b.html",
    "title": "Hands-on Ex4b",
    "section": "",
    "text": "For the purpose of this exercise, the following R packages will be used, they are:\n\ntidyverse, a family of R packages for data science process,\nplotly for creating interactive plot,\ngganimate for creating animation plot,\nDT for displaying interactive html table,\ncrosstalk for for implementing cross-widget interactions (currently, linked brushing and filtering), and\nggdist for visualising distribution and uncertainty.\n\n\ndevtools::install_github(\"wilkelab/ungeviz\")\n\n\npacman::p_load(ungeviz, plotly, crosstalk,\n               DT, ggdist, ggridges,\n               colorspace, gganimate, tidyverse)\n\n\n\n\nFor the purpose of this exercise, Exam_data.csv will be used.\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\n\nA point estimate is a single number, such as a mean. Uncertainty, on the other hand, is expressed as standard error, confidence interval, or credible interval.\nImportant Don’t confuse the uncertainty of a point estimate with the variation in the sample In this section, you will learn how to plot error bars of maths scores by race by using data provided in exam tibble data frame.\nFirstly, code chunk below will be used to derive the necessary summary statistics.\n\nmy_sum &lt;- exam %&gt;%\n  group_by(RACE) %&gt;%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1))\n\nThings to learn from the code chunk above - group_by() of dplyr package is used to group the observation by RACE, - summarise() is used to compute the count of observations, mean, standard deviation - mutate() is used to derive standard error of Maths by RACE, and - the output is save as a tibble data table called my_sum.\nNote For the mathematical explanation, please refer to Slide 20 of Lesson 4.\nNext, the code chunk below will be used to display my_sum tibble data frame in an html table format.\n\nThe code chunkThe table\n\n\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n\n\n\n\n\nRACE\nn\nmean\nsd\nse\n\n\n\n\nChinese\n193\n76.50777\n15.69040\n1.132357\n\n\nIndian\n12\n60.66667\n23.35237\n7.041005\n\n\nMalay\n108\n57.44444\n21.13478\n2.043177\n\n\nOthers\n9\n69.66667\n10.72381\n3.791438\n\n\n\n\n\n\n\n\n\n\n\n\nNow we are ready to plot the standard error bars of mean maths score by race as shown below.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean maths score by rac\")\n\n\n\n\nThings to learn from the code chunk above The error bars are computed by using the formula mean+/-se. For geom_point(), it is important to indicate stat=“identity”.\n\n\n\nInstead of plotting the standard error bar of point estimates, we can also plot the confidence intervals of mean maths score by race.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=reorder(RACE, -mean), \n        ymin=mean-1.96*se, \n        ymax=mean+1.96*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  labs(x = \"Maths score\",\n       title = \"95% confidence interval of mean maths score by race\")\n\n\n\n\nThings to learn from the code chunk above The confidence intervals are computed by using the formula mean+/-1.96*se. The error bars is sorted by using the average maths scores. labs() argument of ggplot2 is used to change the x-axis label.\n\n\n\nIn this section, you will learn how to plot interactive error bars for the 99% confidence interval of mean maths score by race as shown in the figure below.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence interval of average /&lt;br&gt;maths scores by race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))\n\n\n\n\n\n\n\n\nggdist is an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualising distributions and uncertainty. It is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization: for frequentist models, one visualises confidence distributions or bootstrap distributions (see vignette(“freq-uncertainty-vis”)); for Bayesian models, one visualises probability distributions (see the tidybayes package, which builds on top of ggdist).\n\n\nIn the code chunk below, stat_pointinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\nNote This function comes with many arguments, students are advised to read the syntax reference for more detail.\nFor example, in the code chunk below the following arguments are used:\n\n.width = 0.95\n.point = median\n.interval = qi\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point + Multiple-interval plot\")\n\n\n\n\n\n\n\nYour turn Makeover the plot on previous slide by showing 95% and 99% confidence intervals.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval(\n    show.legend = FALSE) +   \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\nGentle advice: This function comes with many arguments, students are advised to read the syntax reference for more detail.\n\n\n\nIn the code chunk below, stat_gradientinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")\n\n\n\n\nGentle advice: This function comes with many arguments, students are advised to read the syntax reference for more detail.\n\n\n\n\nStep 1: Installing ungeviz package\n\ndevtools::install_github(\"wilkelab/ungeviz\")\n\nNote: You only need to perform this step once.\nStep 2: Launch the application in R\n\nlibrary(ungeviz)\n\n\nggplot(data = exam, \n       (aes(x = factor(RACE), y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, width = 0.05), \n    size = 0.4, color = \"#0072B2\", alpha = 1/2) +\n  geom_hpline(data = sampler(25, group = RACE), height = 0.6, color = \"#D55E00\") +\n  theme_bw() + \n  # `.draw` is a generated column indicating the sample draw\n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4b.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4b.html#getting-started",
    "title": "Hands-on Ex4b",
    "section": "",
    "text": "For the purpose of this exercise, the following R packages will be used, they are:\n\ntidyverse, a family of R packages for data science process,\nplotly for creating interactive plot,\ngganimate for creating animation plot,\nDT for displaying interactive html table,\ncrosstalk for for implementing cross-widget interactions (currently, linked brushing and filtering), and\nggdist for visualising distribution and uncertainty.\n\n\ndevtools::install_github(\"wilkelab/ungeviz\")\n\n\npacman::p_load(ungeviz, plotly, crosstalk,\n               DT, ggdist, ggridges,\n               colorspace, gganimate, tidyverse)\n\n\n\n\nFor the purpose of this exercise, Exam_data.csv will be used.\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4b.html#visualising-the-uncertainty-of-pt-estimates-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4b.html#visualising-the-uncertainty-of-pt-estimates-ggplot2-methods",
    "title": "Hands-on Ex4b",
    "section": "",
    "text": "A point estimate is a single number, such as a mean. Uncertainty, on the other hand, is expressed as standard error, confidence interval, or credible interval.\nImportant Don’t confuse the uncertainty of a point estimate with the variation in the sample In this section, you will learn how to plot error bars of maths scores by race by using data provided in exam tibble data frame.\nFirstly, code chunk below will be used to derive the necessary summary statistics.\n\nmy_sum &lt;- exam %&gt;%\n  group_by(RACE) %&gt;%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1))\n\nThings to learn from the code chunk above - group_by() of dplyr package is used to group the observation by RACE, - summarise() is used to compute the count of observations, mean, standard deviation - mutate() is used to derive standard error of Maths by RACE, and - the output is save as a tibble data table called my_sum.\nNote For the mathematical explanation, please refer to Slide 20 of Lesson 4.\nNext, the code chunk below will be used to display my_sum tibble data frame in an html table format.\n\nThe code chunkThe table\n\n\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n\n\n\n\n\nRACE\nn\nmean\nsd\nse\n\n\n\n\nChinese\n193\n76.50777\n15.69040\n1.132357\n\n\nIndian\n12\n60.66667\n23.35237\n7.041005\n\n\nMalay\n108\n57.44444\n21.13478\n2.043177\n\n\nOthers\n9\n69.66667\n10.72381\n3.791438\n\n\n\n\n\n\n\n\n\n\n\n\nNow we are ready to plot the standard error bars of mean maths score by race as shown below.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean maths score by rac\")\n\n\n\n\nThings to learn from the code chunk above The error bars are computed by using the formula mean+/-se. For geom_point(), it is important to indicate stat=“identity”.\n\n\n\nInstead of plotting the standard error bar of point estimates, we can also plot the confidence intervals of mean maths score by race.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=reorder(RACE, -mean), \n        ymin=mean-1.96*se, \n        ymax=mean+1.96*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  labs(x = \"Maths score\",\n       title = \"95% confidence interval of mean maths score by race\")\n\n\n\n\nThings to learn from the code chunk above The confidence intervals are computed by using the formula mean+/-1.96*se. The error bars is sorted by using the average maths scores. labs() argument of ggplot2 is used to change the x-axis label.\n\n\n\nIn this section, you will learn how to plot interactive error bars for the 99% confidence interval of mean maths score by race as shown in the figure below.\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence interval of average /&lt;br&gt;maths scores by race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4b.html#visualising-uncertainty-ggdist",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4b.html#visualising-uncertainty-ggdist",
    "title": "Hands-on Ex4b",
    "section": "",
    "text": "ggdist is an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualising distributions and uncertainty. It is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization: for frequentist models, one visualises confidence distributions or bootstrap distributions (see vignette(“freq-uncertainty-vis”)); for Bayesian models, one visualises probability distributions (see the tidybayes package, which builds on top of ggdist).\n\n\nIn the code chunk below, stat_pointinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\nNote This function comes with many arguments, students are advised to read the syntax reference for more detail.\nFor example, in the code chunk below the following arguments are used:\n\n.width = 0.95\n.point = median\n.interval = qi\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point + Multiple-interval plot\")\n\n\n\n\n\n\n\nYour turn Makeover the plot on previous slide by showing 95% and 99% confidence intervals.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval(\n    show.legend = FALSE) +   \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\nGentle advice: This function comes with many arguments, students are advised to read the syntax reference for more detail.\n\n\n\nIn the code chunk below, stat_gradientinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")\n\n\n\n\nGentle advice: This function comes with many arguments, students are advised to read the syntax reference for more detail."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4b.html#visualising-uncertainty-with-hops",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4b.html#visualising-uncertainty-with-hops",
    "title": "Hands-on Ex4b",
    "section": "",
    "text": "Step 1: Installing ungeviz package\n\ndevtools::install_github(\"wilkelab/ungeviz\")\n\nNote: You only need to perform this step once.\nStep 2: Launch the application in R\n\nlibrary(ungeviz)\n\n\nggplot(data = exam, \n       (aes(x = factor(RACE), y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, width = 0.05), \n    size = 0.4, color = \"#0072B2\", alpha = 1/2) +\n  geom_hpline(data = sampler(25, group = RACE), height = 0.6, color = \"#D55E00\") +\n  theme_bw() + \n  # `.draw` is a generated column indicating the sample draw\n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4c.html",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4c.html",
    "title": "Hands-on Ex4c",
    "section": "",
    "text": "In this exercise, four R packages will be used. They are:\n\nreadr for importing csv into R.\nFunnelPlotR for creating funnel plot.\nggplot2 for creating funnel plot manually.\nknitr for building static html table.\nplotly for creating interactive funnel plot.\n\n\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)\n\n\n\n\nIn this section, COVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal. For this hands-on exercise, we are going to compare the cumulative COVID-19 cases and death by sub-district (i.e. kelurahan) as at 31st July 2021, DKI Jakarta.\nThe code chunk below imports the data into R and save it into a tibble data frame object called covid19.\n\ncovid19 &lt;- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)\n\n\n\n\n\nFunnelPlotR package uses ggplot to generate funnel plots. It requires a numerator (events of interest), denominator (population to be considered) and group. The key arguments selected for customisation are:\nlimit: plot limits (95 or 99). label_outliers: to label outliers (true or false). Poisson_limits: to add Poisson limits to the plot. OD_adjust: to add overdispersed limits to the plot. xrange and yrange: to specify the range to display for axes, acts like a zoom function. Other aesthetic components such as graph title, axis labels etc.\n\n\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Positive,\n  denominator = Death,\n  group = `Sub-district`\n)\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code chunk above.\n\ngroup in this function is different from the scatterplot. Here, it defines the level of the points to be plotted i.e. Sub-district, District or City. If City is chosen, there are only six data points.\nBy default, data_typeargument is “SR”.\nlimit: Plot limits, accepted values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution.\n\n\n\n\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  .data = covid19,  \n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",     #&lt;&lt;\n  x_range = c(0, 6500),  #&lt;&lt;\n  y_range = c(0, 0.05)   #&lt;&lt;\n)\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code chunk above. + data_type argument is used to change from default “SR” to “PR” (i.e. proportions). + xrange and yrange are used to set the range of x-axis and y-axis\n\n\n\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  .data = covid19,  \n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",   \n  x_range = c(0, 6500),  \n  y_range = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code chunk above.\n\nlabel = NA argument is to removed the default label outliers feature.\ntitle argument is used to add plot title.\nx_label and y_label arguments are used to add/edit x-axis and y-axis titles.\n\n\n\n\n\nIn this section, you will gain hands-on experience on building funnel plots step-by-step by using ggplot2. It aims to enhance you working experience of ggplot2 to customise speciallised data visualisation like funnel plot.\n\n\nTo plot the funnel plot from scratch, we need to derive cumulative death rate and standard error of cumulative death rate.\n\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\nNext, the fit.mean is computed by using the code chunk below.\n\nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\n\nThe code chunk below is used to compute the lower and upper limits for 95% confidence interval.\n\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\n\nIn the code chunk below, ggplot2 functions are used to plot a static funnel plot.\n\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\n\nThe funnel plot created using ggplot2 functions can be made interactive with ggplotly() of plotly r package.\n\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4c.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4c.html#getting-started",
    "title": "Hands-on Ex4c",
    "section": "",
    "text": "In this exercise, four R packages will be used. They are:\n\nreadr for importing csv into R.\nFunnelPlotR for creating funnel plot.\nggplot2 for creating funnel plot manually.\nknitr for building static html table.\nplotly for creating interactive funnel plot.\n\n\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)\n\n\n\n\nIn this section, COVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal. For this hands-on exercise, we are going to compare the cumulative COVID-19 cases and death by sub-district (i.e. kelurahan) as at 31st July 2021, DKI Jakarta.\nThe code chunk below imports the data into R and save it into a tibble data frame object called covid19.\n\ncovid19 &lt;- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4c.html#funnelplotr-methods",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4c.html#funnelplotr-methods",
    "title": "Hands-on Ex4c",
    "section": "",
    "text": "FunnelPlotR package uses ggplot to generate funnel plots. It requires a numerator (events of interest), denominator (population to be considered) and group. The key arguments selected for customisation are:\nlimit: plot limits (95 or 99). label_outliers: to label outliers (true or false). Poisson_limits: to add Poisson limits to the plot. OD_adjust: to add overdispersed limits to the plot. xrange and yrange: to specify the range to display for axes, acts like a zoom function. Other aesthetic components such as graph title, axis labels etc.\n\n\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Positive,\n  denominator = Death,\n  group = `Sub-district`\n)\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code chunk above.\n\ngroup in this function is different from the scatterplot. Here, it defines the level of the points to be plotted i.e. Sub-district, District or City. If City is chosen, there are only six data points.\nBy default, data_typeargument is “SR”.\nlimit: Plot limits, accepted values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution.\n\n\n\n\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  .data = covid19,  \n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",     #&lt;&lt;\n  x_range = c(0, 6500),  #&lt;&lt;\n  y_range = c(0, 0.05)   #&lt;&lt;\n)\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code chunk above. + data_type argument is used to change from default “SR” to “PR” (i.e. proportions). + xrange and yrange are used to set the range of x-axis and y-axis\n\n\n\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  .data = covid19,  \n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",   \n  x_range = c(0, 6500),  \n  y_range = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThings to learn from the code chunk above.\n\nlabel = NA argument is to removed the default label outliers feature.\ntitle argument is used to add plot title.\nx_label and y_label arguments are used to add/edit x-axis and y-axis titles."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4c.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4c.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "title": "Hands-on Ex4c",
    "section": "",
    "text": "In this section, you will gain hands-on experience on building funnel plots step-by-step by using ggplot2. It aims to enhance you working experience of ggplot2 to customise speciallised data visualisation like funnel plot.\n\n\nTo plot the funnel plot from scratch, we need to derive cumulative death rate and standard error of cumulative death rate.\n\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\nNext, the fit.mean is computed by using the code chunk below.\n\nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\n\nThe code chunk below is used to compute the lower and upper limits for 95% confidence interval.\n\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\n\nIn the code chunk below, ggplot2 functions are used to plot a static funnel plot.\n\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\n\nThe funnel plot created using ggplot2 functions can be made interactive with ggplotly() of plotly r package.\n\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html",
    "title": "Hands-on Ex5",
    "section": "",
    "text": "The code chunk below loads the following libraries:\n\ntidytext,\ntidyverse (mainly readr, purrr, stringr, ggplot2)\nwidyr, - wordcloud and ggwordcloud,\ntextplot (required igraph, tidygraph and ggraph, )\nDT,\nlubridate and hms.\n\n\npacman::p_load(tidytext, widyr, wordcloud, DT, ggwordcloud, textplot, lubridate, hms,\ntidyverse, tidygraph, ggraph, igraph)\n\n\n\n\n\n\n\nnews20 &lt;- \"data/20news/\"\n\n\n\n\n\nread_folder &lt;- function(infolder) {\n  tibble(file = dir(infolder, \n                    full.names = TRUE)) %&gt;%\n    mutate(text = map(file, \n                      read_lines)) %&gt;%\n    transmute(id = basename(file), \n              text) %&gt;%\n    unnest(text)\n}\n\n\n\n\n\nraw_text &lt;- tibble(folder = \n                     dir(news20, \n                         full.names = TRUE)) %&gt;%\n  mutate(folder_out = map(folder, \n                          read_folder)) %&gt;%\n  unnest(cols = c(folder_out)) %&gt;%\n  transmute(newsgroup = basename(folder), \n            id, text)\nwrite_rds(raw_text, \"data/rds/news20.rds\")\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\n\nread_lines() of readr package is used to read up to n_max lines from a file.\nmap() of purrr package is used to transform their input by applying a function to each element of a list and returning an object of the same length as the input.\nunnest() of dplyr package is used to flatten a list-column of data frames back out into regular columns.\nmutate() of dplyr is used to add new variables and preserves existing ones;\ntransmute() of dplyr is used to add new variables and drops existing ones.\nread_rds() is used to save the extracted and combined data frame as rds file for future use.\n\n\n\n\n\n\n\n\nThe code chunk below gives the Figure that shows the frequency of messages by newsgroup.\n\nraw_text %&gt;%\n  group_by(newsgroup) %&gt;%\n  summarize(messages = n_distinct(id)) %&gt;%\n  ggplot(aes(messages, newsgroup)) +\n  geom_col(fill = \"lightblue\") +\n  labs(y = NULL)\n\n\n\n\n\n\n\n\nUsing tidy data principles in processing, analysing and visualising text data.\nMuch of the infrastructure needed for text mining with tidy data frames already exists in packages like ‘dplyr’, ‘broom’, ‘tidyr’, and ‘ggplot2’.\n\nFigure below shows the workflow using tidytext approach for processing and visualising text data.\n\n\n\nNotice that each message has some structure and extra text that we don’t want to include in our analysis. For example, every message has a header, containing field such as “from:” or “in_reply_to:” that describe the message. Some also have automated email signatures, which occur after a line like “–”.\n\ncleaned_text &lt;- raw_text %&gt;%\n  group_by(newsgroup, id) %&gt;%\n  filter(cumsum(text == \"\") &gt; 0,\n         cumsum(str_detect(\n           text, \"^--\")) == 0) %&gt;%\n  ungroup()\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\n\ncumsum() of base R is used to return a vector whose elements are the cumulative sums of the elements of the argument.\nstr_detect() from stringr is used to detect the presence or absence of a pattern in a string.\n\n\n\n\n\n\nIn this code chunk below, regular expressions are used to remove with nested text representing quotes from other users.\n\ncleaned_text &lt;- cleaned_text %&gt;%\n  filter(str_detect(text, \"^[^&gt;]+[A-Za-z\\\\d]\")\n         | text == \"\",\n         !str_detect(text, \n                     \"writes(:|\\\\.\\\\.\\\\.)$\"),\n         !str_detect(text, \n                     \"^In article &lt;\")\n  )\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\n\nstr_detect() from stringr is used to detect the presence or absence of a pattern in a string.\nfilter() of dplyr package is used to subset a data frame, retaining all rows that satisfy the specified conditions.\n\n\n\n\n\n\nIn this code chunk below, unnest_tokens() of tidytext package is used to split the dataset into tokens, while stop_words() is used to remove stop-words.\n\nusenet_words &lt;- cleaned_text %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  filter(str_detect(word, \"[a-z']$\"),\n         !word %in% stop_words$word)\n\nNow that we’ve removed the headers, signatures, and formatting, we can start exploring common words. For starters, we could find the most common words in the entire dataset, or within particular newsgroups.\n\nusenet_words %&gt;%\n  count(word, sort = TRUE)\n\n# A tibble: 5,542 × 2\n   word           n\n   &lt;chr&gt;      &lt;int&gt;\n 1 people        57\n 2 time          50\n 3 jesus         47\n 4 god           44\n 5 message       40\n 6 br            27\n 7 bible         23\n 8 drive         23\n 9 homosexual    23\n10 read          22\n# ℹ 5,532 more rows\n\n\nInstead of counting individual word, you can also count words within by newsgroup by using the code chunk below.\n\nwords_by_newsgroup &lt;- usenet_words %&gt;%\n  count(newsgroup, word, sort = TRUE) %&gt;%\n  ungroup()\n\n\n\n\nIn this code chunk below, wordcloud() of wordcloud package is used to plot a static wordcloud.\n\nwordcloud(words_by_newsgroup$word,\n          words_by_newsgroup$n,\n          max.words = 300)\n\n\n\n\nA DT table can be used to complement the visual discovery.\n\nDT::datatable(words_by_newsgroup, filter = 'top') %&gt;% \n  formatStyle(0, \n              target = 'row', \n              lineHeight='25%')\n\n\n\n\n\n\n\n\n\nThe wordcloud below is plotted by using ggwordcloud package\n\nset.seed(1234)\n\nwords_by_newsgroup %&gt;%\n  filter(n &gt; 0) %&gt;%\nggplot(aes(label = word,\n           size = n)) +\n  geom_text_wordcloud() +\n  theme_minimal() +\n  facet_wrap(~newsgroup)\n\n\n\n\n\n\n\n\n\ntf–idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection of corpus.\n\n\n\n\nThe code chunk below uses bind_tf_idf() of tidytext to compute and bind the term frequency, inverse document frequency and ti-idf of a tidy text dataset to the dataset.\n\ntf_idf &lt;- words_by_newsgroup %&gt;%\n  bind_tf_idf(word, newsgroup, n) %&gt;%\n  arrange(desc(tf_idf))\n\n###Visualising tf-idf as interactive table\nThe code chunk below uses datatable() of DT package to create a html table that allows pagination of rows and columns.\n\nDT::datatable(tf_idf, filter = 'top') %&gt;% \n  formatRound(columns = c('tf', 'idf', \n                          'tf_idf'), \n              digits = 3) %&gt;%\n  formatStyle(0, \n              target = 'row', \n              lineHeight='25%')\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\n\nfilter() argument is used to turn control the filter UI.\nformatRound() is used to customise the values format. The argument digits define the number of decimal places.\n\nformatStyle() is used to customise the output table. In this example, the arguments target and lineHeight are used to reduce the line height by 25%.\n\n\n\nTo learn more about customising DT’s table, visit this link.\n\n\n\nFacet bar charts technique is used to visualise the tf-idf values of science related newsgroup.\n\ntf_idf %&gt;%\n  filter(str_detect(newsgroup, \"^sci\\\\.\")) %&gt;%\n  group_by(newsgroup) %&gt;%\n  slice_max(tf_idf, \n            n = 12) %&gt;%\n  ungroup() %&gt;%\n  mutate(word = reorder(word, \n                        tf_idf)) %&gt;%\n  ggplot(aes(tf_idf, \n             word, \n             fill = newsgroup)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ newsgroup, \n             scales = \"free\") +\n  labs(x = \"tf-idf\", \n       y = NULL)\n\n\n\n\n\n\n\n\nTo count the number of times that two words appear within the same document, or to see how correlated they are.\nMost operations for finding pairwise counts or correlations need to turn the data into a wide matrix first.\nwidyr package first ‘casts’ a tidy dataset into a wide matrix, performs an operation such as a correlation on it, then re-tidies the result.\n\n\nIn this code chunk below, pairwise_cor() of widyr package is used to compute the correlation between newsgroup based on the common words found.\n\nnewsgroup_cors &lt;- words_by_newsgroup %&gt;%\n  pairwise_cor(newsgroup, \n               word, \n               n, \n               sort = TRUE)\n\n\n\n\nNow, we can visualise the relationship between newgroups in network graph as shown below.\n\nset.seed(2017)\n\nnewsgroup_cors %&gt;%\n  filter(correlation &gt; .025) %&gt;%\n  graph_from_data_frame() %&gt;%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(alpha = correlation, \n                     width = correlation)) +\n  geom_node_point(size = 6, \n                  color = \"lightblue\") +\n  geom_node_text(aes(label = name),\n                 color = \"red\",\n                 repel = TRUE) +\n  theme_void()\n\n\n\n\n\n\n\nIn this code chunk below, a bigram data frame is created by using unnest_tokens() of tidytext.\n\nbigrams &lt;- cleaned_text %&gt;%\n  unnest_tokens(bigram, \n                text, \n                token = \"ngrams\", \n                n = 2)\nbigrams\n\n# A tibble: 28,824 × 3\n   newsgroup   id    bigram    \n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;     \n 1 alt.atheism 54256 &lt;NA&gt;      \n 2 alt.atheism 54256 &lt;NA&gt;      \n 3 alt.atheism 54256 as i      \n 4 alt.atheism 54256 i don't   \n 5 alt.atheism 54256 don't know\n 6 alt.atheism 54256 know this \n 7 alt.atheism 54256 this book \n 8 alt.atheism 54256 book i    \n 9 alt.atheism 54256 i will    \n10 alt.atheism 54256 will use  \n# ℹ 28,814 more rows\n\n\n\n\n\nThe code chunk is used to count and sort the bigram data frame ascendingly.\n\nbigrams_count &lt;- bigrams %&gt;%\n  filter(bigram != 'NA') %&gt;%\n  count(bigram, sort = TRUE)\n\nbigrams_count\n\n# A tibble: 19,885 × 2\n   bigram       n\n   &lt;chr&gt;    &lt;int&gt;\n 1 of the     169\n 2 in the     113\n 3 to the      74\n 4 to be       59\n 5 for the     52\n 6 i have      48\n 7 that the    47\n 8 if you      40\n 9 on the      39\n10 it is       38\n# ℹ 19,875 more rows\n\n\n\n\n\nThe code chunk below is used to seperate the bigram into two words.\n\nbigrams_separated &lt;- bigrams %&gt;%\n  filter(bigram != 'NA') %&gt;%\n  separate(bigram, c(\"word1\", \"word2\"), \n           sep = \" \")\n\nbigrams_filtered &lt;- bigrams_separated %&gt;%\n  filter(!word1 %in% stop_words$word) %&gt;%\n  filter(!word2 %in% stop_words$word)\n\nbigrams_filtered\n\n# A tibble: 4,604 × 4\n   newsgroup   id    word1        word2        \n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;        \n 1 alt.atheism 54256 defines      god          \n 2 alt.atheism 54256 term         preclues     \n 3 alt.atheism 54256 science      ideas        \n 4 alt.atheism 54256 ideas        drawn        \n 5 alt.atheism 54256 supernatural precludes    \n 6 alt.atheism 54256 scientific   assertions   \n 7 alt.atheism 54256 religious    dogma        \n 8 alt.atheism 54256 religion     involves     \n 9 alt.atheism 54256 involves     circumventing\n10 alt.atheism 54256 gain         absolute     \n# ℹ 4,594 more rows\n\n\n\n\n\n\nbigram_counts &lt;- bigrams_filtered %&gt;% \n  count(word1, word2, sort = TRUE)\n\n\n\n\nIn the code chunk below, a network graph is created by using graph_from_data_frame() of igraph package.\n\nbigram_graph &lt;- bigram_counts %&gt;%\n  filter(n &gt; 3) %&gt;%\n  graph_from_data_frame()\nbigram_graph\n\nIGRAPH a48d625 DN-- 40 24 -- \n+ attr: name (v/c), n (e/n)\n+ edges from a48d625 (vertex names):\n [1] 1          -&gt;2           1          -&gt;3           static     -&gt;void       \n [4] time       -&gt;pad         1          -&gt;4           infield    -&gt;fly        \n [7] mat        -&gt;28          vv         -&gt;vv          1          -&gt;5          \n[10] cock       -&gt;crow        noticeshell-&gt;widget      27         -&gt;1993       \n[13] 3          -&gt;4           child      -&gt;molestation cock       -&gt;crew       \n[16] gun        -&gt;violence    heat       -&gt;sink        homosexual -&gt;male       \n[19] homosexual -&gt;women       include    -&gt;xol         mary       -&gt;magdalene  \n[22] read       -&gt;write       rev        -&gt;20          tt         -&gt;ee         \n\n\n\n\n\nIn this code chunk below, ggraph package is used to plot the bigram.\n\nset.seed(1234)\n\nggraph(bigram_graph, layout = \"fr\") +\n  geom_edge_link() +\n  geom_node_point() +\n  geom_node_text(aes(label = name), \n                 vjust = 1, \n                 hjust = 1)\n\n\n\n\n\n\n\nIn this code chunk below, ggraph package is used to plot the bigram.\n\nset.seed(1234)\n\na &lt;- grid::arrow(type = \"closed\", \n                 length = unit(.15,\n                               \"inches\"))\n\nggraph(bigram_graph, \n       layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = n), \n                 show.legend = FALSE,\n                 arrow = a, \n                 end_cap = circle(.07,\n                                  'inches')) +\n  geom_node_point(color = \"lightblue\", \n                  size = 5) +\n  geom_node_text(aes(label = name), \n                 vjust = 1, \n                 hjust = 1) +\n  theme_void()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#getting-started",
    "title": "Hands-on Ex5",
    "section": "",
    "text": "The code chunk below loads the following libraries:\n\ntidytext,\ntidyverse (mainly readr, purrr, stringr, ggplot2)\nwidyr, - wordcloud and ggwordcloud,\ntextplot (required igraph, tidygraph and ggraph, )\nDT,\nlubridate and hms.\n\n\npacman::p_load(tidytext, widyr, wordcloud, DT, ggwordcloud, textplot, lubridate, hms,\ntidyverse, tidygraph, ggraph, igraph)\n\n\n\n\n\n\n\nnews20 &lt;- \"data/20news/\"\n\n\n\n\n\nread_folder &lt;- function(infolder) {\n  tibble(file = dir(infolder, \n                    full.names = TRUE)) %&gt;%\n    mutate(text = map(file, \n                      read_lines)) %&gt;%\n    transmute(id = basename(file), \n              text) %&gt;%\n    unnest(text)\n}\n\n\n\n\n\nraw_text &lt;- tibble(folder = \n                     dir(news20, \n                         full.names = TRUE)) %&gt;%\n  mutate(folder_out = map(folder, \n                          read_folder)) %&gt;%\n  unnest(cols = c(folder_out)) %&gt;%\n  transmute(newsgroup = basename(folder), \n            id, text)\nwrite_rds(raw_text, \"data/rds/news20.rds\")\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\n\nread_lines() of readr package is used to read up to n_max lines from a file.\nmap() of purrr package is used to transform their input by applying a function to each element of a list and returning an object of the same length as the input.\nunnest() of dplyr package is used to flatten a list-column of data frames back out into regular columns.\nmutate() of dplyr is used to add new variables and preserves existing ones;\ntransmute() of dplyr is used to add new variables and drops existing ones.\nread_rds() is used to save the extracted and combined data frame as rds file for future use."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#initial-eda",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#initial-eda",
    "title": "Hands-on Ex5",
    "section": "",
    "text": "The code chunk below gives the Figure that shows the frequency of messages by newsgroup.\n\nraw_text %&gt;%\n  group_by(newsgroup) %&gt;%\n  summarize(messages = n_distinct(id)) %&gt;%\n  ggplot(aes(messages, newsgroup)) +\n  geom_col(fill = \"lightblue\") +\n  labs(y = NULL)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#introducing-tidytext",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#introducing-tidytext",
    "title": "Hands-on Ex5",
    "section": "",
    "text": "Using tidy data principles in processing, analysing and visualising text data.\nMuch of the infrastructure needed for text mining with tidy data frames already exists in packages like ‘dplyr’, ‘broom’, ‘tidyr’, and ‘ggplot2’.\n\nFigure below shows the workflow using tidytext approach for processing and visualising text data.\n\n\n\nNotice that each message has some structure and extra text that we don’t want to include in our analysis. For example, every message has a header, containing field such as “from:” or “in_reply_to:” that describe the message. Some also have automated email signatures, which occur after a line like “–”.\n\ncleaned_text &lt;- raw_text %&gt;%\n  group_by(newsgroup, id) %&gt;%\n  filter(cumsum(text == \"\") &gt; 0,\n         cumsum(str_detect(\n           text, \"^--\")) == 0) %&gt;%\n  ungroup()\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\n\ncumsum() of base R is used to return a vector whose elements are the cumulative sums of the elements of the argument.\nstr_detect() from stringr is used to detect the presence or absence of a pattern in a string.\n\n\n\n\n\n\nIn this code chunk below, regular expressions are used to remove with nested text representing quotes from other users.\n\ncleaned_text &lt;- cleaned_text %&gt;%\n  filter(str_detect(text, \"^[^&gt;]+[A-Za-z\\\\d]\")\n         | text == \"\",\n         !str_detect(text, \n                     \"writes(:|\\\\.\\\\.\\\\.)$\"),\n         !str_detect(text, \n                     \"^In article &lt;\")\n  )\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\n\nstr_detect() from stringr is used to detect the presence or absence of a pattern in a string.\nfilter() of dplyr package is used to subset a data frame, retaining all rows that satisfy the specified conditions.\n\n\n\n\n\n\nIn this code chunk below, unnest_tokens() of tidytext package is used to split the dataset into tokens, while stop_words() is used to remove stop-words.\n\nusenet_words &lt;- cleaned_text %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  filter(str_detect(word, \"[a-z']$\"),\n         !word %in% stop_words$word)\n\nNow that we’ve removed the headers, signatures, and formatting, we can start exploring common words. For starters, we could find the most common words in the entire dataset, or within particular newsgroups.\n\nusenet_words %&gt;%\n  count(word, sort = TRUE)\n\n# A tibble: 5,542 × 2\n   word           n\n   &lt;chr&gt;      &lt;int&gt;\n 1 people        57\n 2 time          50\n 3 jesus         47\n 4 god           44\n 5 message       40\n 6 br            27\n 7 bible         23\n 8 drive         23\n 9 homosexual    23\n10 read          22\n# ℹ 5,532 more rows\n\n\nInstead of counting individual word, you can also count words within by newsgroup by using the code chunk below.\n\nwords_by_newsgroup &lt;- usenet_words %&gt;%\n  count(newsgroup, word, sort = TRUE) %&gt;%\n  ungroup()\n\n\n\n\nIn this code chunk below, wordcloud() of wordcloud package is used to plot a static wordcloud.\n\nwordcloud(words_by_newsgroup$word,\n          words_by_newsgroup$n,\n          max.words = 300)\n\n\n\n\nA DT table can be used to complement the visual discovery.\n\nDT::datatable(words_by_newsgroup, filter = 'top') %&gt;% \n  formatStyle(0, \n              target = 'row', \n              lineHeight='25%')\n\n\n\n\n\n\n\n\n\nThe wordcloud below is plotted by using ggwordcloud package\n\nset.seed(1234)\n\nwords_by_newsgroup %&gt;%\n  filter(n &gt; 0) %&gt;%\nggplot(aes(label = word,\n           size = n)) +\n  geom_text_wordcloud() +\n  theme_minimal() +\n  facet_wrap(~newsgroup)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#basic-concept-of-tf-idf",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5.html#basic-concept-of-tf-idf",
    "title": "Hands-on Ex5",
    "section": "",
    "text": "tf–idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection of corpus.\n\n\n\n\nThe code chunk below uses bind_tf_idf() of tidytext to compute and bind the term frequency, inverse document frequency and ti-idf of a tidy text dataset to the dataset.\n\ntf_idf &lt;- words_by_newsgroup %&gt;%\n  bind_tf_idf(word, newsgroup, n) %&gt;%\n  arrange(desc(tf_idf))\n\n###Visualising tf-idf as interactive table\nThe code chunk below uses datatable() of DT package to create a html table that allows pagination of rows and columns.\n\nDT::datatable(tf_idf, filter = 'top') %&gt;% \n  formatRound(columns = c('tf', 'idf', \n                          'tf_idf'), \n              digits = 3) %&gt;%\n  formatStyle(0, \n              target = 'row', \n              lineHeight='25%')\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\n\nfilter() argument is used to turn control the filter UI.\nformatRound() is used to customise the values format. The argument digits define the number of decimal places.\n\nformatStyle() is used to customise the output table. In this example, the arguments target and lineHeight are used to reduce the line height by 25%.\n\n\n\nTo learn more about customising DT’s table, visit this link.\n\n\n\nFacet bar charts technique is used to visualise the tf-idf values of science related newsgroup.\n\ntf_idf %&gt;%\n  filter(str_detect(newsgroup, \"^sci\\\\.\")) %&gt;%\n  group_by(newsgroup) %&gt;%\n  slice_max(tf_idf, \n            n = 12) %&gt;%\n  ungroup() %&gt;%\n  mutate(word = reorder(word, \n                        tf_idf)) %&gt;%\n  ggplot(aes(tf_idf, \n             word, \n             fill = newsgroup)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ newsgroup, \n             scales = \"free\") +\n  labs(x = \"tf-idf\", \n       y = NULL)\n\n\n\n\n\n\n\n\nTo count the number of times that two words appear within the same document, or to see how correlated they are.\nMost operations for finding pairwise counts or correlations need to turn the data into a wide matrix first.\nwidyr package first ‘casts’ a tidy dataset into a wide matrix, performs an operation such as a correlation on it, then re-tidies the result.\n\n\nIn this code chunk below, pairwise_cor() of widyr package is used to compute the correlation between newsgroup based on the common words found.\n\nnewsgroup_cors &lt;- words_by_newsgroup %&gt;%\n  pairwise_cor(newsgroup, \n               word, \n               n, \n               sort = TRUE)\n\n\n\n\nNow, we can visualise the relationship between newgroups in network graph as shown below.\n\nset.seed(2017)\n\nnewsgroup_cors %&gt;%\n  filter(correlation &gt; .025) %&gt;%\n  graph_from_data_frame() %&gt;%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(alpha = correlation, \n                     width = correlation)) +\n  geom_node_point(size = 6, \n                  color = \"lightblue\") +\n  geom_node_text(aes(label = name),\n                 color = \"red\",\n                 repel = TRUE) +\n  theme_void()\n\n\n\n\n\n\n\nIn this code chunk below, a bigram data frame is created by using unnest_tokens() of tidytext.\n\nbigrams &lt;- cleaned_text %&gt;%\n  unnest_tokens(bigram, \n                text, \n                token = \"ngrams\", \n                n = 2)\nbigrams\n\n# A tibble: 28,824 × 3\n   newsgroup   id    bigram    \n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;     \n 1 alt.atheism 54256 &lt;NA&gt;      \n 2 alt.atheism 54256 &lt;NA&gt;      \n 3 alt.atheism 54256 as i      \n 4 alt.atheism 54256 i don't   \n 5 alt.atheism 54256 don't know\n 6 alt.atheism 54256 know this \n 7 alt.atheism 54256 this book \n 8 alt.atheism 54256 book i    \n 9 alt.atheism 54256 i will    \n10 alt.atheism 54256 will use  \n# ℹ 28,814 more rows\n\n\n\n\n\nThe code chunk is used to count and sort the bigram data frame ascendingly.\n\nbigrams_count &lt;- bigrams %&gt;%\n  filter(bigram != 'NA') %&gt;%\n  count(bigram, sort = TRUE)\n\nbigrams_count\n\n# A tibble: 19,885 × 2\n   bigram       n\n   &lt;chr&gt;    &lt;int&gt;\n 1 of the     169\n 2 in the     113\n 3 to the      74\n 4 to be       59\n 5 for the     52\n 6 i have      48\n 7 that the    47\n 8 if you      40\n 9 on the      39\n10 it is       38\n# ℹ 19,875 more rows\n\n\n\n\n\nThe code chunk below is used to seperate the bigram into two words.\n\nbigrams_separated &lt;- bigrams %&gt;%\n  filter(bigram != 'NA') %&gt;%\n  separate(bigram, c(\"word1\", \"word2\"), \n           sep = \" \")\n\nbigrams_filtered &lt;- bigrams_separated %&gt;%\n  filter(!word1 %in% stop_words$word) %&gt;%\n  filter(!word2 %in% stop_words$word)\n\nbigrams_filtered\n\n# A tibble: 4,604 × 4\n   newsgroup   id    word1        word2        \n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;        \n 1 alt.atheism 54256 defines      god          \n 2 alt.atheism 54256 term         preclues     \n 3 alt.atheism 54256 science      ideas        \n 4 alt.atheism 54256 ideas        drawn        \n 5 alt.atheism 54256 supernatural precludes    \n 6 alt.atheism 54256 scientific   assertions   \n 7 alt.atheism 54256 religious    dogma        \n 8 alt.atheism 54256 religion     involves     \n 9 alt.atheism 54256 involves     circumventing\n10 alt.atheism 54256 gain         absolute     \n# ℹ 4,594 more rows\n\n\n\n\n\n\nbigram_counts &lt;- bigrams_filtered %&gt;% \n  count(word1, word2, sort = TRUE)\n\n\n\n\nIn the code chunk below, a network graph is created by using graph_from_data_frame() of igraph package.\n\nbigram_graph &lt;- bigram_counts %&gt;%\n  filter(n &gt; 3) %&gt;%\n  graph_from_data_frame()\nbigram_graph\n\nIGRAPH a48d625 DN-- 40 24 -- \n+ attr: name (v/c), n (e/n)\n+ edges from a48d625 (vertex names):\n [1] 1          -&gt;2           1          -&gt;3           static     -&gt;void       \n [4] time       -&gt;pad         1          -&gt;4           infield    -&gt;fly        \n [7] mat        -&gt;28          vv         -&gt;vv          1          -&gt;5          \n[10] cock       -&gt;crow        noticeshell-&gt;widget      27         -&gt;1993       \n[13] 3          -&gt;4           child      -&gt;molestation cock       -&gt;crew       \n[16] gun        -&gt;violence    heat       -&gt;sink        homosexual -&gt;male       \n[19] homosexual -&gt;women       include    -&gt;xol         mary       -&gt;magdalene  \n[22] read       -&gt;write       rev        -&gt;20          tt         -&gt;ee         \n\n\n\n\n\nIn this code chunk below, ggraph package is used to plot the bigram.\n\nset.seed(1234)\n\nggraph(bigram_graph, layout = \"fr\") +\n  geom_edge_link() +\n  geom_node_point() +\n  geom_node_text(aes(label = name), \n                 vjust = 1, \n                 hjust = 1)\n\n\n\n\n\n\n\nIn this code chunk below, ggraph package is used to plot the bigram.\n\nset.seed(1234)\n\na &lt;- grid::arrow(type = \"closed\", \n                 length = unit(.15,\n                               \"inches\"))\n\nggraph(bigram_graph, \n       layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = n), \n                 show.legend = FALSE,\n                 arrow = a, \n                 end_cap = circle(.07,\n                                  'inches')) +\n  geom_node_point(color = \"lightblue\", \n                  size = 5) +\n  geom_node_text(aes(label = name), \n                 vjust = 1, \n                 hjust = 1) +\n  theme_void()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html",
    "title": "Hands-on Ex6",
    "section": "",
    "text": "This hands-on exercise will cover how to model, analyse and visualise network data using R, with the following key areas:\n\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph,\nbuild network graph visualisation using appropriate functions of ggraph,\ncompute network geometrics using tidygraph,\nbuild advanced graph visualisation by incorporating the network geometrics, and\nbuild interactive network visualisation using visNetwork package.\n\n\n\n\n\n\nIn this hands-on exercise, 4 network data modelling and visualisation packages will be installed and launched. They are igraph, tidygraph, ggraph and visNetwork. Beside these 4 packages, tidyverse and lubridate, specially designed to handle and wrangle time data, will be installed and launched too.\n\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)\n\n\n\n\n\nThe data sets used in this hands-on exercise is from an oil exploration and extraction company. There are two data sets. One contains the nodes data and the other contains the edges (also know as link) data.\n\n\nIn this step, you will import GAStech_email_node.csv and GAStech_email_edges-v2.csv into RStudio environment by using read_csv() of readr package.\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\n\nGAStech-email_edges.csv which consists of two weeks of 9063 emails correspondances between 55 employees.\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\n\nGAStech_email_nodes.csv which consist of the names, department and title of the 55 employees\n\nglimpse(GAStech_nodes)\n\nRows: 54\nColumns: 4\n$ id         &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 44, 45, 46, 8, 9, 10, 11, 12, 13, 14, …\n$ label      &lt;chr&gt; \"Mat.Bramar\", \"Anda.Ribera\", \"Rachel.Pantanal\", \"Linda.Lago…\n$ Department &lt;chr&gt; \"Administration\", \"Administration\", \"Administration\", \"Admi…\n$ Title      &lt;chr&gt; \"Assistant to CEO\", \"Assistant to CFO\", \"Assistant to CIO\",…\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe output report of GAStech_edges above reveals that the SentDate is treated as “Character” data type instead of date data type. This is an error! Before we continue, it is important for us to change the data type of SentDate field back to “Date”” data type.\n\n\n\n\n\n\nThe code chunk below will be used to perform the changes.\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\n\nboth dmy() and wday() are functions of lubridate package. lubridate is an R package that makes it easier to work with dates and times.\ndmy() transforms the SentDate to Date data type.\nwday() returns the day of the week as a decimal number or an ordered factor if label is TRUE. The argument abbr is FALSE keep the daya spells in full, i.e. Monday. The function will create a new column in the data.frame i.e. Weekday and the output of wday() will save in this newly created field.\nthe values in the Weekday field are in ordinal scale.\n\n\n\n\n\n\nTable below shows the data structure of the reformatted GAStech_edges data frame\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 10\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n$ SendDate    &lt;date&gt; 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-0…\n$ Weekday     &lt;ord&gt; Friday, Friday, Friday, Friday, Friday, Friday, Friday, Fr…\n\n\n\n\n\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation.\nIn view of this, the individual records will be aggregated the by date, senders, receivers, main subject and day of the week.\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\n\nfour functions from dplyr package are used.\nThey are: filter(), group(), summarise(), and ungroup().\nThe output data.frame is called GAStech_edges_aggregated.\nA new field called Weight has been added in GAStech_edges_aggregated.\n\n\n\n\n\n\nTable below shows the data structure of the reformatted GAStech_edges data frame\n\nglimpse(GAStech_edges_aggregated)\n\nRows: 1,372\nColumns: 4\n$ source  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ target  &lt;dbl&gt; 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6,…\n$ Weekday &lt;ord&gt; Sunday, Monday, Tuesday, Wednesday, Friday, Sunday, Monday, Tu…\n$ Weight  &lt;int&gt; 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5,…\n\n\n\n\n\n\nThis section, will cover how to create a graph data model by using tidygraph package. The package provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. tidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow.\nBefore getting started, the following 2 articles should be read:\n\nIntroducing tidygraph\ntidygraph 1.1 - A tidy hope\n\n\n\nTwo functions of tidygraph package can be used to create network objects, they are:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\n\n\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n\niris_tree &lt;- iris_tree %&gt;%\n  activate(nodes) %&gt;%\n  mutate(Species = ifelse(leaf, as.character(iris$Species)[label], NA)) %&gt;%\n  activate(edges) %&gt;%\n  mutate(to_setose = .N()$Species[to] == 'setosa')\niris_tree\n\nIn the above the .N() function is used to gain access to the node data while manipulating the edge data. Similarly .E() will give you the edge data and .G() will give you the tbl_graph object itself.\n\n\n\n\nIn this section, you will use tbl_graph() of tinygraph package to build an tidygraph’s network graph data.frame.\nBefore typing the codes, you are recommended to review to reference guide of tbl_graph()\n\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\n\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,372 × 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\n\n\n\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 4541 edges.\nThe command also prints the first six rows of “Node Data” and the first three of “Edge Data”.\nIt states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n\n\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, we could use activate() and then arrange().\nFor example,\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,372 × 4 (active)\n    from    to Weekday   Weight\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;      &lt;int&gt;\n 1    40    41 Saturday      13\n 2    41    43 Monday        11\n 3    35    31 Tuesday       10\n 4    40    41 Monday        10\n 5    40    43 Monday        10\n 6    36    32 Sunday         9\n 7    40    43 Saturday       9\n 8    41    40 Monday         9\n 9    19    15 Wednesday      8\n10    35    38 Tuesday        8\n# ℹ 1,362 more rows\n#\n# Node Data: 54 × 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows\n\n\n\n\n\n\nggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs.\nAs in all network graph, there are three main aspects to a ggraph’s network graph, they are:\n\nnodes,\nedges and\nlayouts.\n\nFor a comprehensive discussion of each of this aspect of graph, please refer to their respective vignettes provided.\n\n\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. Before your get started, it is advisable to read their respective reference guide at least once.\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\nThe basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired. Both of the arguments for ggraph() are built around igraph. Therefore, ggraph() can use either an igraph object or a tbl_graph object.\n\n\n\n\n\nIn this section, you will use theme_graph() to remove the x and y axes. Before your get started, it is advisable to read it’s reference guide at least once.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\nggraph introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. theme_graph(), besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden). The ggraph theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() in the individual plots.\n\n\n\n\n\nFurthermore, theme_graph() makes it easy to change the coloring of the plot.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph().\n\n\n\n\nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\n\n\n\nlayout argument is used to define the layout to be used.\n\n\n\n\n\n\nIn this section, you will colour each node by referring to their respective departments.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\n\n\nIn the code chunk below, the thickness of the edges will be mapped with the Weight variable.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunks above:\n\n\n\n\ngeom_edge_link draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line.\n\n\n\n\n\n\n\nAnother very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, you will learn how to use faceting technique to visualise network data.\nThere are three functions in ggraph to implement faceting, they are:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in al panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\n\nIn the code chunk below, facet_edges() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\nThe code chunk below uses theme() to change the position of the legend.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\nThe code chunk below adds frame to each graph.\n\nset_graph_style() \n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\nIn the code chunkc below, facet_nodes() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\n\nCentrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector. It is beyond the scope of this hands-on exercise to cover the principles and mathematics of these measure here. Students are encouraged to refer to Chapter 7: Actor Prominence of A User’s Guide to Network Analysis in R to gain better understanding of theses network measures.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\nmutate() of dplyr is used to perform the computation. the algorithm used, on the other hand, is the centrality_betweenness() of tidygraph.\n\n\n\n\n\nIt is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\ng &lt;- GAStech_graph %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph,\nIn the code chunk below group_edge_betweenness() is used.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\nvisNetwork() is a R package for network visualization, using vis.js javascript library.\nvisNetwork() function uses a nodes list and edges list to create an interactive graph.\nThe nodes list must include an “id” column, and the edge list must have “from” and “to” columns.\nThe function also plots the labels for the nodes, using the names of the actors from the “label” column in the node list.\nThe resulting graph is fun to play around with.\nYou can move the nodes and the graph will use an algorithm to keep the nodes properly spaced.\nYou can also zoom in and out on the plot and move it around to re-center it.\n\n\n\nBefore we can plot the interactive network graph, we need to prepare the data model by using the code chunk below.\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n    summarise(weight = n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\nThe code chunk below will be used to plot an interactive network graph by using the data prepared.\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n\n\n\n\nIn the code chunk below, Fruchterman and Reingold layout is used.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\nVisit Igraph to find out more about visIgraphLayout’s argument.\n\n\n\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field.\nThe code chunk below rename Department field to group.\n\nGAStech_nodes &lt;- GAStech_nodes %&gt;%\n  rename(group = Department) \n\nWhen we rerun the code chunk below, visNetwork shades the nodes by assigning unique colour to each category in the group field.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\nIn the code run below visEdges() is used to symbolise the edges.\n\nThe argument arrows is used to define where to place the arrow.\nThe smooth argument is used to plot the edges using a smooth curve.\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit edges to find out more about visEdges’s argument.\n\n\n\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation.\n\nThe argument highlightNearest highlights nearest when clicking a node.\nThe argument nodesIdSelection adds an id node selection creating an HTML select element.\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit Option to find out more about visOption’s argument."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#overview",
    "title": "Hands-on Ex6",
    "section": "",
    "text": "This hands-on exercise will cover how to model, analyse and visualise network data using R, with the following key areas:\n\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph,\nbuild network graph visualisation using appropriate functions of ggraph,\ncompute network geometrics using tidygraph,\nbuild advanced graph visualisation by incorporating the network geometrics, and\nbuild interactive network visualisation using visNetwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#getting-started",
    "title": "Hands-on Ex6",
    "section": "",
    "text": "In this hands-on exercise, 4 network data modelling and visualisation packages will be installed and launched. They are igraph, tidygraph, ggraph and visNetwork. Beside these 4 packages, tidyverse and lubridate, specially designed to handle and wrangle time data, will be installed and launched too.\n\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#the-data",
    "title": "Hands-on Ex6",
    "section": "",
    "text": "The data sets used in this hands-on exercise is from an oil exploration and extraction company. There are two data sets. One contains the nodes data and the other contains the edges (also know as link) data.\n\n\nIn this step, you will import GAStech_email_node.csv and GAStech_email_edges-v2.csv into RStudio environment by using read_csv() of readr package.\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\n\nGAStech-email_edges.csv which consists of two weeks of 9063 emails correspondances between 55 employees.\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\n\nGAStech_email_nodes.csv which consist of the names, department and title of the 55 employees\n\nglimpse(GAStech_nodes)\n\nRows: 54\nColumns: 4\n$ id         &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 44, 45, 46, 8, 9, 10, 11, 12, 13, 14, …\n$ label      &lt;chr&gt; \"Mat.Bramar\", \"Anda.Ribera\", \"Rachel.Pantanal\", \"Linda.Lago…\n$ Department &lt;chr&gt; \"Administration\", \"Administration\", \"Administration\", \"Admi…\n$ Title      &lt;chr&gt; \"Assistant to CEO\", \"Assistant to CFO\", \"Assistant to CIO\",…\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe output report of GAStech_edges above reveals that the SentDate is treated as “Character” data type instead of date data type. This is an error! Before we continue, it is important for us to change the data type of SentDate field back to “Date”” data type.\n\n\n\n\n\n\nThe code chunk below will be used to perform the changes.\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\n\nboth dmy() and wday() are functions of lubridate package. lubridate is an R package that makes it easier to work with dates and times.\ndmy() transforms the SentDate to Date data type.\nwday() returns the day of the week as a decimal number or an ordered factor if label is TRUE. The argument abbr is FALSE keep the daya spells in full, i.e. Monday. The function will create a new column in the data.frame i.e. Weekday and the output of wday() will save in this newly created field.\nthe values in the Weekday field are in ordinal scale.\n\n\n\n\n\n\nTable below shows the data structure of the reformatted GAStech_edges data frame\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 10\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n$ SendDate    &lt;date&gt; 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-0…\n$ Weekday     &lt;ord&gt; Friday, Friday, Friday, Friday, Friday, Friday, Friday, Fr…\n\n\n\n\n\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation.\nIn view of this, the individual records will be aggregated the by date, senders, receivers, main subject and day of the week.\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\n\nfour functions from dplyr package are used.\nThey are: filter(), group(), summarise(), and ungroup().\nThe output data.frame is called GAStech_edges_aggregated.\nA new field called Weight has been added in GAStech_edges_aggregated.\n\n\n\n\n\n\nTable below shows the data structure of the reformatted GAStech_edges data frame\n\nglimpse(GAStech_edges_aggregated)\n\nRows: 1,372\nColumns: 4\n$ source  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ target  &lt;dbl&gt; 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6,…\n$ Weekday &lt;ord&gt; Sunday, Monday, Tuesday, Wednesday, Friday, Sunday, Monday, Tu…\n$ Weight  &lt;int&gt; 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5,…"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#creating-network-objects-using-tidygraph",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#creating-network-objects-using-tidygraph",
    "title": "Hands-on Ex6",
    "section": "",
    "text": "This section, will cover how to create a graph data model by using tidygraph package. The package provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. tidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow.\nBefore getting started, the following 2 articles should be read:\n\nIntroducing tidygraph\ntidygraph 1.1 - A tidy hope\n\n\n\nTwo functions of tidygraph package can be used to create network objects, they are:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\n\n\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n\niris_tree &lt;- iris_tree %&gt;%\n  activate(nodes) %&gt;%\n  mutate(Species = ifelse(leaf, as.character(iris$Species)[label], NA)) %&gt;%\n  activate(edges) %&gt;%\n  mutate(to_setose = .N()$Species[to] == 'setosa')\niris_tree\n\nIn the above the .N() function is used to gain access to the node data while manipulating the edge data. Similarly .E() will give you the edge data and .G() will give you the tbl_graph object itself.\n\n\n\n\nIn this section, you will use tbl_graph() of tinygraph package to build an tidygraph’s network graph data.frame.\nBefore typing the codes, you are recommended to review to reference guide of tbl_graph()\n\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\n\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,372 × 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\n\n\n\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 4541 edges.\nThe command also prints the first six rows of “Node Data” and the first three of “Edge Data”.\nIt states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n\n\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, we could use activate() and then arrange().\nFor example,\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,372 × 4 (active)\n    from    to Weekday   Weight\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;      &lt;int&gt;\n 1    40    41 Saturday      13\n 2    41    43 Monday        11\n 3    35    31 Tuesday       10\n 4    40    41 Monday        10\n 5    40    43 Monday        10\n 6    36    32 Sunday         9\n 7    40    43 Saturday       9\n 8    41    40 Monday         9\n 9    19    15 Wednesday      8\n10    35    38 Tuesday        8\n# ℹ 1,362 more rows\n#\n# Node Data: 54 × 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#plotting-static-network-graphs-using-ggraph",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#plotting-static-network-graphs-using-ggraph",
    "title": "Hands-on Ex6",
    "section": "",
    "text": "ggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs.\nAs in all network graph, there are three main aspects to a ggraph’s network graph, they are:\n\nnodes,\nedges and\nlayouts.\n\nFor a comprehensive discussion of each of this aspect of graph, please refer to their respective vignettes provided.\n\n\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. Before your get started, it is advisable to read their respective reference guide at least once.\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\nThe basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired. Both of the arguments for ggraph() are built around igraph. Therefore, ggraph() can use either an igraph object or a tbl_graph object.\n\n\n\n\n\nIn this section, you will use theme_graph() to remove the x and y axes. Before your get started, it is advisable to read it’s reference guide at least once.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\nggraph introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. theme_graph(), besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden). The ggraph theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() in the individual plots.\n\n\n\n\n\nFurthermore, theme_graph() makes it easy to change the coloring of the plot.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph().\n\n\n\n\nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\n\n\n\nlayout argument is used to define the layout to be used.\n\n\n\n\n\n\nIn this section, you will colour each node by referring to their respective departments.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\n\n\nIn the code chunk below, the thickness of the edges will be mapped with the Weight variable.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunks above:\n\n\n\n\ngeom_edge_link draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#creating-facet-graphs",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#creating-facet-graphs",
    "title": "Hands-on Ex6",
    "section": "",
    "text": "Another very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, you will learn how to use faceting technique to visualise network data.\nThere are three functions in ggraph to implement faceting, they are:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in al panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\n\nIn the code chunk below, facet_edges() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\nThe code chunk below uses theme() to change the position of the legend.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\nThe code chunk below adds frame to each graph.\n\nset_graph_style() \n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\nIn the code chunkc below, facet_nodes() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#network-metrics-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#network-metrics-analysis",
    "title": "Hands-on Ex6",
    "section": "",
    "text": "Centrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector. It is beyond the scope of this hands-on exercise to cover the principles and mathematics of these measure here. Students are encouraged to refer to Chapter 7: Actor Prominence of A User’s Guide to Network Analysis in R to gain better understanding of theses network measures.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\nmutate() of dplyr is used to perform the computation. the algorithm used, on the other hand, is the centrality_betweenness() of tidygraph.\n\n\n\n\n\nIt is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\ng &lt;- GAStech_graph %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph,\nIn the code chunk below group_edge_betweenness() is used.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#building-interactive-network-graph-with-visnetwork",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#building-interactive-network-graph-with-visnetwork",
    "title": "Hands-on Ex6",
    "section": "",
    "text": "visNetwork() is a R package for network visualization, using vis.js javascript library.\nvisNetwork() function uses a nodes list and edges list to create an interactive graph.\nThe nodes list must include an “id” column, and the edge list must have “from” and “to” columns.\nThe function also plots the labels for the nodes, using the names of the actors from the “label” column in the node list.\nThe resulting graph is fun to play around with.\nYou can move the nodes and the graph will use an algorithm to keep the nodes properly spaced.\nYou can also zoom in and out on the plot and move it around to re-center it.\n\n\n\nBefore we can plot the interactive network graph, we need to prepare the data model by using the code chunk below.\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n    summarise(weight = n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\nThe code chunk below will be used to plot an interactive network graph by using the data prepared.\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n\n\n\n\nIn the code chunk below, Fruchterman and Reingold layout is used.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\nVisit Igraph to find out more about visIgraphLayout’s argument.\n\n\n\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field.\nThe code chunk below rename Department field to group.\n\nGAStech_nodes &lt;- GAStech_nodes %&gt;%\n  rename(group = Department) \n\nWhen we rerun the code chunk below, visNetwork shades the nodes by assigning unique colour to each category in the group field.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\nIn the code run below visEdges() is used to symbolise the edges.\n\nThe argument arrows is used to define where to place the arrow.\nThe smooth argument is used to plot the edges using a smooth curve.\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit edges to find out more about visEdges’s argument.\n\n\n\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation.\n\nThe argument highlightNearest highlights nearest when clicking a node.\nThe argument nodesIdSelection adds an id node selection creating an HTML select element.\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\nVisit Option to find out more about visOption’s argument."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html",
    "title": "In-class Ex 1",
    "section": "",
    "text": "p_load() of pacman package is used to load the tidyverse family of packages in the code chunk as follows.\n\npacman::p_load(tidyverse)\n\n\nrealis &lt;- read_csv(\"data/realis2019.csv\")\n\n\nggplot(data = realis,\n       aes(x = `Unit Price ($ psm)`)) +\n  geom_histogram()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#loading-r-packages",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#loading-r-packages",
    "title": "In-class Ex 1",
    "section": "",
    "text": "p_load() of pacman package is used to load the tidyverse family of packages in the code chunk as follows.\n\npacman::p_load(tidyverse)\n\n\nrealis &lt;- read_csv(\"data/realis2019.csv\")\n\n\nggplot(data = realis,\n       aes(x = `Unit Price ($ psm)`)) +\n  geom_histogram()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html",
    "title": "In-class Ex2",
    "section": "",
    "text": "The code chunk below installs and launches the tidyverse, ggdist, ggridges, colourspace & ggthemes packages into R environment\n\npacman:: p_load(tidyverse, ggdist, ggridges,\n                colourspace, ggthemes)\n\n\n\n\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package, which is part of the tidyverse package.\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#getting-started",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#getting-started",
    "title": "In-class Ex2",
    "section": "",
    "text": "The code chunk below installs and launches the tidyverse, ggdist, ggridges, colourspace & ggthemes packages into R environment\n\npacman:: p_load(tidyverse, ggdist, ggridges,\n                colourspace, ggthemes)\n\n\n\n\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package, which is part of the tidyverse package.\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#histogram",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#histogram",
    "title": "In-class Ex2",
    "section": "Histogram",
    "text": "Histogram\n\nggplot(data=exam, \n       aes(x = ENGLISH)) +\n  geom_histogram(\n    color = \"#1696d2\",\n  )"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#probability-density-plot",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#probability-density-plot",
    "title": "In-class Ex2",
    "section": "Probability Density plot",
    "text": "Probability Density plot\n\nggplot(data=exam, \n       aes(x = ENGLISH)) +\n  geom_density(\n    color = \"#1696d2\",\n    adjust = .65,\n    alpha = .6\n  )"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#alternative-design",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#alternative-design",
    "title": "In-class Ex2",
    "section": "Alternative design",
    "text": "Alternative design\nggplot(data=exam_df, aes(x = ENGLISH)) + geom_density( color = “#1696d2”, adjust = .65, alpha = .6 ) + stat_function( fun = dnorm, args = list(mean = mean_eng, sd = std_eng), col = “grey30”, size = .8 ) + geom_vline( aes(xintercept = mean_eng), colour=“4d5887”, linewidth = .6, linetype = “dashed” ) + annotate(geom = “text”, x = mean_eng - 8, y = 0.04, label = paste)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#visualising-distribution-with-ridgeline-plot",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#visualising-distribution-with-ridgeline-plot",
    "title": "In-class Ex2",
    "section": "Visualising Distribution with Ridgeline Plot",
    "text": "Visualising Distribution with Ridgeline Plot\n\nPlotting ridgeline graph: ggridges method\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = colorspace::lighten(\"#7097BB\", .3),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\nVarying fill colors along the x axis\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Temp. [F]\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\nMapping the probabilities directly onto colour\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n\n\n\n\n\nRidgeline plots with quantile lines\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n    ) +\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  theme_ridges()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#visualising-distribution-with-raincloud-plot",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#visualising-distribution-with-raincloud-plot",
    "title": "In-class Ex2",
    "section": "Visualising Distribution with Raincloud Plot",
    "text": "Visualising Distribution with Raincloud Plot\n\nPlotting a Half Eye graph\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA)\n\n\n\n\n\n\nAdding the boxplot with geom_boxplot()\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA)\n\n\n\n\n\n\nAdding the Dot Plots with stat_dots()\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)\n\n\n\n\n\n\nFinishing touch\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 1.5) +\n  coord_flip() +\n  theme_economist()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html",
    "title": "In-class Ex4",
    "section": "",
    "text": "Installing & loading required libraries\nThe code chunk below installs and launches the tidyverse, ggdist, ggridges, colourspace & ggthemes packages into R environment\n\npacman:: p_load(ggstatsplot, tidyverse)\n\n\n\nImporting the data\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package, which is part of the tidyverse package.\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\nset.seed(1234)\n\np &lt;- gghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  bin.args = list(color = \"black\",\n                  fill = \"grey50\",\n                  alpha = 0.7),\n                  normal.curve = TRUE,\n                  normal.curve.args = list(linewidth = 0.5),\n  xlab = \"English scores\"\n)\n\np\n\n\n\n\n\nextract_stats(p)\n\n$subtitle_data\n# A tibble: 1 × 16\n  term       effectsize      estimate conf.level conf.low conf.high    pd\n  &lt;chr&gt;      &lt;chr&gt;              &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Difference Bayesian t-test     7.16       0.95     5.54      8.75     1\n  prior.distribution prior.location prior.scale    bf10 method         \n  &lt;chr&gt;                       &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          \n1 cauchy                          0       0.707 4.54e13 Bayesian t-test\n  conf.method log_e_bf10 n.obs expression\n  &lt;chr&gt;            &lt;dbl&gt; &lt;int&gt; &lt;list&gt;    \n1 ETI               31.4   322 &lt;language&gt;\n\n$caption_data\nNULL\n\n$pairwise_comparisons_data\nNULL\n\n$descriptive_data\nNULL\n\n$one_sample_data\nNULL\n\n$tidy_data\nNULL\n\n$glance_data\nNULL\n\n\n\nggdotplotstats(\n  data = exam,\n  x = ENGLISH,\n  y = CLASS,\n  title = \"\",\n  xlab = \"\"\n)\n\n\n\n\nNote on the above, the y axis ‘CLASS’ categorical variable is arranged according to its x axis values rather than alphabetical order, eg. 3D is above 3A\n\nexam_long &lt;- exam %&gt;%\n  pivot_longer(\n    cols = ENGLISH: SCIENCE,\n    names_to = \"SUBJECT\",\n    values_to = \"SCORES\") %&gt;%\n  filter(CLASS == \"3A\")\n\n\nggwithinstats(\n  data = filter(exam_long,\n                SUBJECT %in%\n                  c(\"MATHS\", \"SCIENCE\")),\n  x = SUBJECT, \n  y = SCORES,\n  type=\"p\"\n)\n\n\n\n\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y= ENGLISH,\n  marginal = TRUE,\n  label.var = ID,\n  label.expression = ENGLISH &gt; 90 & MATHS &gt;90\n)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5.html",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5.html",
    "title": "In-class Ex5",
    "section": "",
    "text": "Installing & loading required libraries\nThe code chunk below installs and launches the tidyverse, ggdist, ggridges, colourspace & ggthemes packages into R environment\n\npacman:: p_load(tidyverse, readtext, tidytext, quanteda)\n\n\n\nImporting the data\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package, which is part of the tidyverse package.\n\ntext_data &lt;- readtext(\"data/articles/*\")\n\n\ncorpus_text &lt;- corpus(text_data)\nsummary(corpus_text,5)\n\nCorpus consisting of 338 documents, showing 5 documents:\n\n                                   Text Types Tokens Sentences\n Alvarez PLC__0__0__Haacklee Herald.txt   206    433        18\n    Alvarez PLC__0__0__Lomark Daily.txt   102    170        12\n   Alvarez PLC__0__0__The News Buoy.txt    90    200         9\n Alvarez PLC__0__1__Haacklee Herald.txt    96    187         8\n    Alvarez PLC__0__1__Lomark Daily.txt   241    504        21\n\n\n\nusenet_words &lt;- text_data %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  filter(str_detect(word,\"[a-z']$\"),\n         !word %in% stop_words$word)\n\n\nusenet_words %&gt;%\n  count(word,sort=TRUE)\n\nreadtext object consisting of 3261 documents and 0 docvars.\n# A data frame: 3,261 × 3\n  word             n text     \n  &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;    \n1 fishing       2177 \"\\\"\\\"...\"\n2 sustainable   1525 \"\\\"\\\"...\"\n3 company       1036 \"\\\"\\\"...\"\n4 practices      838 \"\\\"\\\"...\"\n5 industry       715 \"\\\"\\\"...\"\n6 transactions   696 \"\\\"\\\"...\"\n# ℹ 3,255 more rows\n\n\n\ntext_data_splitted &lt;- text_data %&gt;%\n  separate_wider_delim(\"doc_id\",\n                       delim = \"__0__\",\n                       names = c(\"X\", \"Y\"),\n                       too_few = \"align_end\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex5/In-class_Ex5a.html",
    "href": "In-class_Ex/In-class_Ex5/In-class_Ex5a.html",
    "title": "In-class Ex5a",
    "section": "",
    "text": "Installing & loading required libraries\nThe code chunk below installs and launches the tidyverse, ggdist, ggridges, colourspace & ggthemes packages into R environment\n\npacman:: p_load(jsonlite, tidyverse, tidygraph, ggraph)\n\n\n\nImporting the data\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package, which is part of the tidyverse package.\n\nmc1_data &lt;- fromJSON(\"data/mc1.json\")\n\n\nmc2_data &lt;- fromJSON(\"data/mc2.json\")\n\nmc3_data &lt;- fromJSON(\"data/mc3.json\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html",
    "title": "In-class Ex6",
    "section": "",
    "text": "The code chunk below installs and launches the tidyverse, ggdist, ggridges, colourspace & ggthemes packages into R environment\n\npacman:: p_load(jsonlite, tidyverse, tidygraph, ggraph, corporaexplorer, stringi, rvest, readtext, quanteda)\n\n\n\n\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package, which is part of the tidyverse package.\n\nbible &lt;- readr::read_lines(\"http://www.gutenberg.org/cache/epub/10/pg10.txt\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#getting-started",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#getting-started",
    "title": "In-class Ex6",
    "section": "",
    "text": "The code chunk below installs and launches the tidyverse, ggdist, ggridges, colourspace & ggthemes packages into R environment\n\npacman:: p_load(jsonlite, tidyverse, tidygraph, ggraph, corporaexplorer, stringi, rvest, readtext, quanteda)\n\n\n\n\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package, which is part of the tidyverse package.\n\nbible &lt;- readr::read_lines(\"http://www.gutenberg.org/cache/epub/10/pg10.txt\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#pre-processing-the-text",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#pre-processing-the-text",
    "title": "In-class Ex6",
    "section": "Pre-processing the text",
    "text": "Pre-processing the text\n# Collapsing into one string.\n\nbible &lt;- paste(bible, collapse = \"\\n\")\n\n# Identifying the beginning and end of the Bible / stripping PJ metadata # (technique borrowed from https://quanteda.io/articles/pkgdown/replication/digital-humanities.html).\n\nstart_v &lt;- stri_locate_first_fixed(bible, \"The First Book of Moses: Called Genesis\")[1]\nend_v &lt;- stri_locate_last_fixed(bible, \"Amen.\")[2]\nbible &lt;- stri_sub(bible, start_v, end_v)\n\n# In the file, every book in the bible is preceded by five newlines, # which we use to split our string into a vector where each element is a book.\n\nbooks &lt;- stri_split_regex(bible, \"\\n{5}\") %&gt;%\n    unlist %&gt;%\n    .[-40]\n\n# Removing the heading “The New Testament of the King James Bible”, # which also was preceded by five newlines.\n# Because of the structure of the text in the file: # Replacing double or more newlines with two newlines, and a single newline with space.\n\nbooks &lt;- str_replace_all(books, \"\\n{2,}\", \"NEW_PARAGRAPH\") %&gt;%\n    str_replace_all(\"\\n\", \" \") %&gt;%\n    str_replace_all(\"NEW_PARAGRAPH\", \"\\n\\n\")\nbooks &lt;- books[3:68] \n\n# The two first elements are not books\n# Identifying new chapters within each book and split the text into chapters. # (The first characters in chapter 2 will e.g. be 2:1)\n\nchapters &lt;- str_replace_all(books, \"(\\\\d+:1 )\", \"NEW_CHAPTER\\\\1\") %&gt;%\n    stri_split_regex(\"NEW_CHAPTER\")\n\n# Removing the chapter headings from the text (we want them as metadata).\n\nchapters &lt;- lapply(chapters, function(x) x[-1])"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#metadata",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#metadata",
    "title": "In-class Ex6",
    "section": "Metadata",
    "text": "Metadata\n# We are not quite happy with the long book titles in the King James Bible, # so we retrieve shorter versions from esv.org which will take up less # space in the corpus map plot.\n\nbook_titles &lt;- read_html(\"https://www.esv.org/resources/esv-global-study-bible/list-of-abbreviations\") %&gt;%\n  html_nodes(\"td:nth-child(1)\") %&gt;%\n  html_text() %&gt;%\n  .[13:78]\n\n# Removing irrelevant elements after manual inspection.\n# We add a column indicating whether a book belongs to the Old or New Testament, # knowing that they contain respectively 39 and 27 books.\n\ntestament &lt;- c(rep(\"Old\", 39), rep(\"New\", 27))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#creating-data-frame-with-text-and-metadata",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#creating-data-frame-with-text-and-metadata",
    "title": "In-class Ex6",
    "section": "Creating data frame with text and metadata",
    "text": "Creating data frame with text and metadata\n# Data frame with one book as one row.\n\nbible_df &lt;- tibble::tibble(Text = chapters,\n                           Book = book_titles,\n                           Testament = testament)\n\n# We want each chapter to be one row, but keep the metadata (book and which testament).\n\nbible_df &lt;- tidyr::unnest(bible_df, Text)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#using-corporaexplorer",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#using-corporaexplorer",
    "title": "In-class Ex6",
    "section": "Using corporaexplorer",
    "text": "Using corporaexplorer\nWhen we first have a data frame with text and metadata, creating a “corporaexplorerobject” for exploration is very simple:\n# As this is a corpus which is not organised by date, # we set date_based_corpus to FALSE. # Because we want to organise our exploration around the books in the Bible, # we pass \"Book\" to the grouping_variable argument. # We specify which metadata columns we want to be displayed in the # “Document information” tab, using the columns_doc_info argument.\n\nKJB &lt;- prepare_data(dataset = bible_df,\n                    date_based_corpus = FALSE,\n                    grouping_variable = \"Book\",\n                    columns_doc_info = c(\"Testament\", \"Book\"))\n\n\nclass(KJB)\n\n[1] \"corporaexplorerobject\"\n\n\n\nexplore(KJB)\n\nShiny applications not supported in static R Markdown documents"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6a.html",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6a.html",
    "title": "In-class Ex6a",
    "section": "",
    "text": "The code chunk below installs and launches the tidyverse, ggdist, ggridges, colourspace & ggthemes packages into R environment\n\npacman:: p_load(jsonlite, tidyverse, tidygraph, ggraph, visNetwork, graphlayouts, skimr)\n\n\n\n\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package, which is part of the tidyverse package.\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package, which is part of the tidyverse package.\n\nmc3_data &lt;- fromJSON(\"data/mc3_old.json\")\n\n\nclass(mc3_data)\n\n[1] \"list\"\n\n\n\nmc3_edges &lt;- as_tibble(mc3_data$links) %&gt;%\n  distinct() %&gt;%\n  mutate(source = as.character(source), target = as.character(target), type = as.character(type)) %&gt;%\n  group_by(source, target, type) %&gt;%\n  summarise(weights=n()) %&gt;%\n  filter(source!=target) %&gt;%\n  ungroup()\n\n\nmc3_nodes &lt;- as_tibble(mc3_data$nodes) %&gt;%\n  mutate(country = as.character(country), id = as.character(id), product_services = as.character(product_services), revenue_omu = as.numeric(as.character(revenue_omu)), type = as.character(type)) %&gt;%\n  select(id, country, product_services, revenue_omu, type)\n\n\nid1 &lt;- mc3_edges %&gt;%\n  select(source) %&gt;%\n  rename(id = source)\nid2 &lt;- mc3_edges %&gt;%\n  select(target) %&gt;%\n  rename(id = target)\nmc3_nodes1 &lt;- rbind(id1, id2) %&gt;%\n  distinct() %&gt;%\n  left_join(mc3_nodes,\n            unmatched = \"drop\")\n\n\nmc3_graph &lt;- tbl_graph(nodes = mc3_nodes1,\n                       edges = mc3_edges,\n                       directed = FALSE) %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness(), closeness_centrality = centrality_closeness())\n\n\nmc3_graph %&gt;%\n  filter(betweenness_centrality &gt;= 300000) %&gt;%\n  ggraph(layout = 'fr') + geom_edge_link(aes(alpha = 0.5)) +\n  geom_node_point(aes(\n    size = betweenness_centrality,\n    colors = \"lightblue\",\n    alpha = 0.5)) +\n  scale_size_continuous(range=c(1,10))+\n  theme_graph()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6a.html#getting-started",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6a.html#getting-started",
    "title": "In-class Ex6a",
    "section": "",
    "text": "The code chunk below installs and launches the tidyverse, ggdist, ggridges, colourspace & ggthemes packages into R environment\n\npacman:: p_load(jsonlite, tidyverse, tidygraph, ggraph, visNetwork, graphlayouts, skimr)\n\n\n\n\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package, which is part of the tidyverse package.\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package, which is part of the tidyverse package.\n\nmc3_data &lt;- fromJSON(\"data/mc3_old.json\")\n\n\nclass(mc3_data)\n\n[1] \"list\"\n\n\n\nmc3_edges &lt;- as_tibble(mc3_data$links) %&gt;%\n  distinct() %&gt;%\n  mutate(source = as.character(source), target = as.character(target), type = as.character(type)) %&gt;%\n  group_by(source, target, type) %&gt;%\n  summarise(weights=n()) %&gt;%\n  filter(source!=target) %&gt;%\n  ungroup()\n\n\nmc3_nodes &lt;- as_tibble(mc3_data$nodes) %&gt;%\n  mutate(country = as.character(country), id = as.character(id), product_services = as.character(product_services), revenue_omu = as.numeric(as.character(revenue_omu)), type = as.character(type)) %&gt;%\n  select(id, country, product_services, revenue_omu, type)\n\n\nid1 &lt;- mc3_edges %&gt;%\n  select(source) %&gt;%\n  rename(id = source)\nid2 &lt;- mc3_edges %&gt;%\n  select(target) %&gt;%\n  rename(id = target)\nmc3_nodes1 &lt;- rbind(id1, id2) %&gt;%\n  distinct() %&gt;%\n  left_join(mc3_nodes,\n            unmatched = \"drop\")\n\n\nmc3_graph &lt;- tbl_graph(nodes = mc3_nodes1,\n                       edges = mc3_edges,\n                       directed = FALSE) %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness(), closeness_centrality = centrality_closeness())\n\n\nmc3_graph %&gt;%\n  filter(betweenness_centrality &gt;= 300000) %&gt;%\n  ggraph(layout = 'fr') + geom_edge_link(aes(alpha = 0.5)) +\n  geom_node_point(aes(\n    size = betweenness_centrality,\n    colors = \"lightblue\",\n    alpha = 0.5)) +\n  scale_size_continuous(range=c(1,10))+\n  theme_graph()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608-VAA",
    "section": "",
    "text": "Welcome to ISSS608 Visual Analytics and Applications homepage. In this website, you will find my coursework prepared for this course."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html",
    "title": "Take-home Ex1",
    "section": "",
    "text": "The private residential market in Singapore is a dynamic landscape, influenced by intricate socioeconomic factors and market trends. Separate from public housing, this sector has a unique set of policies and caters to a different market segment that comprises households with monthly incomes exceeding S$14,000, which includes not only Singaporean but also foreign buyers. Understanding this market’s nuances is crucial for both policymakers and potential market participants, whom are potential clients and consumers of our media company.\nThis study would thus delve into the private residential market transactions in Singapore during the first quarter of 2024 and its relations to past quarters. Through data visualisation and analytics, this study would scrutinize market trends & patterns, geographical distributions and other potential interacting factors, aiming to unravel its complexities and provide insights on behaviour of Singapore’s private residential market and sub markets."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#introduction",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#introduction",
    "title": "Take-home Ex1",
    "section": "",
    "text": "The private residential market in Singapore is a dynamic landscape, influenced by intricate socioeconomic factors and market trends. Separate from public housing, this sector has a unique set of policies and caters to a different market segment that comprises households with monthly incomes exceeding S$14,000, which includes not only Singaporean but also foreign buyers. Understanding this market’s nuances is crucial for both policymakers and potential market participants, whom are potential clients and consumers of our media company.\nThis study would thus delve into the private residential market transactions in Singapore during the first quarter of 2024 and its relations to past quarters. Through data visualisation and analytics, this study would scrutinize market trends & patterns, geographical distributions and other potential interacting factors, aiming to unravel its complexities and provide insights on behaviour of Singapore’s private residential market and sub markets."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#objectives",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#objectives",
    "title": "Take-home Ex1",
    "section": "Objectives",
    "text": "Objectives\nThe key objective of this study are to leverage on visual analytics in the following:\n\nData Visualisation 1: To explore the distribution of unit sales over of property classification variables\nData Visualisation 2: To discover the prevalence of these distribution patterns over the past quarters\nData Visualisation 3: To estimate the correlation between transacted price per area and potential interacting factors"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#getting-started",
    "title": "Take-home Ex1",
    "section": "Getting Started",
    "text": "Getting Started\nThe code chunk below loads the following libraries:\n\ntidyverse: an amalgamation of libraries for data handling (including ggplot2, dplyr, tidyr, readr, tibble)\nknitr: for creating dynamic html tables/reports\nggridges: extension of ggplot2 designed for plotting ridgeline plots\nggdist: extension of ggplot2 designed for visualising distribution and uncertainty,\ncolorspace: provides a broad toolbox for selecting individual colors or color palettes, manipulating these colors, and employing them in various kinds of visualisations.\nggrepel: provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: provides additional themes, geoms, and scales for ggplot package\nhrbrthemes: provides typography-centric themes and theme components for ggplot package\npatchwork: preparing composite figure created using ggplot package\nlubridate: for wrangling of date-time data\n\n\npacman::p_load(tidyverse, knitr, ggridges, ggdist, colorspace, ggrepel, ggthemes, hrbrthemes, patchwork, lubridate)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#data-preparation",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#data-preparation",
    "title": "Take-home Ex1",
    "section": "Data Preparation",
    "text": "Data Preparation\nThe main dataset, extracted from URA Realis database, contains the private residential sales transactions in Singapore in the first quarter of 2024 (2024 Q1). In addition, this study will also perform comparisons with the preceding 4 quarters, which is in fact 2023 Q1-Q4, similarly extracted from the URA Realis database.\n\nImporting the data\nThe code chunk below imports 5 quarters of Private Residential Transaction data into R environment by using read_csv() function of readr package, which is part of the tidyverse package.\n\nRealis24Q1 &lt;- read_csv(\"data/ResidentialTrx2024q1.csv\")\nRealis23Q4 &lt;- read_csv(\"data/ResidentialTrx2023q4.csv\")\nRealis23Q3 &lt;- read_csv(\"data/ResidentialTrx2023q3.csv\")\nRealis23Q2 &lt;- read_csv(\"data/ResidentialTrx2023q2.csv\")\nRealis23Q1 &lt;- read_csv(\"data/ResidentialTrx2023Q1.csv\")\n\n\nRealis_merged &lt;- rbind(Realis24Q1, Realis23Q4, Realis23Q3, Realis23Q2, Realis23Q1)\n\n\nData Variables\nTo get a better understanding of the variables in the original dataset, the glimpse() function is used in the following code chunks.\n\nglimpse(Realis24Q1)\n\nRows: 4,902\nColumns: 21\n$ `Project Name`                &lt;chr&gt; \"THE LANDMARK\", \"POLLEN COLLECTION\", \"SK…\n$ `Transacted Price ($)`        &lt;dbl&gt; 2726888, 3850000, 2346000, 2190000, 1954…\n$ `Area (SQFT)`                 &lt;dbl&gt; 1076.40, 1808.35, 1087.16, 807.30, 796.5…\n$ `Unit Price ($ PSF)`          &lt;dbl&gt; 2533, 2129, 2158, 2713, 2453, 2577, 838,…\n$ `Sale Date`                   &lt;chr&gt; \"1-Jan-24\", \"1-Jan-24\", \"1-Jan-24\", \"1-J…\n$ Address                       &lt;chr&gt; \"173 CHIN SWEE ROAD #22-11\", \"34 POLLEN …\n$ `Type of Sale`                &lt;chr&gt; \"New Sale\", \"New Sale\", \"New Sale\", \"New…\n$ `Type of Area`                &lt;chr&gt; \"Strata\", \"Land\", \"Strata\", \"Strata\", \"S…\n$ `Area (SQM)`                  &lt;dbl&gt; 100.0, 168.0, 101.0, 75.0, 74.0, 123.0, …\n$ `Unit Price ($ PSM)`          &lt;dbl&gt; 27269, 22917, 23228, 29200, 26405, 27741…\n$ `Nett Price($)`               &lt;chr&gt; \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", …\n$ `Property Type`               &lt;chr&gt; \"Condominium\", \"Terrace House\", \"Apartme…\n$ `Number of Units`             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Tenure                        &lt;chr&gt; \"99 yrs from 28/08/2020\", \"99 yrs from 0…\n$ `Completion Date`             &lt;chr&gt; \"Uncompleted\", \"Uncompleted\", \"Uncomplet…\n$ `Purchaser Address Indicator` &lt;chr&gt; \"Private\", \"N.A\", \"HDB\", \"N.A\", \"Private…\n$ `Postal Code`                 &lt;chr&gt; \"169878\", \"807233\", \"469657\", \"118992\", …\n$ `Postal District`             &lt;chr&gt; \"03\", \"28\", \"16\", \"05\", \"21\", \"21\", \"28\"…\n$ `Postal Sector`               &lt;chr&gt; \"16\", \"80\", \"46\", \"11\", \"59\", \"58\", \"79\"…\n$ `Planning Region`             &lt;chr&gt; \"Central Region\", \"North East Region\", \"…\n$ `Planning Area`               &lt;chr&gt; \"Outram\", \"Serangoon\", \"Bedok\", \"Queenst…\n\n\n\nglimpse(Realis_merged)\n\nRows: 26,806\nColumns: 21\n$ `Project Name`                &lt;chr&gt; \"THE LANDMARK\", \"POLLEN COLLECTION\", \"SK…\n$ `Transacted Price ($)`        &lt;dbl&gt; 2726888, 3850000, 2346000, 2190000, 1954…\n$ `Area (SQFT)`                 &lt;dbl&gt; 1076.40, 1808.35, 1087.16, 807.30, 796.5…\n$ `Unit Price ($ PSF)`          &lt;dbl&gt; 2533, 2129, 2158, 2713, 2453, 2577, 838,…\n$ `Sale Date`                   &lt;chr&gt; \"1-Jan-24\", \"1-Jan-24\", \"1-Jan-24\", \"1-J…\n$ Address                       &lt;chr&gt; \"173 CHIN SWEE ROAD #22-11\", \"34 POLLEN …\n$ `Type of Sale`                &lt;chr&gt; \"New Sale\", \"New Sale\", \"New Sale\", \"New…\n$ `Type of Area`                &lt;chr&gt; \"Strata\", \"Land\", \"Strata\", \"Strata\", \"S…\n$ `Area (SQM)`                  &lt;dbl&gt; 100.0, 168.0, 101.0, 75.0, 74.0, 123.0, …\n$ `Unit Price ($ PSM)`          &lt;dbl&gt; 27269, 22917, 23228, 29200, 26405, 27741…\n$ `Nett Price($)`               &lt;chr&gt; \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", …\n$ `Property Type`               &lt;chr&gt; \"Condominium\", \"Terrace House\", \"Apartme…\n$ `Number of Units`             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Tenure                        &lt;chr&gt; \"99 yrs from 28/08/2020\", \"99 yrs from 0…\n$ `Completion Date`             &lt;chr&gt; \"Uncompleted\", \"Uncompleted\", \"Uncomplet…\n$ `Purchaser Address Indicator` &lt;chr&gt; \"Private\", \"N.A\", \"HDB\", \"N.A\", \"Private…\n$ `Postal Code`                 &lt;chr&gt; \"169878\", \"807233\", \"469657\", \"118992\", …\n$ `Postal District`             &lt;chr&gt; \"03\", \"28\", \"16\", \"05\", \"21\", \"21\", \"28\"…\n$ `Postal Sector`               &lt;chr&gt; \"16\", \"80\", \"46\", \"11\", \"59\", \"58\", \"79\"…\n$ `Planning Region`             &lt;chr&gt; \"Central Region\", \"North East Region\", \"…\n$ `Planning Area`               &lt;chr&gt; \"Outram\", \"Serangoon\", \"Bedok\", \"Queenst…\n\n\nBased on the output above, the both the 2024 Q1 and merged datasets have a total of 21 variables, as well as 4.9k and 26.8k rows respectively.\n\n\n\nData Cleaning\nBefore data transformation, the cleanliness of the data set is first ascertained by checking for missing and duplicate data.\n\nMissing Data\nBased on the output from Glimpse(), it is noted that there were missing values denoted by “-” for variables such as Nett Price, hence, these values are first converted to NA before we check for NA values as a whole. Noting that there are valid values such as dates that also contain “-” and multibyte values such as ENCHANTe~ in the project name col, the code uses regex to filter for “-” values instead.\ncolSums() and is.NA() functions are thus used to search for missing values as a whole for the 2024 Q1 dataset.\n\n#Convert \"-\" values with no characters before and after to NA\nRealis24Q1 &lt;- Realis24Q1 %&gt;%\n  mutate_all(~ ifelse(grepl(\"^-$\", .), NA, .))\n\n#Find the number of missing values for each col\ncolSums(is.na(Realis24Q1))\n\n               Project Name        Transacted Price ($) \n                          0                           0 \n                Area (SQFT)          Unit Price ($ PSF) \n                          0                           0 \n                  Sale Date                     Address \n                          0                           0 \n               Type of Sale                Type of Area \n                          0                           0 \n                 Area (SQM)          Unit Price ($ PSM) \n                          0                           0 \n              Nett Price($)               Property Type \n                       4896                           0 \n            Number of Units                      Tenure \n                          0                           0 \n            Completion Date Purchaser Address Indicator \n                        118                           0 \n                Postal Code             Postal District \n                          0                           0 \n              Postal Sector             Planning Region \n                          0                           0 \n              Planning Area \n                          0 \n\n\nThe same step is repeated for the merged data set in the code chunk below.\n\n#Convert \"-\" values with no characters before and after to NA\nRealis_merged &lt;- Realis_merged %&gt;%\n  mutate_all(~ ifelse(grepl(\"^-$\", .), NA, .))\n\n#Find the number of missing values for each col\ncolSums(is.na(Realis_merged))\n\n               Project Name        Transacted Price ($) \n                          0                           0 \n                Area (SQFT)          Unit Price ($ PSF) \n                          0                           0 \n                  Sale Date                     Address \n                          0                           0 \n               Type of Sale                Type of Area \n                          0                           0 \n                 Area (SQM)          Unit Price ($ PSM) \n                          6                           0 \n              Nett Price($)               Property Type \n                      26770                           0 \n            Number of Units                      Tenure \n                          0                           0 \n            Completion Date Purchaser Address Indicator \n                        682                           0 \n                Postal Code             Postal District \n                          0                           0 \n              Postal Sector             Planning Region \n                          0                           0 \n              Planning Area \n                          0 \n\n\nFrom the above, the 2 variables of Nett Price ($) and Completion date contain missing values.\n\n\nCheck for duplicates\nUsing duplicated(), duplicate values in the 2024 Q1 data set of are identified and extracted in the following code chunk.\n\nRealis24Q1[duplicated(Realis24Q1), ]\n\n# A tibble: 0 × 21\n# ℹ 21 variables: Project Name &lt;chr&gt;, Transacted Price ($) &lt;dbl&gt;,\n#   Area (SQFT) &lt;dbl&gt;, Unit Price ($ PSF) &lt;dbl&gt;, Sale Date &lt;chr&gt;,\n#   Address &lt;chr&gt;, Type of Sale &lt;chr&gt;, Type of Area &lt;chr&gt;, Area (SQM) &lt;dbl&gt;,\n#   Unit Price ($ PSM) &lt;dbl&gt;, Nett Price($) &lt;chr&gt;, Property Type &lt;chr&gt;,\n#   Number of Units &lt;dbl&gt;, Tenure &lt;chr&gt;, Completion Date &lt;chr&gt;,\n#   Purchaser Address Indicator &lt;chr&gt;, Postal Code &lt;chr&gt;,\n#   Postal District &lt;chr&gt;, Postal Sector &lt;chr&gt;, Planning Region &lt;chr&gt;, …\n\n\nThe same step is repeated with the merged data set.\n\nRealis_merged[duplicated(Realis_merged), ]\n\n# A tibble: 0 × 21\n# ℹ 21 variables: Project Name &lt;chr&gt;, Transacted Price ($) &lt;dbl&gt;,\n#   Area (SQFT) &lt;dbl&gt;, Unit Price ($ PSF) &lt;dbl&gt;, Sale Date &lt;chr&gt;,\n#   Address &lt;chr&gt;, Type of Sale &lt;chr&gt;, Type of Area &lt;chr&gt;, Area (SQM) &lt;dbl&gt;,\n#   Unit Price ($ PSM) &lt;dbl&gt;, Nett Price($) &lt;chr&gt;, Property Type &lt;chr&gt;,\n#   Number of Units &lt;dbl&gt;, Tenure &lt;chr&gt;, Completion Date &lt;chr&gt;,\n#   Purchaser Address Indicator &lt;chr&gt;, Postal Code &lt;chr&gt;,\n#   Postal District &lt;chr&gt;, Postal Sector &lt;chr&gt;, Planning Region &lt;chr&gt;, …\n\n\nFrom the above, there were no duplicates found.\n\n\nCheck for outliers\nAs a pre-cursor to visualise the distribution of unit sales and correlation of property classification factors with Unit Price (PSF), a univariate analysis is done using a box plot to identify the existence of outliers in both dependent variables - (1) No. of Units and (2) Unit Price (PSF) as follows.\n\nNo. of units\n\n\nRealis 2024 Q1 datasetRealis merged dataset\n\n\n\n\nCode\n# Calculate frequency counts for each value of \"Number of Units\"\nfreq_counts &lt;- Realis24Q1 %&gt;%\n  count(`Number of Units`)\n\n# Plot the histogram and Display frequency counts as text\nggplot(data = Realis24Q1, aes(x = `Number of Units`)) +\n  geom_histogram() +\n  geom_text(data = freq_counts, aes(label = n, y = n), vjust = -0.5) +\n  labs(x = \"Number of Units\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nCode\n# Calculate frequency counts for each value of \"Number of Units\"\nfreq_counts &lt;- Realis_merged %&gt;%\n  count(`Number of Units`)\n\n# Plot the histogram and Display frequency counts as text\nggplot(data = Realis_merged, aes(x = `Number of Units`)) +\n  geom_histogram() +\n  geom_text(data = freq_counts, aes(label = n, y = n), vjust = -0.5) +\n  labs(x = \"Number of Units\", y = \"Frequency\")\n\n\n\n\n\n\n\n\nFrom the above, the data is extremely concentrated at 1 unit per transaction, with only a very small minority of less than 1% of the units that belonged to multiple-unit transactions.\n\nUnit Price (PSF)\n\n\nRealis 2024 Q1 datasetRealis merged dataset\n\n\n\n\nCode\n# Plot the box plot\nggplot(data = Realis24Q1, aes(x = `Unit Price ($ PSF)`, y=\" \")) +\n  geom_boxplot() +\n  labs(x = \"Unit Price ($ PSF)\", y=\"Realis 2024 Q1\")\n\n\n\n\n\n\n\n\n\nCode\n# Plot the box plot\nggplot(data = Realis_merged, aes(x = `Unit Price ($ PSF)`, y=\" \")) +\n  geom_boxplot() +\n  labs(x = \"Unit Price ($ PSF)\", y=\"Realis merged\")\n\n\n\n\n\n\n\n\nFrom the above, outliers are not particularly distinct as there are quite a sizable number trailing out from the right tail of the box plot.\n\n\n\nData Transformation\nBased on the findings above, the significance of each variable, the conclusions drawn and transformations required are summarised as follows.\n\n\n\n\n\n\n\n\n\n\n\nS/N\nVariable\nData Type\nDescription\nRemarks\n\n\n\n1\nProject Name\nchr\nPrivate Residential Development Project Name\nNo missing values suggests that all property transactions in the data set belong to a private project development\n\n\n\n2\nTransacted Price ($)\nnum\nTransacted Property Sale Price\nSince price is already known to be strongly correlated to Area, Unit Price (aka Price per Area) will potentially provide greater insights to multiple stakeholders.\n\n\n\n3\nArea (SQFT)\nnum\nProperty Floor Area (aka Unit Size) in SQFT\nSince price is already known to be strongly correlated to Area, Unit Price (aka Price per Area) will potentially provide greater insights to multiple stakeholders.\n\n\n\n4\nUnit Price ($ PSF)\nnum\nSale Price per Floor Area in SQFT\nPSF is chosen over PSM as a indicator for Price per area as a dependent variable for Data Visualisation 3.\n\n\n\n5\nSale Date\nchr\nDate of Sales Transaction\nSale date will be required for Data Visualisation 2 to provide granularity on time-wise trends across the quarters.\nData type will need to be changed to Date instead of chr.\n\n\n\n6\nAddress\nchr\nDetailed line of address\nAddress is closely related to other geographical variables, but is not selected as the preferred variable to be used in this study.\n\n\n\n7\nType of Sale\nchr\nDifferentiates between\n\nsales of a new property (directly from the developer)\nsecondhand sales of unbuilt property (aka subsale)\nresale of existing property\n\nType of sale will be a property classification and potential interacting variable that will be used in Data Visualisation 1, 2 and 3\n\n\n\n8\nType of Area\nchr\nDifferentiates whether the land under the property is:\n\nshared in a development project (strata)\nsolely owned by the property owner\n\nType of Area is closely related to property type, but is not selected as the preferred variable to be used in this study.\n\n\n\n9\nArea (SQM)\nnum\nProperty Floor Area (aka Unit Size) in SQM\nSince price is already known to be strongly correlated to Area, Price per Area will potentially provide greater insights to multiple stakeholders.\n\n\n\n10\nUnit Price ($ PSM)\nnum\nSale Price per Floor Area in SQM\nPSF is chosen over PSM as a indicator for Price per area as a dependent variable for Data visualisation 3.\n\n\n\n11\nNett Price($)\nchr\nPrice taking away officially recorded discounts and grants by URA\nMissing Values found for this variable in majority of rows, and since it has a similar purpose as transaction price, the latter will be used instead.\n\n\n\n12\nProperty Type\nchr\nDifferentiates the property type into:\n\nCondominium\nTerrace\nSemi-detached\nDetached\n\nProperty Type will be a property classification and potential interacting variable that will be used in Data Visualisation 1, 2 and 3\n\n\n\n13\nNumber of Units\nnum\nNumber of units sold in the single transaction\nNumber of units is chosen as the dependent variable for Data Visualisation 1 and 2 to explore the distribution of unit sales by property classifications.\nTo minimise complexity in the visualisation, the multiple-unit transactions which constitutes a small minority of the data, will be filtered away as outliers, which could be revisited in a future study.\n\n\n\n14\nTenure\nchr\nTenure period granted to the property owner over the land starting from the date of completion\nTenure will be a property classification and potential interacting variable that will be used in Data Visualisation 1, 2 and 3\nTo contain the wide variation in representations, the values will be binned such that all lease years less than 900 years will be classified as “Leasehold”, while values above will be classified as “999years/Freehold”\n\n\n\n15\nCompletion Date\nchr\nYear when the first building project is deemed completed upon grant of the tenure of the land.\nThe age of the property can be derived from this variable which will be a property classification and potential interacting variable that will be used in Data Visualisation 1, 2 and 3, however, missing values were found for this variable in a small minority of rows.\nHence data transformation is required to remove rows with missing values, derive property age with 2024 as a reference point, and replace “Uncompleted” properties as 0 in age.\n\n\n\n16\nPurchaser Address Indicator\nchr\nType of residence of the purchaser’s address which he/she need not necessarily own\nThis could be a variable to determine if the purchaser is upgrading, downgrading or perhaps buying a 2nd property, but since there isn’t enough info on whether the buyer is actually selling their current property, significant assumptions need to be made.\n\n\n\n17\nPostal Code\nchr\nGeographical code which is unique to the building address of transacted property\nPostal Code is closely related to other geographical variables, but is not selected as the preferred variable to be used in this study.\n\n\n\n18\nPostal District\nchr\nGeographical code which is unique to the postal district of transacted property, total of 28 districts in Singapore\nPostal District is closely related to other geographical variables, but is not selected as the preferred variable to be used in this study.\n\n\n\n19\nPostal Sector\nchr\nGeographical code unique to postal sector of transacted property, which is a sub area of district, total of 81 sectors in Singapore\nPostal Sector is closely related to other geographical variables, but is not selected as the preferred variable to be used in this study.\n\n\n\n20\nPlanning Region\nchr\nGeographical region of transacted property, total of 5 regions - Central, North, North East, East and West.\nPlanning Region is the least complex amongst the geographical variables, and hence is selected as an ideal property classification and potential interacting variable that will be used in Data Visualisation 1, 2 and 3\n\n\n\n21\nPlanning Area\nchr\nName of geographical area of transacted property, total of 55 areas in Singapore\nPlanning Area is closely related to other geographical variables, but is not selected as the preferred variable to be used in this study.\n\n\n\n\nBased on the above, the data transformations required in preparation for the Data Visualisation is done and subsequently inspected using glimpse() in the following code chunk.\n\n\nCode\n# For version control, start by defining new transformed datasets as copies of original dataset\nRealis24Q1_trfm &lt;- Realis24Q1\nRealis_merged_trfm &lt;- Realis_merged\n\n# Convert Sale date to date data type\nRealis24Q1_trfm$`Sale Date` &lt;- dmy(Realis24Q1$`Sale Date`)\nRealis_merged_trfm$`Sale Date` &lt;- dmy(Realis_merged$`Sale Date`)\n\n# For Completion Date, (1) filter out missing values, (2) Calculate age of property caa 2024, (3) set Uncompleted as age 0\nRealis24Q1_trfm &lt;- Realis24Q1_trfm %&gt;% filter(!is.na(`Completion Date`))\nRealis_merged_trfm &lt;- Realis_merged_trfm %&gt;% filter(!is.na(`Completion Date`))\nRealis24Q1_trfm$`Completion Date`[Realis24Q1_trfm$`Completion Date` == \"Uncompleted\"] &lt;- 2024\nRealis_merged_trfm$`Completion Date`[Realis_merged_trfm$`Completion Date` == \"Uncompleted\"] &lt;- 2024\nRealis24Q1_trfm$Property_Age &lt;- 2024 - as.numeric(Realis24Q1_trfm$`Completion Date`)\nRealis_merged_trfm$Property_Age &lt;- 2024 - as.numeric(Realis_merged_trfm$`Completion Date`)\n\n# For number of units, filter out the multiple-property transactions as outliers\nRealis24Q1_trfm &lt;- Realis24Q1_trfm %&gt;% filter(`Number of Units` &lt;= 1)\nRealis_merged_trfm &lt;- Realis_merged_trfm %&gt;% filter(`Number of Units` &lt;= 1)\n\n# For Tenure, bin values into Leasehold and 999yrs/Freehold\nRealis24Q1_trfm &lt;- Realis24Q1_trfm %&gt;%\n  mutate(Tenure = case_when(\n    grepl(\"^Freehold\", Tenure) ~ \"999yrs/Freehold\",\n    grepl(\"^9[0-9]{2}\", Tenure) ~ \"999yrs/Freehold\",\n    TRUE ~ \"Leasehold\"\n  ))\nRealis_merged_trfm &lt;- Realis_merged_trfm %&gt;%\n  mutate(Tenure = case_when(\n    grepl(\"^Freehold\", Tenure) ~ \"999yrs/Freehold\",\n    grepl(\"^9[0-9]{2}\", Tenure) ~ \"999yrs/Freehold\",\n    TRUE ~ \"Leasehold\"\n  ))\n\n# Review transformed dataset\nglimpse(Realis24Q1_trfm)\n\n\nRows: 4,783\nColumns: 22\n$ `Project Name`                &lt;chr&gt; \"THE LANDMARK\", \"POLLEN COLLECTION\", \"SK…\n$ `Transacted Price ($)`        &lt;dbl&gt; 2726888, 3850000, 2346000, 2190000, 1954…\n$ `Area (SQFT)`                 &lt;dbl&gt; 1076.40, 1808.35, 1087.16, 807.30, 796.5…\n$ `Unit Price ($ PSF)`          &lt;dbl&gt; 2533, 2129, 2158, 2713, 2453, 2577, 838,…\n$ `Sale Date`                   &lt;date&gt; 2024-01-01, 2024-01-01, 2024-01-01, 202…\n$ Address                       &lt;chr&gt; \"173 CHIN SWEE ROAD #22-11\", \"34 POLLEN …\n$ `Type of Sale`                &lt;chr&gt; \"New Sale\", \"New Sale\", \"New Sale\", \"New…\n$ `Type of Area`                &lt;chr&gt; \"Strata\", \"Land\", \"Strata\", \"Strata\", \"S…\n$ `Area (SQM)`                  &lt;dbl&gt; 100.0, 168.0, 101.0, 75.0, 74.0, 123.0, …\n$ `Unit Price ($ PSM)`          &lt;dbl&gt; 27269, 22917, 23228, 29200, 26405, 27741…\n$ `Nett Price($)`               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Property Type`               &lt;chr&gt; \"Condominium\", \"Terrace House\", \"Apartme…\n$ `Number of Units`             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Tenure                        &lt;chr&gt; \"Leasehold\", \"Leasehold\", \"Leasehold\", \"…\n$ `Completion Date`             &lt;chr&gt; \"2024\", \"2024\", \"2024\", \"2024\", \"2024\", …\n$ `Purchaser Address Indicator` &lt;chr&gt; \"Private\", \"N.A\", \"HDB\", \"N.A\", \"Private…\n$ `Postal Code`                 &lt;chr&gt; \"169878\", \"807233\", \"469657\", \"118992\", …\n$ `Postal District`             &lt;chr&gt; \"03\", \"28\", \"16\", \"05\", \"21\", \"21\", \"28\"…\n$ `Postal Sector`               &lt;chr&gt; \"16\", \"80\", \"46\", \"11\", \"59\", \"58\", \"79\"…\n$ `Planning Region`             &lt;chr&gt; \"Central Region\", \"North East Region\", \"…\n$ `Planning Area`               &lt;chr&gt; \"Outram\", \"Serangoon\", \"Bedok\", \"Queenst…\n$ Property_Age                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 20, 11, 9, 11, 8, 8, 1…\n\n\nCode\nglimpse(Realis_merged_trfm)\n\n\nRows: 26,117\nColumns: 22\n$ `Project Name`                &lt;chr&gt; \"THE LANDMARK\", \"POLLEN COLLECTION\", \"SK…\n$ `Transacted Price ($)`        &lt;dbl&gt; 2726888, 3850000, 2346000, 2190000, 1954…\n$ `Area (SQFT)`                 &lt;dbl&gt; 1076.40, 1808.35, 1087.16, 807.30, 796.5…\n$ `Unit Price ($ PSF)`          &lt;dbl&gt; 2533, 2129, 2158, 2713, 2453, 2577, 838,…\n$ `Sale Date`                   &lt;date&gt; 2024-01-01, 2024-01-01, 2024-01-01, 202…\n$ Address                       &lt;chr&gt; \"173 CHIN SWEE ROAD #22-11\", \"34 POLLEN …\n$ `Type of Sale`                &lt;chr&gt; \"New Sale\", \"New Sale\", \"New Sale\", \"New…\n$ `Type of Area`                &lt;chr&gt; \"Strata\", \"Land\", \"Strata\", \"Strata\", \"S…\n$ `Area (SQM)`                  &lt;dbl&gt; 100.0, 168.0, 101.0, 75.0, 74.0, 123.0, …\n$ `Unit Price ($ PSM)`          &lt;dbl&gt; 27269, 22917, 23228, 29200, 26405, 27741…\n$ `Nett Price($)`               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Property Type`               &lt;chr&gt; \"Condominium\", \"Terrace House\", \"Apartme…\n$ `Number of Units`             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Tenure                        &lt;chr&gt; \"Leasehold\", \"Leasehold\", \"Leasehold\", \"…\n$ `Completion Date`             &lt;chr&gt; \"2024\", \"2024\", \"2024\", \"2024\", \"2024\", …\n$ `Purchaser Address Indicator` &lt;chr&gt; \"Private\", \"N.A\", \"HDB\", \"N.A\", \"Private…\n$ `Postal Code`                 &lt;chr&gt; \"169878\", \"807233\", \"469657\", \"118992\", …\n$ `Postal District`             &lt;chr&gt; \"03\", \"28\", \"16\", \"05\", \"21\", \"21\", \"28\"…\n$ `Postal Sector`               &lt;chr&gt; \"16\", \"80\", \"46\", \"11\", \"59\", \"58\", \"79\"…\n$ `Planning Region`             &lt;chr&gt; \"Central Region\", \"North East Region\", \"…\n$ `Planning Area`               &lt;chr&gt; \"Outram\", \"Serangoon\", \"Bedok\", \"Queenst…\n$ Property_Age                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 20, 11, 9, 11, 8, 8, 1…"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#data-visualisation-1-distribution-of-unit-sales-over-of-property-classification-variables",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#data-visualisation-1-distribution-of-unit-sales-over-of-property-classification-variables",
    "title": "Take-home Ex1",
    "section": "Data Visualisation 1: Distribution of unit sales over of property classification variables",
    "text": "Data Visualisation 1: Distribution of unit sales over of property classification variables\n\n# Create bar charts for each categorical variables\nplot1 &lt;- ggplot(Realis24Q1_trfm, aes(x = `Number of Units`, y = `Property Type`)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Number of Units by Property Type\", x = \"Number of Units\", y = \"Property Type\")\n\nplot2 &lt;- ggplot(Realis24Q1_trfm, aes(x = `Number of Units`, y = `Planning Region`)) +\n  geom_bar(stat = \"identity\", fill = \"lightgreen\") +\n  labs(title = \"Number of Units by Planning Region\", x = \"Number of Units\", y = \"Planning Region\")\n\nplot3 &lt;- ggplot(Realis24Q1_trfm, aes(x = `Number of Units`, y = `Type of Sale`)) +\n  geom_bar(stat = \"identity\", fill = \"lightcoral\") +\n  labs(title = \"Number of Units by Type of Sale\", x = \"Number of Units\", y = \"Type of Sale\")\n\nplot4 &lt;- ggplot(Realis24Q1_trfm, aes(x = `Number of Units`, y = `Tenure`)) +\n  geom_bar(stat = \"identity\", fill = \"lightpink\") +\n  labs(title = \"Number of Units by Tenure\", x = \"Number of Units\", y = \"Tenure\")\n\n# Create a histogram for Property_Age\nplot5 &lt;- ggplot(Realis24Q1_trfm, aes(x = Property_Age)) +\n  geom_histogram(bins = 30, fill = \"lightblue\") +\n  labs(title = \"Histogram of Property Age\", x = \"Property Age\")\n\n# Combine plots using operators\n(plot1 / plot2) | (plot3 / plot4) | plot5\n\n\n\n\nTaking the above at face value, the possible combination of preferences of buyers in the 5 property classifications can be inferred, as follows.\n\nFor Property Type, Condominiums, followed by Apartments and Executive Condominiums are seems to be more preferred.\nFor Planning Region, the Central Region, followed by Western and North Eastern Regions are preferred.\nFor Type of Sale, Resale is most preferred followed by New Sale\nFor Tenure, Leasehold is preferred over 999yrs/freehold\nFor Property Age, newer properties are preferred over older properties.\n\nWhile the above inferences could be made, it does not equate to a causality relationship between these individual variables and unit sales, as there could be multiple other confounding variables that explains one or more of these observations, such as the following.\n\nFor example, the overall property price and hence affordability could be an underlying reason that these preferences are surfacing, which could relate to property type, type of sale and tenure, where the most preferred options tend to align with the most affordable private properties in Singapore as well.\nAnother confounding variable could be supply or availability of such categories of properties for sale, which could relate to the property type, type of sale and tenure, since the most preferred options are also the most abundant categories of private properties in Singapore."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#data-visualisation-2-prevalence-of-distribution-patterns-over-past-quarters",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#data-visualisation-2-prevalence-of-distribution-patterns-over-past-quarters",
    "title": "Take-home Ex1",
    "section": "Data Visualisation 2: Prevalence of distribution patterns over past quarters",
    "text": "Data Visualisation 2: Prevalence of distribution patterns over past quarters\n\n# Assign quarters based on date ranges\nRealis_merged_trfm &lt;- Realis_merged_trfm %&gt;%\n  mutate(Quarter = case_when(\n    between(month(`Sale Date`), 1, 3) & year(`Sale Date`) == 2023 ~ 1,\n    between(month(`Sale Date`), 4, 6) & year(`Sale Date`) == 2023 ~ 2,\n    between(month(`Sale Date`), 7, 9) & year(`Sale Date`) == 2023 ~ 3,\n    between(month(`Sale Date`), 10, 12) & year(`Sale Date`) == 2023 ~ 4,\n    between(month(`Sale Date`), 1, 3) & year(`Sale Date`) == 2024 ~ 5,\n    TRUE ~ NA_integer_\n  ))\nRealis_merged_trfm$Quarter &lt;- as.factor(Realis_merged_trfm$Quarter)\n\n# Create ridge plot\nplot6 &lt;- ggplot(Realis_merged_trfm, aes(x = `Unit Price ($ PSF)`, y = Quarter)) +\n  geom_density_ridges() +\n  labs(title = \"Density of Unit Price ($ PSF) by Quarter\",\n       x = \"Unit Price ($ PSF)\", y = \"Quarter\")\n\nplot7 &lt;- ggplot(Realis_merged_trfm, aes(x = `Property_Age`, y = Quarter)) +\n  geom_density_ridges() +\n  labs(title = \"Density of Property Age by Quarter\",\n       x = \"Property Age\", y = \"Quarter\")\n\nplot8 &lt;- ggplot(Realis_merged_trfm, aes(x = `Sale Date`, y = Quarter)) +\n  geom_density_ridges() +\n  labs(title = \"Density of Sale Date by Quarter\",\n       x = \"Sale Date\", y = \"Quarter\")\n\n# Combine plots using operators\nplot6/plot7/plot8\n\n\n\n\nFrom the plots above, broad similarities can be observed across the quarters, elaborated as follows.\n\nFor Unit Price (PSF), all quarters displayed a right skew in the distribution of sales, with the peaks notably aligned at around $1,300/SQFT. This suggests that the PSF for private property sales transaction had maintained fairly consistent over the 5 quarters, with some slight deviation.\nFor Property age, all quarters displayed a right skew in the distribution of sales, with the peaks notably aligned close to 0. This suggests that across the 5 quarters, preferences for newer properties also maintained as a consistent trend.\n\nOn the other hand, there were notable differences for sales distribution over time, where by peaks were observed at random parts of each of the 5 quarters. A closer look at the timing of the peaks and comparing with external data sources, it is found that these peaks in fact align with the 1st 2 weekends of new project launches. Rather than seasonal, the trend in sales peaks are most significantly driven by new project launches."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#data-visualisation-3-correlation-between-price-per-area-and-potential-interacting-factors",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#data-visualisation-3-correlation-between-price-per-area-and-potential-interacting-factors",
    "title": "Take-home Ex1",
    "section": "Data Visualisation 3: Correlation between price per area and potential interacting factors",
    "text": "Data Visualisation 3: Correlation between price per area and potential interacting factors\n\nplot9 &lt;- ggplot(Realis24Q1_trfm, aes(x = `Unit Price ($ PSF)`, y = `Property Type`)) +\n  geom_boxplot() +\n  labs(x = \"Unit Price ($ PSF)\", y = \"Property Type\")\n\nplot10 &lt;- ggplot(Realis24Q1_trfm, aes(x = `Unit Price ($ PSF)`, y = `Planning Region`)) +\n  geom_boxplot() +\n  labs(x = \"Unit Price ($ PSF)\", y = \"Planning Region\")\n\nplot11 &lt;- ggplot(Realis24Q1_trfm, aes(x = `Unit Price ($ PSF)`, y = `Tenure`)) +\n  geom_boxplot() +\n  labs(x = \"Unit Price ($ PSF)\", y = \"Tenure\")\n\nplot12 &lt;- ggplot(Realis24Q1_trfm, aes(x = `Unit Price ($ PSF)`, y = `Property_Age`)) +  # Corrected the aes function\n  geom_point() +\n  geom_smooth()\n  labs(x = \"Unit Price ($ PSF)\", y = \"Property Age\")\n\n$x\n[1] \"Unit Price ($ PSF)\"\n\n$y\n[1] \"Property Age\"\n\nattr(,\"class\")\n[1] \"labels\"\n\nplot13 &lt;- ggplot(Realis24Q1_trfm, aes(x = `Unit Price ($ PSF)`, y = `Transacted Price ($)`)) +  # Corrected the aes function\n  geom_point() +\n  geom_smooth()\n  labs(x = \"Unit Price ($ PSF)\", y = \"Transacted Price ($)\")\n\n$x\n[1] \"Unit Price ($ PSF)\"\n\n$y\n[1] \"Transacted Price ($)\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n# Combine plots using operators\nplot9 / plot10 / plot11 / plot12 / plot13 \n\n\n\n\nBased on the above plots, there are some visible similarities and also relative rankings for Unit Prices (in PSF) between the different categories of private properties.\n\nFor Property Type, Terrace, Semi-detached, Detached and Condominiums seem to have very similar means at around $1,700/PSF, whereas Apartments have the notably highest mean than the rest, and Executive Condominiums seems to have the lowest. While the observed for executive condominium is within expectation, since it did afterall start out at a cheaper price as part of the governments efforts to meet the masstige market needs, the observations for the rest bring an interesting insight, and is worth exploring further beyond this study.\nFor Planning Region, interestingly, the rankings are exactly the same as that of the unit sales, which supports the possibility that this ranking could perhaps be indicative of the a strong preference for location of private properties among buyers in the first quarter of 2024.\nFor Tenure, the observation brings no surprise that 999yrs/freehold is more expensive in Unit Price as compared to leasehold, which is opposite of the rankings for unit sales.\nFor Property Age, the scatter plot and smoothened curve seems to suggest that there is at most a weak linearity relationship between property age and unit Price.\nFor Transacted prices, the scatter plot and smoothened curve similarly seems to suggest that there is at most a weak linearity relationship between Transacted prices and unit Price."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#references",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#references",
    "title": "Take-home Ex1",
    "section": "References",
    "text": "References\n\nhttps://quarto.org/\nhttps://lubridate.tidyverse.org/\nhttps://www.ura.gov.sg/\nhttps://r4va.netlify.app/"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html",
    "title": "Take-home Ex2",
    "section": "",
    "text": "As a follow on to the previous study on Singapore’s private property market, this exercise would entail a closer scrutiny on the Data Visualisation (DataViz) done. For which, one DataViz will be selected, and critiqued based on (a) clarity and (b) aesthetics. Consequently recommendations for an alternative design will be proposed, employing data visualisation design principles and best practices, in order to remake the original design by using ggplot2, ggplot2 extensions and tidyverse packages."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#introduction",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#introduction",
    "title": "Take-home Ex2",
    "section": "",
    "text": "As a follow on to the previous study on Singapore’s private property market, this exercise would entail a closer scrutiny on the Data Visualisation (DataViz) done. For which, one DataViz will be selected, and critiqued based on (a) clarity and (b) aesthetics. Consequently recommendations for an alternative design will be proposed, employing data visualisation design principles and best practices, in order to remake the original design by using ggplot2, ggplot2 extensions and tidyverse packages."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#getting-started",
    "title": "Take-home Ex2",
    "section": "Getting Started",
    "text": "Getting Started\n\nLoading Required R Package Libraries\nThe code chunk below loads the following libraries:\n\ntidyverse: an amalgamation of libraries for data handling (including ggplot2, dplyr, tidyr, readr, tibble)\nknitr: for creating dynamic html tables/reports\nggridges: extension of ggplot2 designed for plotting ridgeline plots\nggdist: extension of ggplot2 designed for visualising distribution and uncertainty,\ncolorspace: provides a broad toolbox for selecting individual colors or color palettes, manipulating these colors, and employing them in various kinds of visualisations.\nggrepel: provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: provides additional themes, geoms, and scales for ggplot package\nhrbrthemes: provides typography-centric themes and theme components for ggplot package\npatchwork: preparing composite figure created using ggplot package\nlubridate: for wrangling of date-time data\nggstatplot: provides alternative statistical inference methods by default as an extension of the ggplot2 package\nplotly: R library for plotting interactive statistical graphs.\n\n\npacman::p_load(tidyverse, knitr, ggridges, ggdist, colorspace, ggrepel, ggthemes, hrbrthemes, patchwork, lubridate, ggstatsplot, plotly) \n\n\n\nImporting the Data\nThe code chunk below imports 5 quarters of Private Residential Transaction data into R environment by using read_csv() function of readr package, which is part of the tidyverse package.\n\nRealis24Q1 &lt;- read_csv(\"data/ResidentialTrx2024q1.csv\")\nRealis23Q4 &lt;- read_csv(\"data/ResidentialTrx2023q4.csv\")\nRealis23Q3 &lt;- read_csv(\"data/ResidentialTrx2023q3.csv\")\nRealis23Q2 &lt;- read_csv(\"data/ResidentialTrx2023q2.csv\")\nRealis23Q1 &lt;- read_csv(\"data/ResidentialTrx2023Q1.csv\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#selected-dataviz",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#selected-dataviz",
    "title": "Take-home Ex2",
    "section": "Selected DataViz",
    "text": "Selected DataViz\nFor this exercise, the selected DataViz is the Comparison of Transacted prices by Type of Sale Between Q1 of 2023 and 2024, which can be accessed in the following link, and shown in the figure below..\n\nOf note, the accompanying explanation for the DataViz covered the following points:\n\nFirstly, based on the code chunk used, the initial box plot plotted the ‘Transacted Price ($)’ in the y-axis against the ‘Type of Sale’ in the x-axis, for transaction data in each of the 1st quarters of 2023 and 2024.\nHowever, since the box plots for both time periods ‘looked very squished’, 2 extreme outliers were excluded for each plot, and finally both plots were compiled side by side, to give the final DataViz figure above.\nThe analysis made gave the following insights:\n\nIn 2024 Q1, the median transacted price for resale is slightly lower than that of new sale, whereas, the range of prices for resale is larger as compared to new sale or sub sale, as shown by its larger interquartile range (IQR).\nSimilar to 2024 Q1, 2023 Q1 box plot shows the median transacted price is lower for resale as compared to new sale, whereas, the IQR of transacted prices of resale and new sale are similar, and that of sub sale is much smaller this time.\n\n\n\nPerforming Relevant Data Preparation\n\nRealis_Q1merged &lt;- rbind(Realis24Q1, Realis23Q1)\n\n# Convert Sale date to date data type\nRealis_Q1merged$`Sale Date` &lt;- dmy(Realis_Q1merged$`Sale Date`)\n\nRealis_Q1merged &lt;- Realis_Q1merged %&gt;%\n  mutate(Quarter = case_when(\n    between(month(`Sale Date`), 1, 3) & year(`Sale Date`) == 2023 ~ \"2023Q1\",\n    between(month(`Sale Date`), 1, 3) & year(`Sale Date`) == 2024 ~ \"2024Q1\",\n    TRUE ~ NA_character_\n  ))\n\nglimpse(Realis_Q1merged)\n\nRows: 9,624\nColumns: 22\n$ `Project Name`                &lt;chr&gt; \"THE LANDMARK\", \"POLLEN COLLECTION\", \"SK…\n$ `Transacted Price ($)`        &lt;dbl&gt; 2726888, 3850000, 2346000, 2190000, 1954…\n$ `Area (SQFT)`                 &lt;dbl&gt; 1076.40, 1808.35, 1087.16, 807.30, 796.5…\n$ `Unit Price ($ PSF)`          &lt;dbl&gt; 2533, 2129, 2158, 2713, 2453, 2577, 838,…\n$ `Sale Date`                   &lt;date&gt; 2024-01-01, 2024-01-01, 2024-01-01, 202…\n$ Address                       &lt;chr&gt; \"173 CHIN SWEE ROAD #22-11\", \"34 POLLEN …\n$ `Type of Sale`                &lt;chr&gt; \"New Sale\", \"New Sale\", \"New Sale\", \"New…\n$ `Type of Area`                &lt;chr&gt; \"Strata\", \"Land\", \"Strata\", \"Strata\", \"S…\n$ `Area (SQM)`                  &lt;dbl&gt; 100.0, 168.0, 101.0, 75.0, 74.0, 123.0, …\n$ `Unit Price ($ PSM)`          &lt;dbl&gt; 27269, 22917, 23228, 29200, 26405, 27741…\n$ `Nett Price($)`               &lt;chr&gt; \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", …\n$ `Property Type`               &lt;chr&gt; \"Condominium\", \"Terrace House\", \"Apartme…\n$ `Number of Units`             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Tenure                        &lt;chr&gt; \"99 yrs from 28/08/2020\", \"99 yrs from 0…\n$ `Completion Date`             &lt;chr&gt; \"Uncompleted\", \"Uncompleted\", \"Uncomplet…\n$ `Purchaser Address Indicator` &lt;chr&gt; \"Private\", \"N.A\", \"HDB\", \"N.A\", \"Private…\n$ `Postal Code`                 &lt;chr&gt; \"169878\", \"807233\", \"469657\", \"118992\", …\n$ `Postal District`             &lt;chr&gt; \"03\", \"28\", \"16\", \"05\", \"21\", \"21\", \"28\"…\n$ `Postal Sector`               &lt;chr&gt; \"16\", \"80\", \"46\", \"11\", \"59\", \"58\", \"79\"…\n$ `Planning Region`             &lt;chr&gt; \"Central Region\", \"North East Region\", \"…\n$ `Planning Area`               &lt;chr&gt; \"Outram\", \"Serangoon\", \"Bedok\", \"Queenst…\n$ Quarter                       &lt;chr&gt; \"2024Q1\", \"2024Q1\", \"2024Q1\", \"2024Q1\", …"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#critique-on-selected-dataviz",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#critique-on-selected-dataviz",
    "title": "Take-home Ex2",
    "section": "Critique on Selected DataViz",
    "text": "Critique on Selected DataViz\n\nOn Clarity\n\nIssue #1: Poor logic in removing outliers and misleading representation of prices since the data points for multiple-unit transactions were included in the price comparison, which gives an unfair comparison against single-unit transactions\nIssue #2: Near impossible to read and compare the position of the median and IQR, due to (a) the wide tick intervals in the y-axis and gridlines and (b) the extremely compressed box plot as the outliers took up majority of the plot even after excluding some data points\nIssue #3: Misalignment of y-axis impeding the comparison between the 2 box plots\nIssue #4: Arrangement of the boxplots by quarters impeding the comparison of sub-market components (3 types of sales) between the 2 quarters\nIssue #5: Skewness of distribution which could be a useful basis for comparison of transacted price was not clearly illustrated with the box plot\n\n\n\nOn Aesthetics\n\nIssue #6: The theme and colour scheme was left to default, which hence did not leverage on any colours to enhance the DataViz such as differentiating the different Type of Sales, or to bring attention to the key areas of analysis such as the median and IQR in this case\nIssue #7: Given the compressed box-plot, the figure size could be enhanced by rotating coordinates and increasing the width to follow the horizontal rectangular convention of graphical plots\nIssue #8: Remove superfluous tick marks on the categorical scale"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#recommended-makeover",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#recommended-makeover",
    "title": "Take-home Ex2",
    "section": "Recommended Makeover",
    "text": "Recommended Makeover\nTo address the aforesaid critique, the following alternatives are suggested\n\n1. Reassess outliers and Re-clean Data\nThis is meant to address issues #1 and potentially #2 and #7, by choosing 1 of the following options:\n(a) Exclude all multiple-unit transactions as outliers,\n(b) Use PSF instead of Transacted Price, or\n(c) Derive a new variable Price per unit by dividing the variable ‘Transacted price ($)’ by the variable ‘Number of Units’\nTo consider if it is meaningful for option (a), a uni-variate analysis on number of units is first done in the following code chunk.\n\n\nCode\n# Calculate frequency counts for each value of \"Number of Units\"\nfreq_counts &lt;- Realis_Q1merged %&gt;%\n  count(`Number of Units`)\n\n# Plot the histogram and Display frequency counts as text\nggplot(data = Realis_Q1merged, aes(x = `Number of Units`)) +\n  geom_histogram() +\n  geom_text(data = freq_counts, aes(label = n, y = n), vjust = -0.5) +\n  labs(x = \"Number of Units\", y = \"Frequency\")\n\n\n\n\n\nUsing the following code chunk, the Project Names for the multiple-unit transactions are extracted.\n\nsubset_df &lt;- subset(Realis_Q1merged, `Number of Units` &gt; 1, select = c(\"Project Name\", \"Number of Units\"))\nsubset_df\n\n# A tibble: 5 × 2\n  `Project Name`         `Number of Units`\n  &lt;chr&gt;                              &lt;dbl&gt;\n1 CLAYMORE PLAZA                         2\n2 BAGNALL COURT                         43\n3 MONDO MANSION BUILDING                 4\n4 MEYER PARK                            60\n5 N.A.                                   3\n\n\nBased on the above, it is found that out of 9.6k rows, only 5 rows contain multiple-unit transaction. For which, Bagnall court and Meyer Park were found to be en bloc sales, and including the rest, amounts to slightly over just 1% of the total units transacted. Hence it seems reasonable to exclude these data points in lieu that there could be different factors affecting the transaction prices and hence will need to be explored separately instead. The data cleaning is hence done in the following code chunk\n\n# Filter out the multiple-unit transactions as outliers\nRealis_Q1merged &lt;- Realis_Q1merged %&gt;% filter(`Number of Units` &lt;= 1)\n\n\n\n2. Synchronise box plots’ axes, visualise same category side by side and enable zoom into box plot to compare summary stats\nThis is meant to address issues #2, #3, #4, #6 & #7, by employing 1 of the following options in the DataViz makeover:\n(a) facet_grid or facet_wrap\n(b) Coordinated Multiple Views with ggiraph\n(c) Interactive Box plot with ggplotly()\nGoing with (c) as potentially the simplest option, a revised DataViz with Coordinated Multiple views with ggplotly() is plotted with the following code chunk.\n\n# Convert transacted price to millions\nRealis_Q1merged &lt;- Realis_Q1merged %&gt;%\n  mutate(`Transacted Price ($ Million)` = `Transacted Price ($)` / 1e6)\n\n# Define the order of combined factor\nRealis_Q1merged$Type_Quarter &lt;- factor(paste(Realis_Q1merged$`Type of Sale`, Realis_Q1merged$Quarter), \n                                       levels = c(\"New Sale 2023Q1\", \"New Sale 2024Q1\", \"Resale 2023Q1\", \"Resale 2024Q1\", \"Sub Sale 2023Q1\", \"Sub Sale 2024Q1\"))\n\n# Create box plots for each combination of Type of Sale and Quarter\np &lt;- ggplot(data = Realis_Q1merged, aes(x = Type_Quarter, y = `Transacted Price ($ Million)`, fill = factor(Quarter))) +\n  geom_boxplot() +\n  labs(x = \"Type of Sale and Quarter\", y = \"Transacted Price ($ Million)\") +\n  ggtitle(\"Box Plot of Transacted Price by Type of Sale and Quarter\") +\n  coord_flip() + # Rotate the plot horizontally\n  scale_y_continuous(labels = function(x) paste0(x, \"M\"), breaks = seq(0, max(Realis_Q1merged$`Transacted Price ($ Million)`), by = 2)) +\n  theme(legend.position = \"none\")\n\n# Convert ggplot object to plotly\nggplotly(p)\n\n\n\n\n\nIn the revised DataViz above, side-by-side comparison can now be done, with the box plot color fill differentiating between 2023Q1 and 2024Q1. And to further address the compressed box plot due to the long tail of outliers, the interactive ggplotly() function allows zooming in. Furthermore, the axes are flipped with x-axis tick intervals (for transaction prices) adjusted to $2 million, along with the addition of data labels for the median, IQR, min and max. These significantly enhances the visualisation in terms of clarity and aesthetics for the differences between the Sales Type between Q1 of 2023 and 2024.\n\n\n3. Clearer indication of skewness of distribution and removal of tick marks for categorial variable\nThis is meant to address the remaining issues #5 & #8, by employing 1 of the following options in the DataViz makeover:\n(a) display mean as a red dot on box plot\n(b) use violin plot with median and mean\n(c) two sample mean test using gg between stats\nGoing with (a) as the simplest option, a revised DataViz to add the indication of the mean and clean up the plot by removing the tick marks for categorical variable is plotted with the following code chunk.\n\n# Create box plots for each combination of Type of Sale and Quarter\np &lt;- ggplot(data = Realis_Q1merged, aes(x = Type_Quarter, y = `Transacted Price ($ Million)`, fill = factor(Quarter))) +\n  geom_boxplot() +\n  stat_summary(fun = mean, geom = \"point\", shape = 20, size = 1.5, color = \"blue\",\n               position = position_dodge(width = 0.75)) +\n  labs(x = \"Type of Sale and Quarter\", y = \"Transacted Price ($ Million)\") +\n  ggtitle(\"Box Plot of Transacted Price by Type of Sale and Quarter\") +\n  coord_flip() + # Rotate the plot horizontally\n  scale_y_continuous(labels = function(x) paste0(x, \"M\"), breaks = seq(0, max(Realis_Q1merged$`Transacted Price ($ Million)`), by = 2)) +\n  theme(axis.ticks.y = element_blank(),\n        legend.position = \"none\")\n\n# Convert ggplot object to plotly\nggplotly(p, tooltip = c(\"y\"))\n\n\n\n\n\nWith the latest version of the DataViz makeover above, the mean is illustrated with the red circle, and the value can also be read from the tooltip upon mouse over the point. With the mean, the skewness of the plot can be determined based on whether the mean is smaller than (left skew) larger than (right skew) or equal (centralized) to the median. Last but not least, the tick marks for the categorical variable which is unnecessary has also been removed.\nConverting to an interactive violin plot further enables the distribution to be visualised and compared, while keeping the labels for the mean and median as red and black circles respectively, however the code would increase in complexity significantly to still include the quartile, as see in the code chunk below.\n\n# Create violin plots for each combination of Type of Sale and Quarter\np &lt;- ggplot(data = Realis_Q1merged, aes(x = Type_Quarter, y = `Transacted Price ($ Million)`, fill = factor(Quarter))) +\n  geom_violin(trim = FALSE, alpha = 0.7, width = 0.8) + # Adjust width of violins\n  stat_summary(fun = mean, geom = \"point\", shape = 20, size = 1.5, color = \"blue\",\n               position = position_dodge(width = 0.75), aes(label = paste(\"Mean Transaction Price = \", round(..y.., 2))), show.legend = FALSE) +\n  stat_summary(fun = median, geom = \"point\", shape = 20, size = 1.5, color = \"black\",\n               position = position_dodge(width = 0.75), aes(label = paste(\"Median Transaction Price = \", round(..y.., 2))), show.legend = FALSE) +\n  stat_summary(fun = function(x) quantile(x, 0.25), geom = \"segment\", aes(x = Type_Quarter, xend = Type_Quarter, yend = ..y..), position = position_dodge(width = 0.75), color = \"blue\", show.legend = FALSE) +\n  stat_summary(fun = function(x) quantile(x, 0.75), geom = \"segment\", aes(x = Type_Quarter, xend = Type_Quarter, yend = ..y..), position = position_dodge(width = 0.75), color = \"blue\", show.legend = FALSE) +\n  labs(x = \"Type of Sale and Quarter\", y = \"Transacted Price ($ Million)\", fill = NULL) +\n  ggtitle(\"Violin Plot of Transacted Price by Type of Sale and Quarter\") +\n  scale_y_continuous(labels = function(x) paste0(x, \"M\"), breaks = seq(0, max(Realis_Q1merged$`Transacted Price ($ Million)`), by = 2),\n                     limits = c(0, max(Realis_Q1merged$`Transacted Price ($ Million)`))) +\n  theme(axis.ticks.y = element_blank(),\n        legend.position = \"none\")\n\n# Convert ggplot object to plotly\nggplotly(p, tooltip = c(\"y\"))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html",
    "title": "Hands-on Ex7",
    "section": "",
    "text": "The Learning Objectves of this Ex are as follows:\n\nplotting a calender heatmap by using ggplot2 functions,\nplotting a cycle plot by using ggplot2 function,\nplotting a slopegraph\nplotting a horizon chart\n\n\n\n\n\n\nThe code chunk below installs and launches the tidyverse, ggdist, ggridges, colourspace & ggthemes packages into R environment\n\npacman:: p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, tidyverse, CGPfunctions)\n\n\n\n\n\nIn this section, a calender heatmap will be plot using ggplot2 package.\nSection learning objectives:\n\nplot a calender heatmap by using ggplot2 functions and extension,\nto write function using R programming,\nto derive specific date and time related field by using base R and lubridate packages\nto perform data preparation task by using tidyr and dplyr packages.\n\n\n\nThe code chunk below imports eventlog.csv into R environment by using read_csv() function of readr package, which is part of the tidyverse package.This data file consists of 199,999 rows of time-series cyber attack records by country.\n\nattacks &lt;- read_csv(\"data/eventlog.csv\")\n\n\n\n\nIt is always a good practice to examine the imported data frame before further analysis is performed.\nkable() can be used to review the structure of the imported data frame.\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address.\n\nStep 1: Deriving weekday and hour of day fields\nBefore plotting the calender heatmap, two new fields namely wkday and hour need to be derived. In this step, a function will be defined to perform the task.\n\nmake_hr_wkday &lt;- function(ts, sc, tz) {\n  real_times &lt;- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\n\n\n\n\n\n\nNote\n\n\n\n\nymd_hms() and hour() are from lubridate package, and\nweekdays() is a base R function.\n\n\n\nStep 2: Deriving the attacks tibble data frame\n\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\n\n\n\n\n\nNote\n\n\n\nBeside extracting the necessary data into attacks data frame, mutate() of dplyr package is used to convert wkday and hour fields into factor so they’ll be ordered when plotting\n\n\nTable below shows the tidy tibble table after processing.\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\n\n\ngrouped &lt;- attacks %&gt;% \n  count(wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk\n\n\n\n\na tibble data table called grouped is derived by aggregating the attack by wkday and hour fields.\na new field called n is derived by using group_by() and count() functions.\nna.omit() is used to exclude missing value.\ngeom_tile() is used to plot tiles (grids) at each x and y position. color and size arguments are used to specify the border color and line size of the tiles.\ntheme_tufte() of ggthemes package is used to remove unnecessary chart junk. To learn which visual components of default ggplot2 have been excluded, you are encouraged to comment out this line to examine the default plot.\ncoord_equal() is used to ensure the plot will have an aspect ratio of 1:1.\nscale_fill_gradient() function is used to creates a two colour gradient (low-high).\n\n\n\nThen the count can be simply grouped by hour and wkday and plotted. Since it’s known that values for every combination there is no need to further preprocess the data.\n\n\n\nTask: Building multiple heatmaps for the top four countries with the highest number of attacks.\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, you are required to do the following:\n\ncount the number of attacks by country,\ncalculate the percent of attackes by country, and\nsave the results in a tibble data frame.\n\n\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\nStep 2: Preparing the tidy data frame\nIn this step, it is required to extract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e. top4_attacks).\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package.\n\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\nIn this section, the learning objective is to plot a cycle plot showing the time-series patterns and trend of visitor arrivals from Vietnam programmatically by using ggplot2 functions.\n\n\nThe code chunk below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")\n\n\n\n\nFirst, Derive month and year fields. 2 new fields called month and year are derived from Month-Year field.\n\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year &lt;- year(ymd(air$`Month-Year`))\n\nNext, the code chunk below is use to extract data for the target country (i.e. Vietnam)\n\nVietnam &lt;- air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)\n\nThen, compute the year average arrivals by month. The code chunk below uses group_by() and summarise() of dplyr to compute year average arrivals by month.\n\nhline.data &lt;- Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\n\n\nFinally the Cycle plot can be built, using the code chunk below.\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_tufte(base_family = \"Helvetica\")\n\n\n\n\n\n\n\n\nIn this section the learning objective is to plot a slopegraph by using R.\nBefore getting start, make sure that CGPfunctions has been installed and loaded onto R environment. Then, refer to Using newggslopegraph to learn more about the function. Lastly, read more about newggslopegraph() and its arguments by referring to this link.\n\n\nImport the rice data set into R environment by using the code chunk below.\n\nrice &lt;- read_csv(\"data/rice.csv\")\n\n\n\n\nNext, code chunk below will be used to plot a basic slopegraph as shown below.\n\nrice %&gt;% \n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Jayexx\")\n\n\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above\n\n\n\nFor effective data visualisation design, factor() is used convert the value type of Year field from numeric to factor."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#learning-objectives",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#learning-objectives",
    "title": "Hands-on Ex7",
    "section": "",
    "text": "The Learning Objectves of this Ex are as follows:\n\nplotting a calender heatmap by using ggplot2 functions,\nplotting a cycle plot by using ggplot2 function,\nplotting a slopegraph\nplotting a horizon chart"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#getting-started",
    "title": "Hands-on Ex7",
    "section": "",
    "text": "The code chunk below installs and launches the tidyverse, ggdist, ggridges, colourspace & ggthemes packages into R environment\n\npacman:: p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, tidyverse, CGPfunctions)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#plotting-calendar-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#plotting-calendar-heatmap",
    "title": "Hands-on Ex7",
    "section": "",
    "text": "In this section, a calender heatmap will be plot using ggplot2 package.\nSection learning objectives:\n\nplot a calender heatmap by using ggplot2 functions and extension,\nto write function using R programming,\nto derive specific date and time related field by using base R and lubridate packages\nto perform data preparation task by using tidyr and dplyr packages.\n\n\n\nThe code chunk below imports eventlog.csv into R environment by using read_csv() function of readr package, which is part of the tidyverse package.This data file consists of 199,999 rows of time-series cyber attack records by country.\n\nattacks &lt;- read_csv(\"data/eventlog.csv\")\n\n\n\n\nIt is always a good practice to examine the imported data frame before further analysis is performed.\nkable() can be used to review the structure of the imported data frame.\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address.\n\nStep 1: Deriving weekday and hour of day fields\nBefore plotting the calender heatmap, two new fields namely wkday and hour need to be derived. In this step, a function will be defined to perform the task.\n\nmake_hr_wkday &lt;- function(ts, sc, tz) {\n  real_times &lt;- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\n\n\n\n\n\n\nNote\n\n\n\n\nymd_hms() and hour() are from lubridate package, and\nweekdays() is a base R function.\n\n\n\nStep 2: Deriving the attacks tibble data frame\n\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\n\n\n\n\n\nNote\n\n\n\nBeside extracting the necessary data into attacks data frame, mutate() of dplyr package is used to convert wkday and hour fields into factor so they’ll be ordered when plotting\n\n\nTable below shows the tidy tibble table after processing.\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\n\n\ngrouped &lt;- attacks %&gt;% \n  count(wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk\n\n\n\n\na tibble data table called grouped is derived by aggregating the attack by wkday and hour fields.\na new field called n is derived by using group_by() and count() functions.\nna.omit() is used to exclude missing value.\ngeom_tile() is used to plot tiles (grids) at each x and y position. color and size arguments are used to specify the border color and line size of the tiles.\ntheme_tufte() of ggthemes package is used to remove unnecessary chart junk. To learn which visual components of default ggplot2 have been excluded, you are encouraged to comment out this line to examine the default plot.\ncoord_equal() is used to ensure the plot will have an aspect ratio of 1:1.\nscale_fill_gradient() function is used to creates a two colour gradient (low-high).\n\n\n\nThen the count can be simply grouped by hour and wkday and plotted. Since it’s known that values for every combination there is no need to further preprocess the data.\n\n\n\nTask: Building multiple heatmaps for the top four countries with the highest number of attacks.\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, you are required to do the following:\n\ncount the number of attacks by country,\ncalculate the percent of attackes by country, and\nsave the results in a tibble data frame.\n\n\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\nStep 2: Preparing the tidy data frame\nIn this step, it is required to extract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e. top4_attacks).\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package.\n\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#plotting-cycle-plot",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#plotting-cycle-plot",
    "title": "Hands-on Ex7",
    "section": "",
    "text": "In this section, the learning objective is to plot a cycle plot showing the time-series patterns and trend of visitor arrivals from Vietnam programmatically by using ggplot2 functions.\n\n\nThe code chunk below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")\n\n\n\n\nFirst, Derive month and year fields. 2 new fields called month and year are derived from Month-Year field.\n\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year &lt;- year(ymd(air$`Month-Year`))\n\nNext, the code chunk below is use to extract data for the target country (i.e. Vietnam)\n\nVietnam &lt;- air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)\n\nThen, compute the year average arrivals by month. The code chunk below uses group_by() and summarise() of dplyr to compute year average arrivals by month.\n\nhline.data &lt;- Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\n\n\nFinally the Cycle plot can be built, using the code chunk below.\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_tufte(base_family = \"Helvetica\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#plotting-slopegraph",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#plotting-slopegraph",
    "title": "Hands-on Ex7",
    "section": "",
    "text": "In this section the learning objective is to plot a slopegraph by using R.\nBefore getting start, make sure that CGPfunctions has been installed and loaded onto R environment. Then, refer to Using newggslopegraph to learn more about the function. Lastly, read more about newggslopegraph() and its arguments by referring to this link.\n\n\nImport the rice data set into R environment by using the code chunk below.\n\nrice &lt;- read_csv(\"data/rice.csv\")\n\n\n\n\nNext, code chunk below will be used to plot a basic slopegraph as shown below.\n\nrice %&gt;% \n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Jayexx\")\n\n\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above\n\n\n\nFor effective data visualisation design, factor() is used convert the value type of Year field from numeric to factor."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8a.html",
    "href": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8a.html",
    "title": "Hands-on Ex8a",
    "section": "",
    "text": "The key learning objective of this hands-on exercise is to plot functional and truthful choropleth maps using appropriate R packages.\n\n\n\nThe code chunk below installs and loads sf, tmap and tidyverse packages into R environment\n\npacman:: p_load(dplyr, tmap, sf, tidyverse, ggplot2)\n\n\n\n\nData set consists of: - Master Plan 2014 Subzone Boundary (Web) from data.gov.sg - Singapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 from Department of Statistics, Singapore\n\n\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\",layer =\"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\jayexx\\ISSS608-VAA\\Hands-on_Ex\\Hands-on_Ex8\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\", show_col_types = FALSE)\n\n\n\n\n\n\nPrepare data based on following variables - YOUNG: age group 0 to 4 until age groyup 20 to 24, - ECONOMY ACTIVE: age group 25-29 until age group 60-64, - AGED: age group 65 and above, - TOTAL: all age group, and - DEPENDENCY: the ratio between young and aged against economy active group\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n\nConvert PA & SZ fields to uppercase with the following code chunk\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\n\n\n\n\n\n\nPlot cartographic standard choropleth map with the following code chunk, where fill argument is used to map the attribute (i.e. DEPENDENCY)\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\nDraw a high quality cartographic choropleth map with the following code chunk\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\n\n\nUsing Quantile data with 5 classes in the following code chunk\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nUsing equal data in the following code chunk\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nUsing Quantile data with 2 classes in the following code chunk\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 2,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nUsing Quantile data with 6 classes in the following code chunk\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nUsing Quantile data with 10 classes in the following code chunk\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 10,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nUsing Quantile data with 20 classes in the following code chunk\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 20,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nClassic style:\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\nDraw other map furniture such as compass, scale bar and grid lines with the following code chunk\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nReset default style to white in the following code chunk\n\ntmap_style(\"white\")\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8a.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8a.html#overview",
    "title": "Hands-on Ex8a",
    "section": "",
    "text": "The key learning objective of this hands-on exercise is to plot functional and truthful choropleth maps using appropriate R packages."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8a.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8a.html#getting-started",
    "title": "Hands-on Ex8a",
    "section": "",
    "text": "The code chunk below installs and loads sf, tmap and tidyverse packages into R environment\n\npacman:: p_load(dplyr, tmap, sf, tidyverse, ggplot2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8a.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8a.html#importing-data",
    "title": "Hands-on Ex8a",
    "section": "",
    "text": "Data set consists of: - Master Plan 2014 Subzone Boundary (Web) from data.gov.sg - Singapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 from Department of Statistics, Singapore\n\n\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\",layer =\"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\jayexx\\ISSS608-VAA\\Hands-on_Ex\\Hands-on_Ex8\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\", show_col_types = FALSE)\n\n\n\n\n\n\nPrepare data based on following variables - YOUNG: age group 0 to 4 until age groyup 20 to 24, - ECONOMY ACTIVE: age group 25-29 until age group 60-64, - AGED: age group 65 and above, - TOTAL: all age group, and - DEPENDENCY: the ratio between young and aged against economy active group\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n\nConvert PA & SZ fields to uppercase with the following code chunk\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8a.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex8/Hands-on_Ex8a.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-on Ex8a",
    "section": "",
    "text": "Plot cartographic standard choropleth map with the following code chunk, where fill argument is used to map the attribute (i.e. DEPENDENCY)\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\nDraw a high quality cartographic choropleth map with the following code chunk\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\n\n\nUsing Quantile data with 5 classes in the following code chunk\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nUsing equal data in the following code chunk\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nUsing Quantile data with 2 classes in the following code chunk\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 2,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nUsing Quantile data with 6 classes in the following code chunk\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nUsing Quantile data with 10 classes in the following code chunk\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 10,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nUsing Quantile data with 20 classes in the following code chunk\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 20,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nClassic style:\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\nDraw other map furniture such as compass, scale bar and grid lines with the following code chunk\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nReset default style to white in the following code chunk\n\ntmap_style(\"white\")\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take_home_Ex3.html",
    "href": "Take-home_Ex/Take-home_Ex3/Take_home_Ex3.html",
    "title": "Take-home Ex3",
    "section": "",
    "text": "This take-home exercise is centred on the learning patterns of learners in a programming course conducted by NorthClass Education Training Institute. The primary focus is to support the institution’s endeavor to analyze and visualise learners’ knowledge mastery levels, monitor the patterns and trends in their learning behaviors, identify and dissect potential factors that contribute to learning difficulties, and hence to derive feasible suggestions to adjust teaching strategies and course design.\n\n\nTo address the above, the key objective of this exercise is:\n\nTo analyse and provide a visual representation of the relationship between learning modes and knowledge acquisition (learners’ ability to absorb, integrate, and apply knowledge)\n\nThis would entail the following sub-task requirements:\n\nTo visualise and uncover the various learning modes, and\nTo visualise and uncover the patterns in distribution in learner’s performance in each various learning modes, and\nTo visualise and determine the statistical differences and correlations that learning mode may have with learners’ performance"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take_home_Ex3.html#introduction",
    "href": "Take-home_Ex/Take-home_Ex3/Take_home_Ex3.html#introduction",
    "title": "Take-home Ex3",
    "section": "",
    "text": "This take-home exercise is centred on the learning patterns of learners in a programming course conducted by NorthClass Education Training Institute. The primary focus is to support the institution’s endeavor to analyze and visualise learners’ knowledge mastery levels, monitor the patterns and trends in their learning behaviors, identify and dissect potential factors that contribute to learning difficulties, and hence to derive feasible suggestions to adjust teaching strategies and course design.\n\n\nTo address the above, the key objective of this exercise is:\n\nTo analyse and provide a visual representation of the relationship between learning modes and knowledge acquisition (learners’ ability to absorb, integrate, and apply knowledge)\n\nThis would entail the following sub-task requirements:\n\nTo visualise and uncover the various learning modes, and\nTo visualise and uncover the patterns in distribution in learner’s performance in each various learning modes, and\nTo visualise and determine the statistical differences and correlations that learning mode may have with learners’ performance"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take_home_Ex3.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex3/Take_home_Ex3.html#getting-started",
    "title": "Take-home Ex3",
    "section": "Getting Started",
    "text": "Getting Started\n\nLoading Required R Package Libraries\nThe code chunk below loads the following libraries:\n\ntidyverse: an amalgamation of libraries for data handling (including ggplot2, dplyr, tidyr, readr, tibble)\nknitr: for creating dynamic html tables/reports\nggridges: extension of ggplot2 designed for plotting ridgeline plots\nggdist: extension of ggplot2 designed for visualising distribution and uncertainty,\ncolorspace: provides a broad toolbox for selecting individual colors or color palettes, manipulating these colors, and employing them in various kinds of visualisations.\nggrepel: provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: provides additional themes, geoms, and scales for ggplot package\nhrbrthemes: provides typography-centric themes and theme components for ggplot package\npatchwork: preparing composite figure created using ggplot package\nlubridate: for wrangling of date-time data\nggstatplot: provides alternative statistical inference methods by default as an extension of the ggplot2 package\nplotly: R library for plotting interactive statistical graphs.\nrjson: Methods for Cluster analysis.\nvisNetwork: Extract and Visualize the Results of Multivariate Data Analyses.\nBiocManager: Extension of ggplot2 by adding several functions to reduce the complexity of combining geometric objects with transformed data.\nigraph: Extension of ggplot2 by adding several functions to reduce the complexity of combining geometric objects with transformed data.\ncluster\nfactoextra\nstats\nhms\ncaret\nggfortify\ngridExtra\nGGally\nparallelPlot\nseriation\ndendextend\nheatmaply,\n\n\npacman::p_load(tidyverse, knitr, ggridges, ggdist, colorspace, ggrepel, ggthemes, hrbrthemes, patchwork, lubridate, ggstatsplot, plotly, rjson, visNetwork, BiocManager, igraph, cluster, factoextra, stats, hms, caret, ggfortify, gridExtra, GGally, parallelPlot, seriation, dendextend, heatmaply) \n\n\n\nImporting the Data\nThe data for this exercise was collected from a select group of learners over a specified set of programming tasks over a particular learning period, which was compiled in 3 datasets described below. It is accompanied by a separate document providing a more detailed description of the data and variables.\n\nDataset 1: Student Information - This comprises of 5 Cols, 1364 Rows, providing individualised demographic variables of the learners (a.k.a students) within the scope this project\nDataset 2: Learning Subject Title Information - This comprises of 5 Cols, 44 Rows, providing variables of the questions from the programming tasks which are collated in the scope of this project\nDataset 3: Class Submission Records - This comprises of 15 datasets, each with 10 Cols and various number of rows, providing supposedly the participating learners’ answering variables to the questions collated in the scope of this project\n\nThe code chunk below imports the dataset into R environment by using read_csv() function of readr, which is part of the tidyverse package.\n\ndf_StudentInfo &lt;- read_csv(\"data/Data_StudentInfo.csv\")\n\n\ndf_TitleInfo &lt;- read_csv(\"data/Data_TitleInfo.csv\")\n\n\ncsv_file_list &lt;- dir('data/Data_SubmitRecord')\ncsv_file_list &lt;- paste0(\"./data/Data_SubmitRecord/\",csv_file_list)\n\ndf_StudentRecord &lt;- NULL\nfor (file in csv_file_list) { # for every file...\n  file &lt;- read_csv(file)\n    df_StudentRecord &lt;- rbind(df_StudentRecord, file) # then stick together by rows\n}"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take_home_Ex3.html#data-preparation",
    "href": "Take-home_Ex/Take-home_Ex3/Take_home_Ex3.html#data-preparation",
    "title": "Take-home Ex3",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nData Cleaning\nBefore data transformation, the cleanliness of the data set is first ascertained by checking for missing and duplicate data.\n\nMissing Data\ncolSums() and is.NA() functions are used to search for missing values as a whole for the 3 data sets in the code chunks as follows.\n\n#Find the number of missing values for each col\ncolSums(is.na(df_StudentInfo))\n\n     index student_ID        sex        age      major \n         0          0          0          0          0 \n\n\n\n#Find the number of missing values for each col\ncolSums(is.na(df_TitleInfo))\n\n        index      title_ID         score     knowledge sub_knowledge \n            0             0             0             0             0 \n\n\n\n#Find the number of missing values for each col\ncolSums(is.na(df_StudentRecord))\n\n      index       class        time       state       score    title_ID \n          0           0           0           0           0           0 \n     method      memory timeconsume  student_ID \n          0           0           0           0 \n\n\nFrom the outputs above, none of the variables contain missing values.\n\n\nCheck for duplicate rows\nUsing duplicated(), duplicate rows in each of the 3 data sets are identified and extracted in the following code chunks.\n\ndf_StudentInfo[duplicated(df_StudentInfo), ]\n\n# A tibble: 0 × 5\n# ℹ 5 variables: index &lt;dbl&gt;, student_ID &lt;chr&gt;, sex &lt;chr&gt;, age &lt;dbl&gt;,\n#   major &lt;chr&gt;\n\n\n\ndf_TitleInfo[duplicated(df_TitleInfo), ]\n\n# A tibble: 0 × 5\n# ℹ 5 variables: index &lt;dbl&gt;, title_ID &lt;chr&gt;, score &lt;dbl&gt;, knowledge &lt;chr&gt;,\n#   sub_knowledge &lt;chr&gt;\n\n\n\ndf_StudentRecord[duplicated(df_StudentRecord), ]\n\n# A tibble: 0 × 10\n# ℹ 10 variables: index &lt;dbl&gt;, class &lt;chr&gt;, time &lt;dbl&gt;, state &lt;chr&gt;,\n#   score &lt;dbl&gt;, title_ID &lt;chr&gt;, method &lt;chr&gt;, memory &lt;dbl&gt;, timeconsume &lt;chr&gt;,\n#   student_ID &lt;chr&gt;\n\n\nFrom the outputs above, there were no duplicate rows found.\n\n\n\nData Wrangling for Inconsistencies\nTo get a better understanding of the variables in the original dataset, the glimpse() function is used in the following code chunks.\n\nglimpse(df_StudentInfo)\n\nRows: 1,364\nColumns: 5\n$ index      &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ student_ID &lt;chr&gt; \"8b6d1125760bd3939b6e\", \"63eef37311aaac915a45\", \"5d89810b20…\n$ sex        &lt;chr&gt; \"female\", \"female\", \"female\", \"female\", \"male\", \"male\", \"ma…\n$ age        &lt;dbl&gt; 24, 21, 23, 21, 22, 19, 21, 18, 21, 24, 23, 20, 18, 18, 23,…\n$ major      &lt;chr&gt; \"J23517\", \"J87654\", \"J87654\", \"J78901\", \"J40192\", \"J57489\",…\n\n\n\nglimpse(df_TitleInfo)\n\nRows: 44\nColumns: 5\n$ index         &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ title_ID      &lt;chr&gt; \"Question_VgKw8PjY1FR6cm2QI9XW\", \"Question_q7OpB2zCMmW9w…\n$ score         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ knowledge     &lt;chr&gt; \"r8S3g\", \"r8S3g\", \"r8S3g\", \"r8S3g\", \"r8S3g\", \"r8S3g\", \"t…\n$ sub_knowledge &lt;chr&gt; \"r8S3g_l0p5viby\", \"r8S3g_n0m9rsw4\", \"r8S3g_l0p5viby\", \"r…\n\n\n\nglimpse(df_StudentRecord)\n\nRows: 232,818\nColumns: 10\n$ index       &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …\n$ class       &lt;chr&gt; \"Class1\", \"Class1\", \"Class1\", \"Class1\", \"Class1\", \"Class1\"…\n$ time        &lt;dbl&gt; 1704209872, 1704209852, 1704209838, 1704208923, 1704208359…\n$ state       &lt;chr&gt; \"Absolutely_Correct\", \"Absolutely_Correct\", \"Absolutely_Co…\n$ score       &lt;dbl&gt; 3, 3, 3, 3, 4, 0, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 4, 0, 0…\n$ title_ID    &lt;chr&gt; \"Question_bumGRTJ0c8p4v5D6eHZa\", \"Question_62XbhBvJ8NUSnAp…\n$ method      &lt;chr&gt; \"Method_Cj9Ya2R7fZd6xs1q5mNQ\", \"Method_gj1NLb4Jn7URf9K2kQP…\n$ memory      &lt;dbl&gt; 320, 356, 196, 308, 320, 0, 308, 312, 312, 328, 512, 324, …\n$ timeconsume &lt;chr&gt; \"3\", \"3\", \"2\", \"2\", \"3\", \"5\", \"2\", \"2\", \"3\", \"2\", \"3\", \"2\"…\n$ student_ID  &lt;chr&gt; \"8b6d1125760bd3939b6e\", \"8b6d1125760bd3939b6e\", \"8b6d11257…\n\n\n\nIdentifying Other Unexpected Duplicate Values\nConsidering intuitively unique values for certain variables or dependent variables, other forms of duplicates are also identified and cleaned where relevant.\n\nDuplicate student_ID in StudentInfo\n\n\n# Find the duplicated student_IDs\nduplicates &lt;- df_StudentInfo[duplicated(df_StudentInfo$student_ID) | duplicated(df_StudentInfo$student_ID, fromLast = TRUE), ]\n\n# Display the rows with duplicate student_IDs\nduplicates\n\n# A tibble: 0 × 5\n# ℹ 5 variables: index &lt;dbl&gt;, student_ID &lt;chr&gt;, sex &lt;chr&gt;, age &lt;dbl&gt;,\n#   major &lt;chr&gt;\n\n\nFrom the output above, no duplicates found.\n\nDuplicate title_ID (aka questions) in TitleInfo\n\n\n# Find the duplicated title_IDs\nduplicates &lt;- df_TitleInfo[duplicated(df_TitleInfo$title_ID) | duplicated(df_TitleInfo$title_ID, fromLast = TRUE), ]\n\n# Display the rows with duplicate title_IDs\nduplicates\n\n# A tibble: 12 × 5\n   index title_ID                      score knowledge sub_knowledge \n   &lt;dbl&gt; &lt;chr&gt;                         &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;         \n 1     2 Question_q7OpB2zCMmW9wS8uNt3H     1 r8S3g     r8S3g_n0m9rsw4\n 2     3 Question_q7OpB2zCMmW9wS8uNt3H     1 r8S3g     r8S3g_l0p5viby\n 3    21 Question_QRm48lXxzdP7Tn1WgNOf     3 y9W5d     y9W5d_c0w4mj5h\n 4    22 Question_QRm48lXxzdP7Tn1WgNOf     3 m3D1v     m3D1v_r1d7fr3l\n 5    23 Question_pVKXjZn0BkSwYcsa7C31     3 y9W5d     y9W5d_c0w4mj5h\n 6    24 Question_pVKXjZn0BkSwYcsa7C31     3 m3D1v     m3D1v_r1d7fr3l\n 7    26 Question_lU2wvHSZq7m43xiVroBc     3 y9W5d     y9W5d_c0w4mj5h\n 8    27 Question_lU2wvHSZq7m43xiVroBc     3 k4W1c     k4W1c_h5r6nux7\n 9    30 Question_x2Fy7rZ3SwYl9jMQkpOD     3 y9W5d     y9W5d_c0w4mj5h\n10    31 Question_x2Fy7rZ3SwYl9jMQkpOD     3 s8Y2f     s8Y2f_v4x8by9j\n11    36 Question_oCjnFLbIs4Uxwek9rBpu     3 g7R2j     g7R2j_e0v1yls8\n12    37 Question_oCjnFLbIs4Uxwek9rBpu     3 m3D1v     m3D1v_r1d7fr3l\n\n\n\nunique(duplicates$knowledge)\n\n[1] \"r8S3g\" \"y9W5d\" \"m3D1v\" \"k4W1c\" \"s8Y2f\" \"g7R2j\"\n\nunique(duplicates$sub_knowledge)\n\n[1] \"r8S3g_n0m9rsw4\" \"r8S3g_l0p5viby\" \"y9W5d_c0w4mj5h\" \"m3D1v_r1d7fr3l\"\n[5] \"k4W1c_h5r6nux7\" \"s8Y2f_v4x8by9j\" \"g7R2j_e0v1yls8\"\n\n\nFrom the outputs above, some questions (title_ID) belong to up to 2 knowledge areas or 2 sub-knowledge areas, where the scores for the former are consistently 3, and for the latter, 1. This overlap in title_ID affects 6 title_IDs, spreads across 6 knowledge areas and 7 sub-knowledge areas.\nThe unique values for knowledge and sub-knowledge areas are obtained in the following code chunk to better understand the complexity of these 2 variables.\n\nunique(df_TitleInfo$knowledge)\n\n[1] \"r8S3g\" \"t5V9e\" \"m3D1v\" \"y9W5d\" \"k4W1c\" \"s8Y2f\" \"g7R2j\" \"b3C9s\"\n\nunique(df_TitleInfo$sub_knowledge)\n\n [1] \"r8S3g_l0p5viby\" \"r8S3g_n0m9rsw4\" \"t5V9e_e1k6cixp\" \"m3D1v_r1d7fr3l\"\n [5] \"m3D1v_v3d9is1x\" \"m3D1v_t0v5ts9h\" \"y9W5d_c0w4mj5h\" \"k4W1c_h5r6nux7\"\n [9] \"s8Y2f_v4x8by9j\" \"y9W5d_p8g6dgtv\" \"y9W5d_e2j7p95s\" \"g7R2j_e0v1yls8\"\n[13] \"g7R2j_j1g8gd3v\" \"b3C9s_l4z6od7y\" \"b3C9s_j0v1yls8\"\n\n\nBased on the output above, there is a total of 8 knowledge areas and 15 sub-knowledge areas. This suggests that majority of the knowledge areas and approximately half of sub-knowledge areas have overlapping title_ID. From the nomenclature, each sub-knowledge area is tagged to only 1 knowledge area.\nTo meaningfully analyse the relationship between knowledge areas & sub knowledge areas and other variables, additional columns are introduced where the values in these 2 columnns are transposed as column labels with binary values to indicate the tagging of each question to that value. This is done in the following code chunk.\n\n# Transpose the knowledge column to create new columns for each unique value\ndf_TitleInfo1 &lt;- df_TitleInfo %&gt;%\n  mutate(knowledge_presence = 1) %&gt;%\n  spread(key = knowledge, value = knowledge_presence, fill = 0)\n\n# Transpose the sub_knowledge column to create new columns for each unique value\ndf_TitleInfo2 &lt;- df_TitleInfo %&gt;%\n  mutate(sub_knowledge_presence = 1) %&gt;%\n  spread(key = sub_knowledge, value = sub_knowledge_presence, fill = 0)\n\n# Combine the new columns with the original dataframe\ndf_TitleInfo3 &lt;- df_TitleInfo %&gt;%\n  distinct(title_ID, .keep_all = TRUE) %&gt;%\n  left_join(df_TitleInfo1, by = \"title_ID\") %&gt;%\n  distinct(title_ID, .keep_all = TRUE) %&gt;%\n  left_join(df_TitleInfo2, by = \"title_ID\") %&gt;%\n  \n  rename(knowledge = knowledge.x,\n         sub_knowledge = sub_knowledge.x) %&gt;%\n  select(-index.y,\n         -score.y,\n         -knowledge.y,\n         -sub_knowledge.y,\n         -index.x,\n         -score.x,\n         -index)\n\n# Reassign values to the knowledge & sub_knowledge columns for repeated title_ID rows\ndf_TitleInfo3 &lt;- df_TitleInfo3 %&gt;%\n  group_by(title_ID) %&gt;%\n  mutate(knowledge = paste(unique(df_TitleInfo$knowledge[df_TitleInfo$title_ID == title_ID]), collapse = \"_\"),\n         sub_knowledge = paste(unique(df_TitleInfo$sub_knowledge[df_TitleInfo$title_ID == title_ID]), collapse = \"_\")) %&gt;%\n  ungroup() %&gt;%\n  distinct(title_ID, .keep_all = TRUE)\n\n\n# Group by title_ID\ndf_TitleInfo_gp &lt;- df_TitleInfo3 %&gt;%\n  group_by(title_ID) %&gt;%\n  summarise_all(~first(.))\n\nglimpse(df_TitleInfo_gp)\n\nRows: 38\nColumns: 27\n$ title_ID       &lt;chr&gt; \"Question_3MwAFlmNO8EKrpY5zjUd\", \"Question_3oPyUzDmQtcM…\n$ knowledge      &lt;chr&gt; \"t5V9e\", \"t5V9e\", \"m3D1v\", \"g7R2j\", \"y9W5d\", \"m3D1v\", \"…\n$ sub_knowledge  &lt;chr&gt; \"t5V9e_e1k6cixp\", \"t5V9e_e1k6cixp\", \"m3D1v_r1d7fr3l\", \"…\n$ b3C9s          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j          &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ k4W1c          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v          &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0…\n$ r8S3g          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ s8Y2f          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ t5V9e          &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y9W5d          &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1…\n$ score          &lt;dbl&gt; 2, 2, 3, 3, 3, 3, 3, 3, 1, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3…\n$ b3C9s_j0v1yls8 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n$ b3C9s_l4z6od7y &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j_e0v1yls8 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j_j1g8gd3v &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ k4W1c_h5r6nux7 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v_r1d7fr3l &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0…\n$ m3D1v_t0v5ts9h &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0…\n$ m3D1v_v3d9is1x &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g_l0p5viby &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g_n0m9rsw4 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ s8Y2f_v4x8by9j &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ t5V9e_e1k6cixp &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y9W5d_c0w4mj5h &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1…\n$ y9W5d_e2j7p95s &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y9W5d_p8g6dgtv &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…\n\nunique(df_TitleInfo_gp$knowledge)\n\n [1] \"t5V9e\"       \"m3D1v\"       \"g7R2j\"       \"y9W5d\"       \"r8S3g\"      \n [6] \"b3C9s\"       \"y9W5d_m3D1v\" \"y9W5d_k4W1c\" \"g7R2j_m3D1v\" \"y9W5d_s8Y2f\"\n\nunique(df_TitleInfo_gp$sub_knowledge)\n\n [1] \"t5V9e_e1k6cixp\"                \"m3D1v_r1d7fr3l\"               \n [3] \"g7R2j_e0v1yls8\"                \"y9W5d_c0w4mj5h\"               \n [5] \"m3D1v_v3d9is1x\"                \"y9W5d_p8g6dgtv\"               \n [7] \"r8S3g_n0m9rsw4\"                \"y9W5d_e2j7p95s\"               \n [9] \"b3C9s_j0v1yls8\"                \"m3D1v_t0v5ts9h\"               \n[11] \"y9W5d_c0w4mj5h_m3D1v_r1d7fr3l\" \"r8S3g_l0p5viby\"               \n[13] \"g7R2j_j1g8gd3v\"                \"b3C9s_l4z6od7y\"               \n[15] \"y9W5d_c0w4mj5h_k4W1c_h5r6nux7\" \"g7R2j_e0v1yls8_m3D1v_r1d7fr3l\"\n[17] \"r8S3g_n0m9rsw4_r8S3g_l0p5viby\" \"y9W5d_c0w4mj5h_s8Y2f_v4x8by9j\"\n\n\n\nDuplicate class for each Individual Students in StudentRecord\n\n\n# Identify students with multiple classes\nstudents_multiple_classes &lt;- df_StudentRecord %&gt;%\n  group_by(student_ID) %&gt;%\n  summarise(unique_classes = n_distinct(class)) %&gt;%\n  filter(unique_classes &gt; 1)\n\nstudents_multiple_classes_entries &lt;- df_StudentRecord %&gt;%\n  filter(student_ID %in% students_multiple_classes$student_ID) %&gt;%\n  group_by(student_ID, class) %&gt;%\n  summarise(count = n()) %&gt;%\n  arrange(desc(count)) %&gt;%\n  arrange(desc(student_ID))\n\n# Display the results\nprint(students_multiple_classes_entries)\n\n# A tibble: 12 × 3\n# Groups:   student_ID [6]\n   student_ID           class   count\n   &lt;chr&gt;                &lt;chr&gt;   &lt;int&gt;\n 1 r9m46ndmmmzeeehft96z Class15   140\n 2 r9m46ndmmmzeeehft96z class       1\n 3 qz6jjynwbd3szlp0rj04 Class1    136\n 4 qz6jjynwbd3szlp0rj04 class       1\n 5 nd9xpohv0s4ttw0o7fts Class8    143\n 6 nd9xpohv0s4ttw0o7fts class       1\n 7 lqm8jh0uggps7yd0lx2x Class8    132\n 8 lqm8jh0uggps7yd0lx2x class       1\n 9 isa355t9q5rut5fm8aml Class1    142\n10 isa355t9q5rut5fm8aml class       1\n11 ezdogkk0jqt4nvvvbnxp Class7    125\n12 ezdogkk0jqt4nvvvbnxp class       1\n\n\nBased on the output above, it is apparent that the 2nd class for each of the student above is an erroneous value. Hence this inconsistency will be cleaned in the following code chunk\n\n# Step 1: Identify the correct class for each student (the class with the highest frequency)\ncorrect_classes &lt;- df_StudentRecord %&gt;%\n  filter(student_ID %in% students_multiple_classes$student_ID) %&gt;%\n  group_by(student_ID, class) %&gt;%\n  summarise(count = n()) %&gt;%\n  arrange(desc(count)) %&gt;%\n  slice(1) %&gt;%\n  select(student_ID, correct_class = class)\n\n# Step 2: Replace wrong class values\ndf_StudentRecord &lt;- df_StudentRecord %&gt;%\n  left_join(correct_classes, by = \"student_ID\") %&gt;%\n  mutate(class = ifelse(!is.na(correct_class), correct_class, class)) %&gt;%\n  select(-correct_class)\n\nFor completeness, a check is done for existence of other students with class that has no class number in the following code chunk.\n\nMissingClassNo &lt;- df_StudentRecord %&gt;%\n  filter(class == \"class\")\nMissingClassNo\n\n# A tibble: 0 × 10\n# ℹ 10 variables: index &lt;dbl&gt;, class &lt;chr&gt;, time &lt;dbl&gt;, state &lt;chr&gt;,\n#   score &lt;dbl&gt;, title_ID &lt;chr&gt;, method &lt;chr&gt;, memory &lt;dbl&gt;, timeconsume &lt;chr&gt;,\n#   student_ID &lt;chr&gt;\n\n\nBased on the output above, there are no further students with class without number.\n\n\nIdentifying Other Unexpected and/or Missing Values\n\nMissing Student_ID and title_ID in StudentRecord are also identified.\n\n\nmissing_students &lt;- anti_join(df_StudentRecord, df_StudentInfo, by = \"student_ID\")\n\n# Display the missing student IDs\nmissing_student_ids &lt;- missing_students %&gt;% select(student_ID) %&gt;% distinct()\nprint(missing_student_ids)\n\n# A tibble: 1 × 1\n  student_ID           \n  &lt;chr&gt;                \n1 44c7cf3881ae07f7fb3eD\n\n\n\nmissing_questions &lt;- anti_join(df_StudentRecord, df_TitleInfo, by = \"title_ID\")\n\n# Display the missing title IDs\nmissing_questions &lt;- missing_questions %&gt;% select(title_ID) %&gt;% distinct()\nprint(missing_questions)\n\n# A tibble: 0 × 1\n# ℹ 1 variable: title_ID &lt;chr&gt;\n\n\nThere is 1 missing student between either StudentRecord or StudentInfo, but no missing questions. Since there is partial missing info on this student, it isn’t meaningful to include in this analysis, hence the student_ID will be excluded in the following code chunk.\n\ndf_StudentInfo &lt;- df_StudentInfo %&gt;%\n  filter (student_ID != '44c7cf3881ae07f7fb3eD')\ndf_StudentRecord &lt;- df_StudentRecord %&gt;%\n  filter (student_ID != '44c7cf3881ae07f7fb3eD')\n\n\nOther unexpected values\n\nThe unique values for each column is queried to check for unexpected values in the following code chunk, wherein Index, time, class, title_ID and student_ID are excluded since they will be dealt with separately\n\nunique(df_StudentRecord$state)\n\n [1] \"Absolutely_Correct\" \"Error1\"             \"Absolutely_Error\"  \n [4] \"Error6\"             \"Error4\"             \"Partially_Correct\" \n [7] \"Error2\"             \"Error3\"             \"Error5\"            \n[10] \"Error7\"             \"Error8\"             \"Error9\"            \n[13] \"�������\"           \n\nunique(df_StudentRecord$score)\n\n[1] 3 4 0 1 2\n\nunique(df_StudentRecord$method)\n\n[1] \"Method_Cj9Ya2R7fZd6xs1q5mNQ\" \"Method_gj1NLb4Jn7URf9K2kQPd\"\n[3] \"Method_5Q4KoXthUuYz3bvrTDFm\" \"Method_m8vwGkEZc3TSW2xqYUoR\"\n[5] \"Method_BXr9AIsPQhwNvyGdZL57\"\n\nunique(df_StudentRecord$memory)\n\n  [1]   320   356   196   308     0   312   328   512   324   188   316   344\n [13]   444   192   332   484   360   200   340   184   476   492   180   448\n [25]   464  8544   204   496   364   460   508   456   352   480   348   488\n [37]   468   400   616   472   384   376   452   336   588   604   440   600\n [49]   580   500   640   520   436   368   612   504   736   632  8448   220\n [61]   372   208   828   256   568   576   628   756   620   700   212   592\n [73]   380   396   432   404   644   564   748   216   264   708   768   304\n [85]   420   624  8516  8644   288  8632  8640  8512   408   260   292   608\n [97]  8580   636   536   424   596   272   388   300   280   268   176   160\n[109]   296   416   240   284   248   172  8388   832  4164  4284   428   168\n[121]   572   164   276   528   392   412  8668  8500  8540  8664  8536  8576\n[133]  8628  8504  8800  8524  8392  8548   692   952  8508  8648  9664  9536\n[145]  9564 49852 59616  1332   948   824   724  2876  3024 24668 25208 26712\n[157] 23968   732 25248 22740   712  8520   720 18264   224  4984  8696 20272\n[169] 19576   516  8976  9028  9532   544   584   552   524  5624 29688   688\n[181] 30940 44020   740   556 51376 14656 65536   680 30440 30284 23128 28112\n[193]   760 15060 25660 23356 31796   804 24768 24232 12792 14720 26172 29020\n[205] 32992 28492 10568  8460  8404   908   652   540  8620 34268 11348 11640\n[217] 13124   532 12608 15028  1400 32544 39612 27272 28852 29248  8452  8616\n[229]  8480  8528   560 13576  8436   548  2012 24896   232 21728 21148  4424\n[241]  7640 43512 39912 19936 12580  2412  2436 24224  4296  4332  6392 25912\n[253] 21332 20128   668 35948  2360  8612  8384  5560 26548 25532 13112 15288\n[265] 13992 49336 53216 15040 13780  8496  8424 37184  8476  8400  8408 30656\n[277]  8156  8140  8064  8136 11236  5616  8160  4192 23116 19784 22908 21176\n[289] 18276 20708 19868 16348 18716 17208 19588 14824 20780 20204 24932 21084\n[301] 24992 21884 18764 26624 24368 13240 22988  3740 43532 26084 26320 13340\n[313] 11372 46460 49464 13356  8144  8564  1720 13892 14488 10580 23576  8396\n[325] 15212 15340   872 25648 25920 27028 24356 23544  7416  6560  4852  8556\n[337] 32088 32716 44216  4292   228 33212 33736 27228 27288 11764 10540 11560\n[349] 10456 11384 10708 32932 25940 17800 16764 46908 30512  9368  9472 19156\n[361]  2348 36136  8132  4708 39048 21152 30632 27200   656   252 47096  8552\n[373]  8464 14040 36984  2384  1792  6084  5844  2456  2440 26452 27364   648\n[385]   244 23168 24324  8420 41460 40568 34316   896  1472  7156 23740  6444\n[397]  6972  6200  6060  7488  6700  6580  5184  4948  5052  5820  6120  5404\n[409]  5028  5180  5100  5068  5020  5204  5976  5176  5048  5884  5824  5828\n[421]  5060  5072  5056  6076  6328  5076  8492  8428   236  7340  6668  7492\n[433]  8412  8652 17176  6852  6616  6032 45288 50140 40348 16848 21820 20856\n[445] 26296 28128 31560 17272 17656 37548 34476 38428 30456 41624 34224 18148\n[457] 20816   128   808   156   844   728   716   696   836   676  4324   860\n[469]  1980  8812   660  8636   684  8756   704  8532  8572  1920  1972  2332\n[481]  2172  2296  2280 13908 63088 15432 15680 15624 15824 15956 15724 15292\n[493]  8796  1880  1996  1992 11256 11268 11264 29240 29144 28752 27988  6068\n[505]  1180 28536 11032 39216 35632 28600  2104  8656 36028 38432 12456 30164\n[517]  1268  1328  1316  1240 50220  4540 35888  1976  4440 14336 14384 45680\n[529] 39080 28484 39104 53732  8680  8692  8660 14136  4564  4480 28848 29112\n[541] 18856  8792  8600  8592 41404 37052 36532 37804 33084 37368 30820 50620\n[553] 26248 22264 26616 25900   752 47040 14644 40636 43128 33568 36248 33088\n[565] 28140 28084 30532 30572 48376 47640 17400 20288 28724 20216 12664 12204\n[577] 11960 27188 15700 15664  4580  4584 28036 28732 34004 33508 31808  1528\n[589]  1716 13752  9592  9520  9784  9208  8828 28716 27536 28584  1704  1620\n[601] 13096 14132 14584 57528 45500  7096  2168  2236 12984 20412 31172 29296\n[613] 54356 54336 47548 41664 41812 13624  1336  1348 13496 55524  1352  1356\n[625] 42052   744   996   984   940  1016 29012 28080 26036  7344  7232  7476\n[637]  7828 13956 43452  1456  1324  1364 43196 27964 10812   972  1340  4692\n[649] 27248 44592 44860 46576 20464 52656 52996 48964 49516  6904  6592  6584\n[661]  8672 46852 40364 14500 14712 17740 17620 52584  8488 36488 44204 44500\n[673] 42300 45228 17980 37460 28240 28988 53288 58424  9540  9524  6936  6204\n[685] 54596 28604 29528 42804 12856 13776 15720  4156 12472  8704  8688 29300\n[697] 18612 12976 32376  8776 13548 26456  1884  1752   764  4172 53316 52160\n[709] 47036 45632 53396 51320 12468 11496 53604\n\nunique(df_StudentRecord$timeconsume)\n\n  [1] \"3\"   \"2\"   \"5\"   \"4\"   \"1\"   \"9\"   \"6\"   \"--\"  \"18\"  \"61\"  \"7\"   \"59\" \n [13] \"10\"  \"8\"   \"12\"  \"13\"  \"16\"  \"15\"  \"183\" \"68\"  \"314\" \"64\"  \"60\"  \"11\" \n [25] \"96\"  \"94\"  \"58\"  \"67\"  \"54\"  \"17\"  \"122\" \"19\"  \"126\" \"14\"  \"91\"  \"50\" \n [37] \"21\"  \"40\"  \"23\"  \"20\"  \"80\"  \"31\"  \"118\" \"400\" \"63\"  \"25\"  \"27\"  \"29\" \n [49] \"24\"  \"26\"  \"62\"  \"152\" \"39\"  \"22\"  \"117\" \"30\"  \"28\"  \"48\"  \"309\" \"331\"\n [61] \"36\"  \"65\"  \"47\"  \"46\"  \"45\"  \"52\"  \"32\"  \"42\"  \"34\"  \"38\"  \"187\" \"37\" \n [73] \"190\" \"163\" \"41\"  \"53\"  \"51\"  \"307\" \"201\" \"184\" \"44\"  \"43\"  \"109\" \"33\" \n [85] \"66\"  \"326\" \"73\"  \"49\"  \"77\"  \"82\"  \"70\"  \"71\"  \"81\"  \"35\"  \"57\"  \"75\" \n [97] \"394\" \"385\" \"164\" \"78\"  \"220\" \"217\" \"115\" \"86\"  \"72\"  \"88\"  \"76\"  \"134\"\n[109] \"55\"  \"84\"  \"56\"  \"106\" \"166\" \"124\" \"373\" \"289\" \"-\"   \"135\" \"103\" \"114\"\n[121] \"258\" \"254\" \"85\"  \"69\"  \"90\"  \"132\" \"173\" \"272\" \"113\" \"116\" \"215\" \"123\"\n[133] \"246\" \"146\" \"89\"  \"245\" \"285\" \"205\" \"162\" \"165\" \"266\" \"172\" \"143\" \"377\"\n[145] \"160\" \"159\" \"182\" \"74\"  \"264\" \"153\" \"83\"  \"286\" \"275\" \"280\" \"274\" \"269\"\n[157] \"288\" \"271\" \"136\" \"276\" \"277\" \"356\" \"79\"  \"147\" \"350\" \"315\" \"321\" \"302\"\n\n\n\nunique(df_StudentInfo$sex)\n\n[1] \"female\" \"male\"  \n\nunique(df_StudentInfo$age)\n\n[1] 24 21 23 22 19 18 20\n\nunique(df_StudentInfo$major)\n\n[1] \"J23517\" \"J87654\" \"J78901\" \"J40192\" \"J57489\"\n\n\n\nunique(df_TitleInfo$score)\n\n[1] 1 2 3 4\n\nunique(df_TitleInfo$knowledge)\n\n[1] \"r8S3g\" \"t5V9e\" \"m3D1v\" \"y9W5d\" \"k4W1c\" \"s8Y2f\" \"g7R2j\" \"b3C9s\"\n\nunique(df_TitleInfo$sub_knowledge)\n\n [1] \"r8S3g_l0p5viby\" \"r8S3g_n0m9rsw4\" \"t5V9e_e1k6cixp\" \"m3D1v_r1d7fr3l\"\n [5] \"m3D1v_v3d9is1x\" \"m3D1v_t0v5ts9h\" \"y9W5d_c0w4mj5h\" \"k4W1c_h5r6nux7\"\n [9] \"s8Y2f_v4x8by9j\" \"y9W5d_p8g6dgtv\" \"y9W5d_e2j7p95s\" \"g7R2j_e0v1yls8\"\n[13] \"g7R2j_j1g8gd3v\" \"b3C9s_l4z6od7y\" \"b3C9s_j0v1yls8\"\n\n\nFrom the outputs above, there is an unexpected value for state and timeconsume in StudentRecord.\nStarting with state, the rows with unexpected value(s) are queried in the following code chunk to better understand the number of affected rows.\n\nOutlier_state &lt;- df_StudentRecord %&gt;%\n  filter (state == '�������')\nOutlier_state\n\n# A tibble: 6 × 10\n  index class     time state score title_ID method memory timeconsume student_ID\n  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;     \n1  6344 Class10 1.70e9 ����…     0 Questio… Metho…  65536 309         c681117f7…\n2  6346 Class10 1.70e9 ����…     0 Questio… Metho…  65536 331         c681117f7…\n3  6347 Class10 1.70e9 ����…     0 Questio… Metho…  65536 331         c681117f7…\n4 10138 Class8  1.69e9 ����…     0 Questio… Metho…  65536 356         1883af270…\n5 16420 Class8  1.69e9 ����…     0 Questio… Metho…  65536 356         hpb03ydul…\n6 16458 Class8  1.69e9 ����…     0 Questio… Metho…  65536 356         ljylby8in…\n\n\nFrom the output above, there are only 6 rows that are affected. Further cross-validation with the data description document found that there should only be 12 unique values for this variable, and including this outlier state value will give 13. Hence this is likely a wrong entry, and so it will be excluded from the analysis in the following code chunk.\n\ndf_StudentRecord &lt;- df_StudentRecord %&gt;%\n  filter (state != '�������')\n\nFor timeconsume, the rows with unexpected value(s) are queried in the following code chunk to better understand the number of affected rows.\n\nOutlier_timeconsume &lt;- df_StudentRecord %&gt;%\n  filter (timeconsume %in% c('-', '--'))\nOutlier_timeconsume\n\n# A tibble: 2,612 × 10\n   index class    time state score title_ID method memory timeconsume student_ID\n   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;     \n 1   191 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          9417c1b4c…\n 2   321 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          8b1fbc973…\n 3   322 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          8b1fbc973…\n 4   366 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          9ea29e4a7…\n 5   396 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          9ea29e4a7…\n 6   397 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          9ea29e4a7…\n 7   422 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          f06c3ddb1…\n 8   423 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          f06c3ddb1…\n 9   424 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          f06c3ddb1…\n10   425 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          f06c3ddb1…\n# ℹ 2,602 more rows\n\n\nBased on the output, there is a significant number of 2,612 rows with the unexpected value. Hence these rows will be kept in the analysis and replaced with 0 (since there is no existing values of 0 too), however subsequent analysis in this exercise involving the timeconsume variable will treat these values as missing values. This is done in the following code chunk\n\ndf_StudentRecord &lt;- df_StudentRecord %&gt;%\n  mutate(timeconsume = ifelse(timeconsume %in% c(\"-\", \"--\"), 0, timeconsume))\nunique(df_StudentRecord$timeconsume)\n\n  [1] \"3\"   \"2\"   \"5\"   \"4\"   \"1\"   \"9\"   \"6\"   \"0\"   \"18\"  \"61\"  \"7\"   \"59\" \n [13] \"10\"  \"8\"   \"12\"  \"13\"  \"16\"  \"15\"  \"183\" \"68\"  \"314\" \"64\"  \"60\"  \"11\" \n [25] \"96\"  \"94\"  \"58\"  \"67\"  \"54\"  \"17\"  \"122\" \"19\"  \"126\" \"14\"  \"91\"  \"50\" \n [37] \"21\"  \"40\"  \"23\"  \"20\"  \"80\"  \"31\"  \"118\" \"400\" \"63\"  \"25\"  \"27\"  \"29\" \n [49] \"24\"  \"26\"  \"62\"  \"152\" \"39\"  \"22\"  \"117\" \"30\"  \"28\"  \"48\"  \"36\"  \"65\" \n [61] \"47\"  \"46\"  \"45\"  \"52\"  \"32\"  \"42\"  \"34\"  \"38\"  \"187\" \"37\"  \"190\" \"163\"\n [73] \"41\"  \"53\"  \"51\"  \"307\" \"201\" \"184\" \"44\"  \"43\"  \"109\" \"33\"  \"66\"  \"326\"\n [85] \"73\"  \"49\"  \"77\"  \"82\"  \"70\"  \"71\"  \"81\"  \"35\"  \"57\"  \"75\"  \"394\" \"385\"\n [97] \"164\" \"78\"  \"220\" \"217\" \"115\" \"86\"  \"72\"  \"88\"  \"76\"  \"134\" \"55\"  \"84\" \n[109] \"56\"  \"106\" \"166\" \"124\" \"373\" \"289\" \"135\" \"103\" \"114\" \"258\" \"254\" \"85\" \n[121] \"69\"  \"90\"  \"132\" \"173\" \"272\" \"113\" \"116\" \"215\" \"123\" \"246\" \"146\" \"89\" \n[133] \"245\" \"285\" \"205\" \"162\" \"165\" \"266\" \"172\" \"143\" \"377\" \"160\" \"159\" \"182\"\n[145] \"74\"  \"264\" \"153\" \"83\"  \"286\" \"275\" \"331\" \"280\" \"274\" \"269\" \"288\" \"271\"\n[157] \"136\" \"276\" \"277\" \"79\"  \"147\" \"350\" \"315\" \"321\" \"302\"\n\n\n\n\nRemoving Index Col\nEach data set contains an index column, which is possibly to keep track of the original order and the total number of rows. This is no longer required and relevant in the analysis, hence it will be excluded.\n\n#remove index column\ndf_StudentRecord &lt;- df_StudentRecord %&gt;% select(-1)\ndf_TitleInfo &lt;- df_TitleInfo %&gt;% select(-1)\ndf_StudentInfo &lt;- df_StudentInfo %&gt;% select(-1)\n\n\n\nCorrecting Data Types\nBased on the glimpse() function, the time variable of the StudentRecord is currently in numerical format. This will be corrected to date time format with the following steps.\nStep 1: From the data description document, the data collection period spans 148 days from 31/8/2023 to 25/1/2024, and the time variable of the StudentRecord in this data set is in seconds. This is compared against the min and max values of the time variable converted to days and deducted from the given start and end date of the collection period given, in the following code chunk.\n\n# Get the min and max values of the time column\nmin_time &lt;- min(df_StudentRecord$time, na.rm = TRUE)\nmax_time &lt;- max(df_StudentRecord$time, na.rm = TRUE)\n\n# Display the min & max values\ndate_adjustment1 &lt;- as.numeric(as.Date(\"2023-08-31\")) - (min_time / 24 / 60 / 60)\ndate_adjustment2 &lt;- as.numeric(as.Date(\"2024-01-25\")) - (max_time / 24 / 60 / 60)\ndate_adjustmentavg &lt;- as.Date((date_adjustment1 + date_adjustment2)/2, origin = \"1970-01-01\")\ndate_adjustmentavg\n\n[1] \"1969-12-31\"\n\n\nStep 2: Apply date_adjustmentavg to the time variable to amend the data type to date time format in the folloiwing code chunk\n\n# Convert time from timestamp to POSIXct\ndf_StudentRecord$time_change &lt;- as.POSIXct(df_StudentRecord$time, origin=date_adjustmentavg, tz=\"UTC\")\n\nglimpse(df_StudentRecord)\n\nRows: 232,811\nColumns: 10\n$ class       &lt;chr&gt; \"Class1\", \"Class1\", \"Class1\", \"Class1\", \"Class1\", \"Class1\"…\n$ time        &lt;dbl&gt; 1704209872, 1704209852, 1704209838, 1704208923, 1704208359…\n$ state       &lt;chr&gt; \"Absolutely_Correct\", \"Absolutely_Correct\", \"Absolutely_Co…\n$ score       &lt;dbl&gt; 3, 3, 3, 3, 4, 0, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 4, 0, 0…\n$ title_ID    &lt;chr&gt; \"Question_bumGRTJ0c8p4v5D6eHZa\", \"Question_62XbhBvJ8NUSnAp…\n$ method      &lt;chr&gt; \"Method_Cj9Ya2R7fZd6xs1q5mNQ\", \"Method_gj1NLb4Jn7URf9K2kQP…\n$ memory      &lt;dbl&gt; 320, 356, 196, 308, 320, 0, 308, 312, 312, 328, 512, 324, …\n$ timeconsume &lt;chr&gt; \"3\", \"3\", \"2\", \"2\", \"3\", \"5\", \"2\", \"2\", \"3\", \"2\", \"3\", \"2\"…\n$ student_ID  &lt;chr&gt; \"8b6d1125760bd3939b6e\", \"8b6d1125760bd3939b6e\", \"8b6d11257…\n$ time_change &lt;dttm&gt; 2024-01-02 08:45:17, 2024-01-02 08:44:57, 2024-01-02 08:4…\n\n\nFurther, the timeconsume variable will be converted to numeric, wherein since the ‘-’ and ‘–’ values found earlier had taken the value of 0 and there will not be an issue of NA values affecting subsequent analysis.\n\ndf_StudentRecord &lt;- df_StudentRecord %&gt;%\n  mutate(timeconsume = as.numeric(timeconsume))\n\nglimpse(df_StudentRecord)\n\nRows: 232,811\nColumns: 10\n$ class       &lt;chr&gt; \"Class1\", \"Class1\", \"Class1\", \"Class1\", \"Class1\", \"Class1\"…\n$ time        &lt;dbl&gt; 1704209872, 1704209852, 1704209838, 1704208923, 1704208359…\n$ state       &lt;chr&gt; \"Absolutely_Correct\", \"Absolutely_Correct\", \"Absolutely_Co…\n$ score       &lt;dbl&gt; 3, 3, 3, 3, 4, 0, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 4, 0, 0…\n$ title_ID    &lt;chr&gt; \"Question_bumGRTJ0c8p4v5D6eHZa\", \"Question_62XbhBvJ8NUSnAp…\n$ method      &lt;chr&gt; \"Method_Cj9Ya2R7fZd6xs1q5mNQ\", \"Method_gj1NLb4Jn7URf9K2kQP…\n$ memory      &lt;dbl&gt; 320, 356, 196, 308, 320, 0, 308, 312, 312, 328, 512, 324, …\n$ timeconsume &lt;dbl&gt; 3, 3, 2, 2, 3, 5, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 5…\n$ student_ID  &lt;chr&gt; \"8b6d1125760bd3939b6e\", \"8b6d1125760bd3939b6e\", \"8b6d11257…\n$ time_change &lt;dttm&gt; 2024-01-02 08:45:17, 2024-01-02 08:44:57, 2024-01-02 08:4…\n\n\n\n\n\nCreate Merged Dataset\nTo prepare for cross dataset visualisation and analysis of variables, the 3 data sets are joined on title_id and student_id variables in the following code chunks.\n\n# Merge StudentInfo with SubmitRecord based on student_ID\nmerged_data &lt;- merge(df_StudentRecord, df_StudentInfo, by = \"student_ID\")\n\n# Merge TitleInfo with the already merged data based on title_ID\nmerged_data &lt;- merge(merged_data, df_TitleInfo_gp, by = \"title_ID\")\n\nmerged_data &lt;- merged_data %&gt;%\n  rename(\n    actual_score = score.x,\n    question_score = score.y\n  )\n\n\nsaveRDS(merged_data, \"merged_data_df.rds\")\n\n\nsummary (merged_data)\n\n   title_ID          student_ID           class                time          \n Length:232811      Length:232811      Length:232811      Min.   :1.693e+09  \n Class :character   Class :character   Class :character   1st Qu.:1.697e+09  \n Mode  :character   Mode  :character   Mode  :character   Median :1.699e+09  \n                                                          Mean   :1.699e+09  \n                                                          3rd Qu.:1.701e+09  \n                                                          Max.   :1.706e+09  \n    state            actual_score       method              memory       \n Length:232811      Min.   :0.0000   Length:232811      Min.   :    0.0  \n Class :character   1st Qu.:0.0000   Class :character   1st Qu.:  188.0  \n Mode  :character   Median :0.0000   Mode  :character   Median :  324.0  \n                    Mean   :0.8992                      Mean   :  345.7  \n                    3rd Qu.:2.0000                      3rd Qu.:  356.0  \n                    Max.   :4.0000                      Max.   :65536.0  \n  timeconsume       time_change                         sex           \n Min.   :  0.000   Min.   :2023-08-31 01:53:48.50   Length:232811     \n 1st Qu.:  3.000   1st Qu.:2023-10-08 05:16:53.50   Class :character  \n Median :  4.000   Median :2023-10-29 04:48:53.50   Mode  :character  \n Mean   :  8.991   Mean   :2023-10-31 19:22:50.95                     \n 3rd Qu.:  5.000   3rd Qu.:2023-11-25 08:34:41.00                     \n Max.   :400.000   Max.   :2024-01-24 22:06:11.50                     \n      age           major            knowledge         sub_knowledge     \n Min.   :18.00   Length:232811      Length:232811      Length:232811     \n 1st Qu.:19.00   Class :character   Class :character   Class :character  \n Median :21.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :21.06                                                           \n 3rd Qu.:23.00                                                           \n Max.   :24.00                                                           \n     b3C9s             g7R2j            k4W1c       m3D1v       \n Min.   :0.00000   Min.   :0.0000   Min.   :0   Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0   1st Qu.:0.0000  \n Median :0.00000   Median :0.0000   Median :0   Median :0.0000  \n Mean   :0.06348   Mean   :0.1419   Mean   :0   Mean   :0.2051  \n 3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0   3rd Qu.:0.0000  \n Max.   :1.00000   Max.   :1.0000   Max.   :0   Max.   :1.0000  \n     r8S3g            s8Y2f       t5V9e            y9W5d        question_score \n Min.   :0.0000   Min.   :0   Min.   :0.0000   Min.   :0.0000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:0   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:2.000  \n Median :0.0000   Median :0   Median :0.0000   Median :0.0000   Median :3.000  \n Mean   :0.1578   Mean   :0   Mean   :0.1631   Mean   :0.2686   Mean   :2.549  \n 3rd Qu.:0.0000   3rd Qu.:0   3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:3.000  \n Max.   :1.0000   Max.   :0   Max.   :1.0000   Max.   :1.0000   Max.   :4.000  \n b3C9s_j0v1yls8   b3C9s_l4z6od7y    g7R2j_e0v1yls8  g7R2j_j1g8gd3v   \n Min.   :0.0000   Min.   :0.00000   Min.   :0.000   Min.   :0.00000  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.000   1st Qu.:0.00000  \n Median :0.0000   Median :0.00000   Median :0.000   Median :0.00000  \n Mean   :0.0272   Mean   :0.03629   Mean   :0.116   Mean   :0.02595  \n 3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:0.000   3rd Qu.:0.00000  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.000   Max.   :1.00000  \n k4W1c_h5r6nux7 m3D1v_r1d7fr3l  m3D1v_t0v5ts9h    m3D1v_v3d9is1x   \n Min.   :0      Min.   :0.000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:0      1st Qu.:0.000   1st Qu.:0.00000   1st Qu.:0.00000  \n Median :0      Median :0.000   Median :0.00000   Median :0.00000  \n Mean   :0      Mean   :0.147   Mean   :0.02589   Mean   :0.03214  \n 3rd Qu.:0      3rd Qu.:0.000   3rd Qu.:0.00000   3rd Qu.:0.00000  \n Max.   :0      Max.   :1.000   Max.   :1.00000   Max.   :1.00000  \n r8S3g_l0p5viby    r8S3g_n0m9rsw4   s8Y2f_v4x8by9j t5V9e_e1k6cixp  \n Min.   :0.00000   Min.   :0.0000   Min.   :0      Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0      1st Qu.:0.0000  \n Median :0.00000   Median :0.0000   Median :0      Median :0.0000  \n Mean   :0.02526   Mean   :0.1325   Mean   :0      Mean   :0.1631  \n 3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0      3rd Qu.:0.0000  \n Max.   :1.00000   Max.   :1.0000   Max.   :0      Max.   :1.0000  \n y9W5d_c0w4mj5h   y9W5d_e2j7p95s    y9W5d_p8g6dgtv   \n Min.   :0.0000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.00000  \n Median :0.0000   Median :0.00000   Median :0.00000  \n Mean   :0.1983   Mean   :0.02861   Mean   :0.04174  \n 3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:0.00000  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.00000"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take_home_Ex3.html#learner-modes",
    "href": "Take-home_Ex/Take-home_Ex3/Take_home_Ex3.html#learner-modes",
    "title": "Take-home Ex3",
    "section": "Learner modes",
    "text": "Learner modes\n\nFeature engineering\nSplitting Date and time up from the earlier created time_change date-time variable with the following code chunk\n\nmerged_data &lt;- merged_data %&gt;%\n  mutate(\n    date = as.Date(time_change),\n    time = as_hms(format(time_change, \"%H:%M:%S\"))\n)\nglimpse(merged_data)\n\nRows: 232,811\nColumns: 40\n$ title_ID       &lt;chr&gt; \"Question_3MwAFlmNO8EKrpY5zjUd\", \"Question_3MwAFlmNO8EK…\n$ student_ID     &lt;chr&gt; \"d554e419f820fa5cb0ca\", \"b92448e12093e45dc6ff\", \"6b2292…\n$ class          &lt;chr&gt; \"Class9\", \"Class8\", \"Class12\", \"Class7\", \"Class1\", \"Cla…\n$ time           &lt;time&gt; 04:09:22, 07:11:39, 01:22:28, 22:25:49, 08:11:04, 02:4…\n$ state          &lt;chr&gt; \"Partially_Correct\", \"Partially_Correct\", \"Error1\", \"Pa…\n$ actual_score   &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1…\n$ method         &lt;chr&gt; \"Method_BXr9AIsPQhwNvyGdZL57\", \"Method_BXr9AIsPQhwNvyGd…\n$ memory         &lt;dbl&gt; 196, 332, 0, 196, 0, 0, 336, 320, 324, 204, 340, 320, 3…\n$ timeconsume    &lt;dbl&gt; 2, 6, 2, 3, 4, 3, 4, 4, 2, 2, 3, 5, 3, 4, 3, 5, 1, 3, 3…\n$ time_change    &lt;dttm&gt; 2023-10-03 04:09:22, 2023-11-10 07:11:39, 2023-10-16 0…\n$ sex            &lt;chr&gt; \"male\", \"female\", \"female\", \"male\", \"male\", \"male\", \"ma…\n$ age            &lt;dbl&gt; 19, 21, 23, 20, 21, 20, 19, 20, 21, 21, 21, 21, 21, 21,…\n$ major          &lt;chr&gt; \"J40192\", \"J23517\", \"J87654\", \"J87654\", \"J40192\", \"J401…\n$ knowledge      &lt;chr&gt; \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"…\n$ sub_knowledge  &lt;chr&gt; \"t5V9e_e1k6cixp\", \"t5V9e_e1k6cixp\", \"t5V9e_e1k6cixp\", \"…\n$ b3C9s          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ k4W1c          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ s8Y2f          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ t5V9e          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ y9W5d          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ question_score &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ b3C9s_j0v1yls8 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ b3C9s_l4z6od7y &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j_e0v1yls8 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j_j1g8gd3v &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ k4W1c_h5r6nux7 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v_r1d7fr3l &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v_t0v5ts9h &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v_v3d9is1x &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g_l0p5viby &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g_n0m9rsw4 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ s8Y2f_v4x8by9j &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ t5V9e_e1k6cixp &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ y9W5d_c0w4mj5h &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y9W5d_e2j7p95s &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y9W5d_p8g6dgtv &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ date           &lt;date&gt; 2023-10-03, 2023-11-10, 2023-10-16, 2023-09-28, 2023-1…\n\n\n\n\nDetermine number of cluster for KMeans\n\nSilhouette analysis\n\n# Function to compute silhouette width for different numbers of clusters\nsilhouette_analysis &lt;- function(merged_data, max_clusters) {\n  # Convert character columns to factors\n  merged_data &lt;- merged_data %&gt;%\n    mutate_if(is.character, as.factor)\n  \n    # Compute Gower distance matrix\n  gower_dist &lt;- daisy(merged_data, metric = \"gower\")\n  \n  avg_sil_widths &lt;- numeric(max_clusters)\n  \n  for (k in 2:max_clusters) {\n    # Perform clustering using PAM\n    pam_result &lt;- pam(gower_dist, diss = TRUE, k = k)\n    \n    # Compute silhouette widths\n    sil &lt;- silhouette(pam_result$clustering, gower_dist)\n    \n    # Calculate average silhouette width\n    avg_sil_widths[k] &lt;- mean(sil[, 3])\n  }\n  \n  return(avg_sil_widths)\n}\n\n# Sample data (replace this with your actual data)\nset.seed(123)\nmerged_data &lt;- data.frame(\n  x = rnorm(100),\n  y = rnorm(100),\n  category = sample(LETTERS[1:3], 100, replace = TRUE) # Categorical column\n)\n\n# Determine the maximum number of clusters to test\nmax_clusters &lt;- 10\n\n# Perform silhouette analysis\navg_sil_widths &lt;- silhouette_analysis(merged_data, max_clusters)\n\n# Plot the average silhouette widths\nplot(2:max_clusters, avg_sil_widths[2:max_clusters], type = \"b\", pch = 19, frame = FALSE,\n     xlab = \"Number of clusters\", ylab = \"Average silhouette width\",\n     main = \"Silhouette Analysis for Determining Optimal Number of Clusters\")\n\n# Highlight the optimal number of clusters\noptimal_clusters &lt;- which.max(avg_sil_widths)\npoints(optimal_clusters, avg_sil_widths[optimal_clusters], col = \"red\", pch = 19)\n\n\n\n\n\n\nSSE-Elbow method\n\n# Function to one-hot encode categorical variables\none_hot_encode &lt;- function(df) {\n  # Convert character columns to factors\n  df &lt;- df %&gt;%\n    mutate_if(is.character, as.factor)\n  \n  # Apply one-hot encoding\n  dummies &lt;- dummyVars(~ ., data = df)\n  df_encoded &lt;- data.frame(predict(dummies, newdata = df))\n  \n  return(df_encoded)\n}\n\n# Function to compute SSE for different numbers of clusters\ncompute_sse &lt;- function(data, max_clusters) {\n  sse &lt;- numeric(max_clusters)\n  \n  for (k in 1:max_clusters) {\n    # Perform k-means clustering\n    kmeans_result &lt;- kmeans(data, centers = k, nstart = 25)\n    \n    # Compute SSE\n    sse[k] &lt;- kmeans_result$tot.withinss\n  }\n  \n  return(sse)\n}\n\n# One-hot encode the merged_data\nmerged_data_encoded &lt;- one_hot_encode(merged_data)\n\n# Determine the maximum number of clusters to test\nmax_clusters &lt;- 10\n\n# Compute SSE for each number of clusters\nsse_values &lt;- compute_sse(merged_data_encoded, max_clusters)\n\n# Plot SSE against number of clusters\nplot(1:max_clusters, sse_values, type = \"b\", pch = 19, frame = FALSE,\n     xlab = \"Number of clusters\", ylab = \"SSE\",\n     main = \"Elbow Method for Optimal Number of Clusters\")\n\n# Add text for elbow point\nelbow_point &lt;- which.min(diff(sse_values)) + 1\ntext(elbow_point, sse_values[elbow_point], labels = paste(\"Elbow Point:\", elbow_point), pos = 4, col = \"red\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take_home_Ex3.html#learning-modes",
    "href": "Take-home_Ex/Take-home_Ex3/Take_home_Ex3.html#learning-modes",
    "title": "Take-home Ex3",
    "section": "Learning modes",
    "text": "Learning modes\nBased on the given data, the relevant features that best defines a learner’s learning mode is assessed to be as follows:\n\nPeak answering hours determined by (a) day of the week and (b) time of the day\nVariety of question types attempted determined by (a) total number of different questions attempted, (b) total number of different knowledge and sub knowledge areas covered,\nDepth of question types and answers determined by (a) mean question scores, (b) mean memory size of file submissions\nLevel of learning effort determined by (a) total number of answering attempts, (b) total number of different answering methods, (C) total memory size of file submission\n\n\nFeature engineering\n\nPeak answering hours Boolean Integer Variables\nSplitting Date and time up from the earlier created time_change date-time variable, and adding 2 derived variables for boolean integer values for weekday (Mon to Fri) and working hours (8am to 8pm) with the following code chunk.\n\nmerged_data_lm &lt;- merged_data %&gt;%\n  mutate(\n    date = as.Date(time_change),\n    time = as_hms(format(time_change, \"%H:%M:%S\")),\n    is_weekday = as.numeric(wday(date) %in% 2:6),  # Monday to Friday 1, else 0\n    is_working_hours = as.numeric(hour(time) &gt;= 8 & hour(time) &lt; 20)  # 8am to 8pm 1, else 0\n  )\n\nglimpse(merged_data_lm)\n\nRows: 232,811\nColumns: 42\n$ title_ID         &lt;chr&gt; \"Question_3MwAFlmNO8EKrpY5zjUd\", \"Question_3MwAFlmNO8…\n$ student_ID       &lt;chr&gt; \"d554e419f820fa5cb0ca\", \"b92448e12093e45dc6ff\", \"6b22…\n$ class            &lt;chr&gt; \"Class9\", \"Class8\", \"Class12\", \"Class7\", \"Class1\", \"C…\n$ time             &lt;time&gt; 04:09:22, 07:11:39, 01:22:28, 22:25:49, 08:11:04, 02…\n$ state            &lt;chr&gt; \"Partially_Correct\", \"Partially_Correct\", \"Error1\", \"…\n$ actual_score     &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 1, 0, 1, 1, 0, 1, 0, 1,…\n$ method           &lt;chr&gt; \"Method_BXr9AIsPQhwNvyGdZL57\", \"Method_BXr9AIsPQhwNvy…\n$ memory           &lt;dbl&gt; 196, 332, 0, 196, 0, 0, 336, 320, 324, 204, 340, 320,…\n$ timeconsume      &lt;dbl&gt; 2, 6, 2, 3, 4, 3, 4, 4, 2, 2, 3, 5, 3, 4, 3, 5, 1, 3,…\n$ time_change      &lt;dttm&gt; 2023-10-03 04:09:22, 2023-11-10 07:11:39, 2023-10-16…\n$ sex              &lt;chr&gt; \"male\", \"female\", \"female\", \"male\", \"male\", \"male\", \"…\n$ age              &lt;dbl&gt; 19, 21, 23, 20, 21, 20, 19, 20, 21, 21, 21, 21, 21, 2…\n$ major            &lt;chr&gt; \"J40192\", \"J23517\", \"J87654\", \"J87654\", \"J40192\", \"J4…\n$ knowledge        &lt;chr&gt; \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\",…\n$ sub_knowledge    &lt;chr&gt; \"t5V9e_e1k6cixp\", \"t5V9e_e1k6cixp\", \"t5V9e_e1k6cixp\",…\n$ b3C9s            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ g7R2j            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ k4W1c            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ m3D1v            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ r8S3g            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ s8Y2f            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ t5V9e            &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ y9W5d            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ question_score   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ b3C9s_j0v1yls8   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ b3C9s_l4z6od7y   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ g7R2j_e0v1yls8   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ g7R2j_j1g8gd3v   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ k4W1c_h5r6nux7   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ m3D1v_r1d7fr3l   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ m3D1v_t0v5ts9h   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ m3D1v_v3d9is1x   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ r8S3g_l0p5viby   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ r8S3g_n0m9rsw4   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ s8Y2f_v4x8by9j   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ t5V9e_e1k6cixp   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ y9W5d_c0w4mj5h   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ y9W5d_e2j7p95s   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ y9W5d_p8g6dgtv   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ date             &lt;date&gt; 2023-10-03, 2023-11-10, 2023-10-16, 2023-09-28, 2023…\n$ is_weekday       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,…\n$ is_working_hours &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,…\n\n\n\n\nGroup By Student ID\nThe following variables will be obtained with the code chunk below in preparation for clustering analysis\n\nPeak answering hours\n\n\npercentage of answers on weekdays,\npercentage of answers during working hours\n\n\nVariety of question types attempted\n\n\ntotal number of different questions attempted,\ntotal number of different knowledge and sub knowledge areas covered,\n\n\nDepth of question types and answers\n\n\nmean question scores,\nmean memory size of file submissions\n\n\nLevel of learning effort\n\n\ntotal number of answering attempts,\ntotal number of different answering methods,\n\n\n\ntotal memory size of file submission\n\n\nStudentLM_data &lt;- merged_data_lm %&gt;%\n  group_by(student_ID) %&gt;%\n  summarize(\n    pct_answers_weekdays = sum(is_weekday, na.rm = TRUE) / n() * 100,\n    pct_answers_working_hours = sum(is_working_hours, na.rm = TRUE) / n() * 100,\n    total_diff_questions_attempted = n_distinct(title_ID, na.rm = TRUE),\n    total_diff_knowledge_areas = sum(colSums(across(16:23, as.numeric)) &gt; 0),\n    total_diff_sub_knowledge_areas = sum(colSums(across(25:39, as.numeric)) &gt; 0),\n    mean_question_scores = mean(question_score, na.rm = TRUE),\n    mean_memory_size = mean(memory, na.rm = TRUE),\n    total_answering_attempts = n(),\n    total_diff_answering_methods = n_distinct(method, na.rm = TRUE),\n    total_memory_size = sum(memory, na.rm = TRUE)\n  )\n\nglimpse(StudentLM_data)\n\nRows: 1,364\nColumns: 11\n$ student_ID                     &lt;chr&gt; \"0088dc183f73c83f763e\", \"00cbf05221bb47…\n$ pct_answers_weekdays           &lt;dbl&gt; 94.88372, 88.75000, 78.03347, 65.54622,…\n$ pct_answers_working_hours      &lt;dbl&gt; 14.883721, 10.416667, 15.899582, 21.008…\n$ total_diff_questions_attempted &lt;int&gt; 38, 38, 38, 38, 38, 38, 38, 38, 38, 35,…\n$ total_diff_knowledge_areas     &lt;int&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 2, 6, 6, …\n$ total_diff_sub_knowledge_areas &lt;int&gt; 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,…\n$ mean_question_scores           &lt;dbl&gt; 2.339535, 2.266667, 2.744770, 2.546218,…\n$ mean_memory_size               &lt;dbl&gt; 224.0186, 407.5833, 285.5565, 273.8151,…\n$ total_answering_attempts       &lt;int&gt; 215, 240, 478, 119, 204, 145, 253, 543,…\n$ total_diff_answering_methods   &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 5, 5, …\n$ total_memory_size              &lt;dbl&gt; 48164, 97820, 136496, 32584, 35260, 424…\n\n\n\n\nRemoving highly skewed columns\nInspecting the data frame, 3 variables were found to be highly skewed and concentrated within a small range of values, hence they are removed for more meaningful analysis, with the following code chunk.\n\nStudentLM_data &lt;- StudentLM_data %&gt;%\n  select(-total_diff_knowledge_areas, -total_diff_sub_knowledge_areas, -total_diff_answering_methods)\n\nglimpse(StudentLM_data)\n\nRows: 1,364\nColumns: 8\n$ student_ID                     &lt;chr&gt; \"0088dc183f73c83f763e\", \"00cbf05221bb47…\n$ pct_answers_weekdays           &lt;dbl&gt; 94.88372, 88.75000, 78.03347, 65.54622,…\n$ pct_answers_working_hours      &lt;dbl&gt; 14.883721, 10.416667, 15.899582, 21.008…\n$ total_diff_questions_attempted &lt;int&gt; 38, 38, 38, 38, 38, 38, 38, 38, 38, 35,…\n$ mean_question_scores           &lt;dbl&gt; 2.339535, 2.266667, 2.744770, 2.546218,…\n$ mean_memory_size               &lt;dbl&gt; 224.0186, 407.5833, 285.5565, 273.8151,…\n$ total_answering_attempts       &lt;int&gt; 215, 240, 478, 119, 204, 145, 253, 543,…\n$ total_memory_size              &lt;dbl&gt; 48164, 97820, 136496, 32584, 35260, 424…\n\n\n\n\n\nDetermine number of K-Means clusters\nTo determine the ideal number of clusters for K-means clustering on the recompiled learners’ learning mode features, a silhouette analysis and SSE elbow method are performed in the following code chunks.\n\nSilhouette analysis\n\n# Exclude non-numeric columns\nStudentLM_data_numeric &lt;- StudentLM_data %&gt;%\n  select(-student_ID)\n\n# Function to compute silhouette widths\nsilhouette_analysis &lt;- function(data, max_clusters) {\n  avg_sil_widths &lt;- numeric(max_clusters)\n  \n  for (k in 2:max_clusters) {\n    # Perform k-means clustering\n    kmeans_result &lt;- kmeans(data, centers = k, nstart = 25)\n    \n    # Compute silhouette widths\n    sil &lt;- silhouette(kmeans_result$cluster, dist(data))\n    \n    # Calculate average silhouette width\n    avg_sil_widths[k] &lt;- mean(sil[, 3])\n  }\n  \n  return(avg_sil_widths)\n}\n\n# Determine the maximum number of clusters to test\nmax_clusters &lt;- 10\n\n# Perform silhouette analysis\navg_sil_widths &lt;- silhouette_analysis(StudentLM_data_numeric, max_clusters)\n\n# Plot the average silhouette widths\nplot(1:max_clusters, avg_sil_widths, type = \"b\", pch = 19, frame = FALSE,\n     xlab = \"Number of clusters\", ylab = \"Average silhouette width\",\n     main = \"Silhouette Analysis for Determining Optimal Number of Clusters\")\n\n# Highlight the optimal number of clusters\noptimal_clusters &lt;- which.max(avg_sil_widths)\npoints(optimal_clusters, avg_sil_widths[optimal_clusters], col = \"red\", pch = 19)\n\n\n\n\n\n\nSSE-Elbow method\n\n# Function to compute SSE for different numbers of clusters\ncompute_sse &lt;- function(data, max_clusters) {\n  sse &lt;- numeric(max_clusters)\n  \n  for (k in 1:max_clusters) {\n    # Perform k-means clustering\n    kmeans_result &lt;- kmeans(data, centers = k, nstart = 25)\n    \n    # Compute SSE\n    sse[k] &lt;- kmeans_result$tot.withinss\n  }\n  \n  return(sse)\n}\n\n# Determine the maximum number of clusters to test\nmax_clusters &lt;- 10\n\n# Compute SSE for each number of clusters\nsse_values &lt;- compute_sse(StudentLM_data_numeric, max_clusters)\n\n# Plot SSE against number of clusters\nplot(1:max_clusters, sse_values, type = \"b\", pch = 19, frame = FALSE,\n     xlab = \"Number of clusters\", ylab = \"SSE\",\n     main = \"Elbow Method for Optimal Number of Clusters\")\n\n# Add text for elbow point\nelbow_point &lt;- which.min(diff(sse_values)) + 1\ntext(elbow_point, sse_values[elbow_point], labels = paste(\"Elbow Point:\", elbow_point), pos = 4, col = \"red\")\n\n\n\n\n\n\n\nK Means clustering\nK Means clustering is then performed on the recompiled learners’ learning mode features with the number of clusters set as 2 based on the above results, in the following code chunk\n\n# Drop the student_ID column\nclustering_data &lt;- StudentLM_data %&gt;%\n  select(-student_ID)\n\n# Standardize the data\nclustering_data_scaled &lt;- scale(clustering_data)\n\n# Perform k-means clustering\nset.seed(123)  # For reproducibility\nkmeans_result &lt;- kmeans(clustering_data_scaled, centers = 2, nstart = 25)\n\n# Add the cluster assignments to the original data\nStudentLM_data$cluster &lt;- kmeans_result$cluster\n\n\n\nVisualisation of K Means clusters\nThe first plot for visualisation of the K means cluster is the Principal Component Analysis (PCA) Plot, which gives an initial sensing of the separation of the clusters based on first 2 PCA components that rank the highest in distinctness amongst the features used. This is plotted with the following code chunk.\n\n# Perform PCA\npca_result &lt;- prcomp(StudentLM_data[-1], scale. = TRUE)\n\n# Get PCA scores\npca_scores &lt;- as.data.frame(predict(pca_result))\n\n# Add cluster information to PCA scores\npca_scores$cluster &lt;- factor(StudentLM_data$cluster)\n\n# Plot PCA results with cluster color coding\npca_plot &lt;- ggplot(pca_scores, aes(PC1, PC2, color = cluster)) +\n  geom_point(size = 3) +\n  scale_color_discrete(name = \"Cluster\") +\n  labs(x = \"Principal Component 1\", y = \"Principal Component 2\",\n       title = \"PCA Plot of Clusters\") +\n  theme_minimal()\n\n# Display the plot\npca_plot\n\n\n\n\nBased on the PCA plot, the clusters are visually clearly separated, suggesting that the clusters are distinct, especially in relation to the top 2 PCA components in the x and y-axis.\nNext to visualise the distribution of the 2 clusters across all the features used for the K Means clustering, a parallel coordinate plot is used, with the following code chunk.\n\n#| fig-width: 15\n#| fig-height: 12\n\nStudentLM_data_factor &lt;- StudentLM_data\nStudentLM_data_factor$cluster &lt;- as.character(StudentLM_data_factor$cluster)\n\nggparcoord(data = StudentLM_data_factor, \n           columns = c(2:8), \n           groupColumn = 9,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of Students' learning modes\")+\n   theme(axis.text.x = element_text(angle = 30))\n\n\n\n\nBased on the plot, there is varying degree of separation and distinctness between the 2 clusters across different variables. The more distinct separation are in variables such as total-memory size of answers, total answering attempt and mean memory size, where cluster 2 tends to fare better in these metrics suggesting that perhaps cluster 2 is the more hardworking learning mode among the 2.\n\n\nHierarchical Clustering and Visualisation with heatmap\nAs an alternative to K means, hierarchical clustering is also considered, and initiates with mapping the data frame into a data matrix, and thereby using the dend_expend function to determine the best clustering method.\n\nrow.names(StudentLM_data) &lt;- StudentLM_data$student_ID\nStudentLM_data1 &lt;- select(StudentLM_data, c(1, 2:8))\nStudentLM_data_matrix &lt;- data.matrix(StudentLM_data1)\n\nStudentLM_data_d &lt;- dist(normalize(StudentLM_data_matrix[, -c(1)]), method = \"euclidean\")\ndend_expend(StudentLM_data_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.4338557\n2      unknown        ward.D2 0.4397626\n3      unknown         single 0.8240479\n4      unknown       complete 0.6816024\n5      unknown        average 0.8551659\n6      unknown       mcquitty 0.7179679\n7      unknown         median 0.6816340\n8      unknown       centroid 0.8361293\n\n\nBased on the output above, the average method will be the most optimal.\nA silhoutte plot in the same approach as before is also done with the following code chunk to give an initial sensing of the number of clusters to achieve higher cluster separation and distinctness. This is done in the following code chunk.\n\nStudentLM_data_clust &lt;- hclust(StudentLM_data_d, method = \"average\")\nnum_k &lt;- find_k(StudentLM_data_clust)\nplot(num_k)\n\n\n\n\nBased on the output above, the same conclusion is arrived at, with 2 clusters being the idea number.\nNow using the 2 parameters, the hierarchical clustering using an interactive heatmap for visualisation is plot with the following code chunk.\n\n#| fig-width: 15\n#| fig-height: 12\nheatmaply(normalize(StudentLM_data_matrix[, -c(1)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 2,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,          \n          main=\"Students' Learning Mode Clustering \\nDataTransformation using Normalise Method\",\n          xlab = \"Student_IDs\",\n          ylab = \"Learning Mode Features\"\n)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take_home_Ex3.html#knowledge-acquisition",
    "href": "Take-home_Ex/Take-home_Ex3/Take_home_Ex3.html#knowledge-acquisition",
    "title": "Take-home Ex3",
    "section": "Knowledge Acquisition",
    "text": "Knowledge Acquisition\nBased on the given data, the relevant features that best defines a learner’s knowledge acquisition is assessed to be as follows:\n\nknowledge mastery determined by (a) overall sum of highest actual score for each question attempted and (b) sum of highest actual score of each question by knowledge area\ncorrect answering rate determined by (a) percentage of answers absolutely correct, (b) total number of questions with answers absolutely correct and partially correct\n\n\nFeature engineering\n\nGroup By Student ID\nThe following variables will be obtained with the code chunk below in preparation for visualisation and analysis of Knowledge Acquisition with respect to the various learning modes.\n\nknowledge mastery\n\n\noverall sum of highest actual score for each question attempted and\nsum of highest actual score of each question by knowledge area\n\n\ncorrect answering rate\n\n\npercentage of answers absolutely correct,\ntotal number of questions with answers absolutely correct and partially correct\n\n\nStudentKA_data &lt;- merged_data %&gt;%\n  group_by(student_ID) %&gt;%\n  summarize(\n    # Part (a): Sum of highest actual score for each question attempted\n    sum_highest_actual_score = sum(sapply(unique(title_ID), function(x) {\n      max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, \"actual_score\"])\n    })),\n    \n    # Part (b): Sum of highest actual score for each knowledge area\n    sum_highest_actual_score_b3C9s = sum(sapply(unique(title_ID), function(x) {\n      ifelse(any(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 16] == 1),\n             max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID & merged_data[, 16] == 1, \"actual_score\"]),\n             0)\n    })),\n    sum_highest_actual_score_g7R2j = sum(sapply(unique(title_ID), function(x) {\n      ifelse(any(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 17] == 1),\n             max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID & merged_data[, 17] == 1, \"actual_score\"]),\n             0)\n    })),\n    sum_highest_actual_score_k4W1c = sum(sapply(unique(title_ID), function(x) {\n      ifelse(any(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 18] == 1),\n             max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID & merged_data[, 18] == 1, \"actual_score\"]),\n             0)\n    })),\n    sum_highest_actual_score_m3D1v = sum(sapply(unique(title_ID), function(x) {\n      ifelse(any(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 19] == 1),\n             max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID & merged_data[, 19] == 1, \"actual_score\"]),\n             0)\n    })),\n    sum_highest_actual_score_r8S3g = sum(sapply(unique(title_ID), function(x) {\n      ifelse(any(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 20] == 1),\n             max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID & merged_data[, 20] == 1, \"actual_score\"]),\n             0)\n    })),\n    sum_highest_actual_score_s8Y2f = sum(sapply(unique(title_ID), function(x) {\n      ifelse(any(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 21] == 1),\n             max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID & merged_data[, 21] == 1, \"actual_score\"]),\n             0)\n    })),\n    sum_highest_actual_score_t5V9e = sum(sapply(unique(title_ID), function(x) {\n      ifelse(any(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 22] == 1),\n             max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID & merged_data[, 22] == 1, \"actual_score\"]),\n             0)\n    })),\n    sum_highest_actual_score_y9W5d = sum(sapply(unique(title_ID), function(x) {\n      ifelse(any(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 23] == 1),\n             max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID & merged_data[, 23] == 1, \"actual_score\"]),\n             0)\n    })),\n    \n    # Part (c): Percentage of answers absolutely correct\n    pct_answers_absolutely_correct = (sum(state == \"Absolutely_Correct\") / n()) * 100,\n    \n    # Part (d): Total number of questions with answers absolutely correct and partially correct\n    total_questions_correct_or_partial = length(unique(title_ID[state %in% c(\"Partially_Correct\", \"Absolutely_Correct\")]))\n  )\n\n\nglimpse(StudentKA_data)\n\nRows: 1,364\nColumns: 12\n$ student_ID                         &lt;chr&gt; \"0088dc183f73c83f763e\", \"00cbf05221…\n$ sum_highest_actual_score           &lt;dbl&gt; 100, 100, 100, 100, 100, 100, 100, …\n$ sum_highest_actual_score_b3C9s     &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10,…\n$ sum_highest_actual_score_g7R2j     &lt;dbl&gt; 15, 15, 15, 15, 15, 15, 15, 15, 15,…\n$ sum_highest_actual_score_k4W1c     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ sum_highest_actual_score_m3D1v     &lt;dbl&gt; 27, 27, 27, 27, 27, 27, 27, 27, 27,…\n$ sum_highest_actual_score_r8S3g     &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 0, 5,…\n$ sum_highest_actual_score_s8Y2f     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ sum_highest_actual_score_t5V9e     &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10,…\n$ sum_highest_actual_score_y9W5d     &lt;dbl&gt; 33, 33, 33, 33, 33, 33, 33, 33, 33,…\n$ pct_answers_absolutely_correct     &lt;dbl&gt; 20.930233, 19.166667, 11.087866, 32…\n$ total_questions_correct_or_partial &lt;int&gt; 38, 38, 38, 38, 38, 38, 38, 38, 38,…\n\n\n\n\nRemoving highly skewed columns\nInspecting the data frame, 2 variables were found to be highly skewed and concentrated within a small range of values, hence they are removed for more meaningful analysis, with the following code chunk.\n\nStudentKA_data &lt;- StudentKA_data %&gt;%\n  select(-sum_highest_actual_score_k4W1c, -sum_highest_actual_score_s8Y2f)\n\nglimpse(StudentKA_data)\n\nRows: 1,364\nColumns: 10\n$ student_ID                         &lt;chr&gt; \"0088dc183f73c83f763e\", \"00cbf05221…\n$ sum_highest_actual_score           &lt;dbl&gt; 100, 100, 100, 100, 100, 100, 100, …\n$ sum_highest_actual_score_b3C9s     &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10,…\n$ sum_highest_actual_score_g7R2j     &lt;dbl&gt; 15, 15, 15, 15, 15, 15, 15, 15, 15,…\n$ sum_highest_actual_score_m3D1v     &lt;dbl&gt; 27, 27, 27, 27, 27, 27, 27, 27, 27,…\n$ sum_highest_actual_score_r8S3g     &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 0, 5,…\n$ sum_highest_actual_score_t5V9e     &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10,…\n$ sum_highest_actual_score_y9W5d     &lt;dbl&gt; 33, 33, 33, 33, 33, 33, 33, 33, 33,…\n$ pct_answers_absolutely_correct     &lt;dbl&gt; 20.930233, 19.166667, 11.087866, 32…\n$ total_questions_correct_or_partial &lt;int&gt; 38, 38, 38, 38, 38, 38, 38, 38, 38,…\n\n\n\n\n\nMerging Students’ Learning Modes with Knowledge Acqusition features\nWith the both data frames prepared, they will now be merged for next sub-task which involves comparison of learners’ knowledge acqusition in respect to learning mode, and subsequently to identify patterns and relationship\n\n# Join the two dataframes on the column student_ID\nStudentLMKA_data &lt;- left_join(StudentLM_data, StudentKA_data, by = \"student_ID\")\n\nglimpse(StudentLMKA_data)\n\nRows: 1,364\nColumns: 18\n$ student_ID                         &lt;chr&gt; \"0088dc183f73c83f763e\", \"00cbf05221…\n$ pct_answers_weekdays               &lt;dbl&gt; 94.88372, 88.75000, 78.03347, 65.54…\n$ pct_answers_working_hours          &lt;dbl&gt; 14.883721, 10.416667, 15.899582, 21…\n$ total_diff_questions_attempted     &lt;int&gt; 38, 38, 38, 38, 38, 38, 38, 38, 38,…\n$ mean_question_scores               &lt;dbl&gt; 2.339535, 2.266667, 2.744770, 2.546…\n$ mean_memory_size                   &lt;dbl&gt; 224.0186, 407.5833, 285.5565, 273.8…\n$ total_answering_attempts           &lt;int&gt; 215, 240, 478, 119, 204, 145, 253, …\n$ total_memory_size                  &lt;dbl&gt; 48164, 97820, 136496, 32584, 35260,…\n$ cluster                            &lt;int&gt; 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2,…\n$ sum_highest_actual_score           &lt;dbl&gt; 100, 100, 100, 100, 100, 100, 100, …\n$ sum_highest_actual_score_b3C9s     &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10,…\n$ sum_highest_actual_score_g7R2j     &lt;dbl&gt; 15, 15, 15, 15, 15, 15, 15, 15, 15,…\n$ sum_highest_actual_score_m3D1v     &lt;dbl&gt; 27, 27, 27, 27, 27, 27, 27, 27, 27,…\n$ sum_highest_actual_score_r8S3g     &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 0, 5,…\n$ sum_highest_actual_score_t5V9e     &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10,…\n$ sum_highest_actual_score_y9W5d     &lt;dbl&gt; 33, 33, 33, 33, 33, 33, 33, 33, 33,…\n$ pct_answers_absolutely_correct     &lt;dbl&gt; 20.930233, 19.166667, 11.087866, 32…\n$ total_questions_correct_or_partial &lt;int&gt; 38, 38, 38, 38, 38, 38, 38, 38, 38,…\n\n\n\n\nVisualisation of Knowledge Aquisition by learning mode clusters\nTo visualise differences in the performance in total number of questions that had correct or partially correct answers, a ridgeline plot to compare the shape of distribution of students in both clusters in the same axis, using the following code chunk.\n\nStudentLMKA_data$cluster &lt;- as.factor(StudentLMKA_data$cluster)\n\n# Plot\nggplot(StudentLMKA_data, \n       aes(x = total_questions_correct_or_partial, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\nCluster 2 has a sharper peak and more packed to the right which suggests that students in this cluster generally performed better, while for cluster 1 there is a 2nd smaller group of that performs even worse.\n\n#| fig-width: 16\n#| fig-height: 20\n\na &lt;- ggplot(StudentLMKA_data, \n       aes(x = sum_highest_actual_score_b3C9s, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\nb &lt;- ggplot(StudentLMKA_data, \n       aes(x = sum_highest_actual_score_g7R2j,\n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\nc &lt;- ggplot(StudentLMKA_data, \n       aes(x = sum_highest_actual_score_m3D1v, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\nd &lt;- ggplot(StudentLMKA_data, \n       aes(x = sum_highest_actual_score_r8S3g, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\ne &lt;- ggplot(StudentLMKA_data, \n       aes(x = sum_highest_actual_score_t5V9e, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\nf &lt;- ggplot(StudentLMKA_data, \n       aes(x = sum_highest_actual_score_y9W5d, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n(a + b) / (c + d) / (e + f)\n\n\n\n\nA statistical violin plot to perform both a mathematical 2 sample mean test in tandem with a visual analysis of the difference in the distribution of the students’ total actual score in the answering records in respect of the 2 clusters is plot with the following code chunk.\n\nggbetweenstats(\n  data = StudentLMKA_data,\n  x = cluster, \n  y = sum_highest_actual_score,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\nBased on the figures, the p-value is extremely small which suggest that there is strong statistical significance between the 2 clusters in the performance of total actual score of students in each cluster, wherein cluster 2 fared better than cluster 1, it also shows that cluster 2 is much smaller than cluster 1.\nLastly a similar statistical violin plot to analyse the differences in percentage of answers that were absolutely correct in respect of the 2 clusters is plot in the following code chunk.\n\nggbetweenstats(\n  data = StudentLMKA_data,\n  x = cluster, \n  y = pct_answers_absolutely_correct,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\nBased on the figures, the p-value is extremely small which suggest that there is strong statistical significance between the 2 clusters in the performance of total actual score of students in each cluster, wherein surprisingly, cluster 1 had fared better than cluster 2, cluster 2 had a smaller spread and more concentrated compared to cluster 1."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html",
    "title": "Take-home Ex3",
    "section": "",
    "text": "This take-home exercise is centred on the learning patterns of learners in a programming course conducted by NorthClass Education Training Institute. The primary focus is to support the institution’s endeavor to analyze and visualise learners’ knowledge mastery levels, monitor the patterns and trends in their learning behaviors, identify and dissect potential factors that contribute to learning difficulties, and hence to derive feasible suggestions to adjust teaching strategies and course design.\n\n\nTo address the above, the key objective of this exercise is:\n\nTo analyse and provide a visual representation of the relationship between learning modes and knowledge acquisition (learners’ ability to absorb, integrate, and apply knowledge)\n\nThis would entail the following sub-task requirements:\n\nTo visualise and uncover the various learning modes, and\nTo visualise and uncover the patterns in distribution in learner’s performance in each various learning modes, and\nTo visualise and determine the statistical differences and correlations that learning mode may have with learners’ performance"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#introduction",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#introduction",
    "title": "Take-home Ex3",
    "section": "",
    "text": "This take-home exercise is centred on the learning patterns of learners in a programming course conducted by NorthClass Education Training Institute. The primary focus is to support the institution’s endeavor to analyze and visualise learners’ knowledge mastery levels, monitor the patterns and trends in their learning behaviors, identify and dissect potential factors that contribute to learning difficulties, and hence to derive feasible suggestions to adjust teaching strategies and course design.\n\n\nTo address the above, the key objective of this exercise is:\n\nTo analyse and provide a visual representation of the relationship between learning modes and knowledge acquisition (learners’ ability to absorb, integrate, and apply knowledge)\n\nThis would entail the following sub-task requirements:\n\nTo visualise and uncover the various learning modes, and\nTo visualise and uncover the patterns in distribution in learner’s performance in each various learning modes, and\nTo visualise and determine the statistical differences and correlations that learning mode may have with learners’ performance"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#getting-started",
    "title": "Take-home Ex3",
    "section": "Getting Started",
    "text": "Getting Started\n\nLoading Required R Package Libraries\nThe code chunk below loads the following libraries:\n\ntidyverse: an amalgamation of libraries for data handling (including ggplot2, dplyr, tidyr, readr, tibble)\nknitr: for creating dynamic html tables/reports\nggridges: extension of ggplot2 designed for plotting ridgeline plots\nggdist: extension of ggplot2 designed for visualising distribution and uncertainty,\ncolorspace: provides a broad toolbox for selecting individual colors or color palettes, manipulating these colors, and employing them in various kinds of visualisations.\nggrepel: provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: provides additional themes, geoms, and scales for ggplot package\nhrbrthemes: provides typography-centric themes and theme components for ggplot package\npatchwork: preparing composite figure created using ggplot package\nlubridate: for wrangling of date-time data\nggstatplot: provides alternative statistical inference methods by default as an extension of the ggplot2 package\nplotly: R library for plotting interactive statistical graphs.\nrjson: Methods for Cluster analysis.\nvisNetwork: Extract and Visualize the Results of Multivariate Data Analyses.\nBiocManager: Extension of ggplot2 by adding several functions to reduce the complexity of combining geometric objects with transformed data.\nigraph: Extension of ggplot2 by adding several functions to reduce the complexity of combining geometric objects with transformed data.\ncluster\nfactoextra\nstats\nhms\ncaret\nggfortify\ngridExtra\nGGally\nparallelPlot\nseriation\ndendextend\nheatmaply,\n\n\npacman::p_load(tidyverse, knitr, ggridges, ggdist, colorspace, ggrepel, ggthemes, hrbrthemes, patchwork, lubridate, ggstatsplot, plotly, rjson, visNetwork, BiocManager, igraph, cluster, factoextra, stats, hms, caret, ggfortify, gridExtra, GGally, parallelPlot, seriation, dendextend, heatmaply) \n\n\n\nImporting the Data\nThe data for this exercise was collected from a select group of learners over a specified set of programming tasks over a particular learning period, which was compiled in 3 datasets described below. It is accompanied by a separate document providing a more detailed description of the data and variables.\n\nDataset 1: Student Information - This comprises of 5 Cols, 1364 Rows, providing individualised demographic variables of the learners (a.k.a students) within the scope this project\nDataset 2: Learning Subject Title Information - This comprises of 5 Cols, 44 Rows, providing variables of the questions from the programming tasks which are collated in the scope of this project\nDataset 3: Class Submission Records - This comprises of 15 datasets, each with 10 Cols and various number of rows, providing supposedly the participating learners’ answering variables to the questions collated in the scope of this project\n\nThe code chunk below imports the dataset into R environment by using read_csv() function of readr, which is part of the tidyverse package.\n\ndf_StudentInfo &lt;- read_csv(\"data/Data_StudentInfo.csv\")\n\n\ndf_TitleInfo &lt;- read_csv(\"data/Data_TitleInfo.csv\")\n\n\ncsv_file_list &lt;- dir('data/Data_SubmitRecord')\ncsv_file_list &lt;- paste0(\"./data/Data_SubmitRecord/\",csv_file_list)\n\ndf_StudentRecord &lt;- NULL\nfor (file in csv_file_list) { # for every file...\n  file &lt;- read_csv(file)\n    df_StudentRecord &lt;- rbind(df_StudentRecord, file) # then stick together by rows\n}"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#data-preparation",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#data-preparation",
    "title": "Take-home Ex3",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nData Cleaning\nBefore data transformation, the cleanliness of the data set is first ascertained by checking for missing and duplicate data.\n\nMissing Data\ncolSums() and is.NA() functions are used to search for missing values as a whole for the 3 data sets in the code chunks as follows.\n\n#Find the number of missing values for each col\ncolSums(is.na(df_StudentInfo))\n\n     index student_ID        sex        age      major \n         0          0          0          0          0 \n\n\n\n#Find the number of missing values for each col\ncolSums(is.na(df_TitleInfo))\n\n        index      title_ID         score     knowledge sub_knowledge \n            0             0             0             0             0 \n\n\n\n#Find the number of missing values for each col\ncolSums(is.na(df_StudentRecord))\n\n      index       class        time       state       score    title_ID \n          0           0           0           0           0           0 \n     method      memory timeconsume  student_ID \n          0           0           0           0 \n\n\nFrom the outputs above, none of the variables contain missing values.\n\n\nCheck for duplicate rows\nUsing duplicated(), duplicate rows in each of the 3 data sets are identified and extracted in the following code chunks.\n\ndf_StudentInfo[duplicated(df_StudentInfo), ]\n\n# A tibble: 0 × 5\n# ℹ 5 variables: index &lt;dbl&gt;, student_ID &lt;chr&gt;, sex &lt;chr&gt;, age &lt;dbl&gt;,\n#   major &lt;chr&gt;\n\n\n\ndf_TitleInfo[duplicated(df_TitleInfo), ]\n\n# A tibble: 0 × 5\n# ℹ 5 variables: index &lt;dbl&gt;, title_ID &lt;chr&gt;, score &lt;dbl&gt;, knowledge &lt;chr&gt;,\n#   sub_knowledge &lt;chr&gt;\n\n\n\ndf_StudentRecord[duplicated(df_StudentRecord), ]\n\n# A tibble: 0 × 10\n# ℹ 10 variables: index &lt;dbl&gt;, class &lt;chr&gt;, time &lt;dbl&gt;, state &lt;chr&gt;,\n#   score &lt;dbl&gt;, title_ID &lt;chr&gt;, method &lt;chr&gt;, memory &lt;dbl&gt;, timeconsume &lt;chr&gt;,\n#   student_ID &lt;chr&gt;\n\n\nFrom the outputs above, there were no duplicate rows found.\n\n\n\nData Wrangling for Inconsistencies\nTo get a better understanding of the variables in the original dataset, the glimpse() function is used in the following code chunks.\n\nglimpse(df_StudentInfo)\n\nRows: 1,364\nColumns: 5\n$ index      &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ student_ID &lt;chr&gt; \"8b6d1125760bd3939b6e\", \"63eef37311aaac915a45\", \"5d89810b20…\n$ sex        &lt;chr&gt; \"female\", \"female\", \"female\", \"female\", \"male\", \"male\", \"ma…\n$ age        &lt;dbl&gt; 24, 21, 23, 21, 22, 19, 21, 18, 21, 24, 23, 20, 18, 18, 23,…\n$ major      &lt;chr&gt; \"J23517\", \"J87654\", \"J87654\", \"J78901\", \"J40192\", \"J57489\",…\n\n\n\nglimpse(df_TitleInfo)\n\nRows: 44\nColumns: 5\n$ index         &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ title_ID      &lt;chr&gt; \"Question_VgKw8PjY1FR6cm2QI9XW\", \"Question_q7OpB2zCMmW9w…\n$ score         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ knowledge     &lt;chr&gt; \"r8S3g\", \"r8S3g\", \"r8S3g\", \"r8S3g\", \"r8S3g\", \"r8S3g\", \"t…\n$ sub_knowledge &lt;chr&gt; \"r8S3g_l0p5viby\", \"r8S3g_n0m9rsw4\", \"r8S3g_l0p5viby\", \"r…\n\n\n\nglimpse(df_StudentRecord)\n\nRows: 232,818\nColumns: 10\n$ index       &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …\n$ class       &lt;chr&gt; \"Class1\", \"Class1\", \"Class1\", \"Class1\", \"Class1\", \"Class1\"…\n$ time        &lt;dbl&gt; 1704209872, 1704209852, 1704209838, 1704208923, 1704208359…\n$ state       &lt;chr&gt; \"Absolutely_Correct\", \"Absolutely_Correct\", \"Absolutely_Co…\n$ score       &lt;dbl&gt; 3, 3, 3, 3, 4, 0, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 4, 0, 0…\n$ title_ID    &lt;chr&gt; \"Question_bumGRTJ0c8p4v5D6eHZa\", \"Question_62XbhBvJ8NUSnAp…\n$ method      &lt;chr&gt; \"Method_Cj9Ya2R7fZd6xs1q5mNQ\", \"Method_gj1NLb4Jn7URf9K2kQP…\n$ memory      &lt;dbl&gt; 320, 356, 196, 308, 320, 0, 308, 312, 312, 328, 512, 324, …\n$ timeconsume &lt;chr&gt; \"3\", \"3\", \"2\", \"2\", \"3\", \"5\", \"2\", \"2\", \"3\", \"2\", \"3\", \"2\"…\n$ student_ID  &lt;chr&gt; \"8b6d1125760bd3939b6e\", \"8b6d1125760bd3939b6e\", \"8b6d11257…\n\n\n\nIdentifying Other Unexpected Duplicate Values\nConsidering intuitively unique values for certain variables or dependent variables, other forms of duplicates are also identified and cleaned where relevant.\n\nDuplicate student_ID in StudentInfo\n\n\n# Find the duplicated student_IDs\nduplicates &lt;- df_StudentInfo[duplicated(df_StudentInfo$student_ID) | duplicated(df_StudentInfo$student_ID, fromLast = TRUE), ]\n\n# Display the rows with duplicate student_IDs\nduplicates\n\n# A tibble: 0 × 5\n# ℹ 5 variables: index &lt;dbl&gt;, student_ID &lt;chr&gt;, sex &lt;chr&gt;, age &lt;dbl&gt;,\n#   major &lt;chr&gt;\n\n\nFrom the output above, no duplicates found.\n\nDuplicate title_ID (aka questions) in TitleInfo\n\n\n# Find the duplicated title_IDs\nduplicates &lt;- df_TitleInfo[duplicated(df_TitleInfo$title_ID) | duplicated(df_TitleInfo$title_ID, fromLast = TRUE), ]\n\n# Display the rows with duplicate title_IDs\nduplicates\n\n# A tibble: 12 × 5\n   index title_ID                      score knowledge sub_knowledge \n   &lt;dbl&gt; &lt;chr&gt;                         &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;         \n 1     2 Question_q7OpB2zCMmW9wS8uNt3H     1 r8S3g     r8S3g_n0m9rsw4\n 2     3 Question_q7OpB2zCMmW9wS8uNt3H     1 r8S3g     r8S3g_l0p5viby\n 3    21 Question_QRm48lXxzdP7Tn1WgNOf     3 y9W5d     y9W5d_c0w4mj5h\n 4    22 Question_QRm48lXxzdP7Tn1WgNOf     3 m3D1v     m3D1v_r1d7fr3l\n 5    23 Question_pVKXjZn0BkSwYcsa7C31     3 y9W5d     y9W5d_c0w4mj5h\n 6    24 Question_pVKXjZn0BkSwYcsa7C31     3 m3D1v     m3D1v_r1d7fr3l\n 7    26 Question_lU2wvHSZq7m43xiVroBc     3 y9W5d     y9W5d_c0w4mj5h\n 8    27 Question_lU2wvHSZq7m43xiVroBc     3 k4W1c     k4W1c_h5r6nux7\n 9    30 Question_x2Fy7rZ3SwYl9jMQkpOD     3 y9W5d     y9W5d_c0w4mj5h\n10    31 Question_x2Fy7rZ3SwYl9jMQkpOD     3 s8Y2f     s8Y2f_v4x8by9j\n11    36 Question_oCjnFLbIs4Uxwek9rBpu     3 g7R2j     g7R2j_e0v1yls8\n12    37 Question_oCjnFLbIs4Uxwek9rBpu     3 m3D1v     m3D1v_r1d7fr3l\n\n\n\nunique(duplicates$knowledge)\n\n[1] \"r8S3g\" \"y9W5d\" \"m3D1v\" \"k4W1c\" \"s8Y2f\" \"g7R2j\"\n\nunique(duplicates$sub_knowledge)\n\n[1] \"r8S3g_n0m9rsw4\" \"r8S3g_l0p5viby\" \"y9W5d_c0w4mj5h\" \"m3D1v_r1d7fr3l\"\n[5] \"k4W1c_h5r6nux7\" \"s8Y2f_v4x8by9j\" \"g7R2j_e0v1yls8\"\n\n\nFrom the outputs above, some questions (title_ID) belong to up to 2 knowledge areas or 2 sub-knowledge areas, where the scores for the former are consistently 3, and for the latter, 1. This overlap in title_ID affects 6 title_IDs, spreads across 6 knowledge areas and 7 sub-knowledge areas.\nThe unique values for knowledge and sub-knowledge areas are obtained in the following code chunk to better understand the complexity of these 2 variables.\n\nunique(df_TitleInfo$knowledge)\n\n[1] \"r8S3g\" \"t5V9e\" \"m3D1v\" \"y9W5d\" \"k4W1c\" \"s8Y2f\" \"g7R2j\" \"b3C9s\"\n\nunique(df_TitleInfo$sub_knowledge)\n\n [1] \"r8S3g_l0p5viby\" \"r8S3g_n0m9rsw4\" \"t5V9e_e1k6cixp\" \"m3D1v_r1d7fr3l\"\n [5] \"m3D1v_v3d9is1x\" \"m3D1v_t0v5ts9h\" \"y9W5d_c0w4mj5h\" \"k4W1c_h5r6nux7\"\n [9] \"s8Y2f_v4x8by9j\" \"y9W5d_p8g6dgtv\" \"y9W5d_e2j7p95s\" \"g7R2j_e0v1yls8\"\n[13] \"g7R2j_j1g8gd3v\" \"b3C9s_l4z6od7y\" \"b3C9s_j0v1yls8\"\n\n\nBased on the output above, there is a total of 8 knowledge areas and 15 sub-knowledge areas. This suggests that majority of the knowledge areas and approximately half of sub-knowledge areas have overlapping title_ID. From the nomenclature, each sub-knowledge area is tagged to only 1 knowledge area.\nTo meaningfully analyse the relationship between knowledge areas & sub knowledge areas and other variables, additional columns are introduced where the values in these 2 columnns are transposed as column labels with binary values to indicate the tagging of each question to that value. This is done in the following code chunk.\n\n# Transpose the knowledge column to create new columns for each unique value\ndf_TitleInfo1 &lt;- df_TitleInfo %&gt;%\n  mutate(knowledge_presence = 1) %&gt;%\n  spread(key = knowledge, value = knowledge_presence, fill = 0)\n\n# Transpose the sub_knowledge column to create new columns for each unique value\ndf_TitleInfo2 &lt;- df_TitleInfo %&gt;%\n  mutate(sub_knowledge_presence = 1) %&gt;%\n  spread(key = sub_knowledge, value = sub_knowledge_presence, fill = 0)\n\n# Combine the new columns with the original dataframe\ndf_TitleInfo3 &lt;- df_TitleInfo %&gt;%\n  distinct(title_ID, .keep_all = TRUE) %&gt;%\n  left_join(df_TitleInfo1, by = \"title_ID\") %&gt;%\n  distinct(title_ID, .keep_all = TRUE) %&gt;%\n  left_join(df_TitleInfo2, by = \"title_ID\") %&gt;%\n  \n  rename(knowledge = knowledge.x,\n         sub_knowledge = sub_knowledge.x) %&gt;%\n  select(-index.y,\n         -score.y,\n         -knowledge.y,\n         -sub_knowledge.y,\n         -index.x,\n         -score.x,\n         -index)\n\n# Reassign values to the knowledge & sub_knowledge columns for repeated title_ID rows\ndf_TitleInfo3 &lt;- df_TitleInfo3 %&gt;%\n  group_by(title_ID) %&gt;%\n  mutate(knowledge = paste(unique(df_TitleInfo$knowledge[df_TitleInfo$title_ID == title_ID]), collapse = \"_\"),\n         sub_knowledge = paste(unique(df_TitleInfo$sub_knowledge[df_TitleInfo$title_ID == title_ID]), collapse = \"_\")) %&gt;%\n  ungroup() %&gt;%\n  distinct(title_ID, .keep_all = TRUE)\n\n\n# Group by title_ID\ndf_TitleInfo_gp &lt;- df_TitleInfo3 %&gt;%\n  group_by(title_ID) %&gt;%\n  summarise_all(~first(.))\n\nglimpse(df_TitleInfo_gp)\n\nRows: 38\nColumns: 27\n$ title_ID       &lt;chr&gt; \"Question_3MwAFlmNO8EKrpY5zjUd\", \"Question_3oPyUzDmQtcM…\n$ knowledge      &lt;chr&gt; \"t5V9e\", \"t5V9e\", \"m3D1v\", \"g7R2j\", \"y9W5d\", \"m3D1v\", \"…\n$ sub_knowledge  &lt;chr&gt; \"t5V9e_e1k6cixp\", \"t5V9e_e1k6cixp\", \"m3D1v_r1d7fr3l\", \"…\n$ b3C9s          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j          &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ k4W1c          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v          &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0…\n$ r8S3g          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ s8Y2f          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ t5V9e          &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y9W5d          &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1…\n$ score          &lt;dbl&gt; 2, 2, 3, 3, 3, 3, 3, 3, 1, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3…\n$ b3C9s_j0v1yls8 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n$ b3C9s_l4z6od7y &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j_e0v1yls8 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ g7R2j_j1g8gd3v &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ k4W1c_h5r6nux7 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ m3D1v_r1d7fr3l &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0…\n$ m3D1v_t0v5ts9h &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0…\n$ m3D1v_v3d9is1x &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g_l0p5viby &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ r8S3g_n0m9rsw4 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ s8Y2f_v4x8by9j &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ t5V9e_e1k6cixp &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y9W5d_c0w4mj5h &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1…\n$ y9W5d_e2j7p95s &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ y9W5d_p8g6dgtv &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…\n\nunique(df_TitleInfo_gp$knowledge)\n\n [1] \"t5V9e\"       \"m3D1v\"       \"g7R2j\"       \"y9W5d\"       \"r8S3g\"      \n [6] \"b3C9s\"       \"y9W5d_m3D1v\" \"y9W5d_k4W1c\" \"g7R2j_m3D1v\" \"y9W5d_s8Y2f\"\n\nunique(df_TitleInfo_gp$sub_knowledge)\n\n [1] \"t5V9e_e1k6cixp\"                \"m3D1v_r1d7fr3l\"               \n [3] \"g7R2j_e0v1yls8\"                \"y9W5d_c0w4mj5h\"               \n [5] \"m3D1v_v3d9is1x\"                \"y9W5d_p8g6dgtv\"               \n [7] \"r8S3g_n0m9rsw4\"                \"y9W5d_e2j7p95s\"               \n [9] \"b3C9s_j0v1yls8\"                \"m3D1v_t0v5ts9h\"               \n[11] \"y9W5d_c0w4mj5h_m3D1v_r1d7fr3l\" \"r8S3g_l0p5viby\"               \n[13] \"g7R2j_j1g8gd3v\"                \"b3C9s_l4z6od7y\"               \n[15] \"y9W5d_c0w4mj5h_k4W1c_h5r6nux7\" \"g7R2j_e0v1yls8_m3D1v_r1d7fr3l\"\n[17] \"r8S3g_n0m9rsw4_r8S3g_l0p5viby\" \"y9W5d_c0w4mj5h_s8Y2f_v4x8by9j\"\n\n\n\nDuplicate class for each Individual Students in StudentRecord\n\n\n# Identify students with multiple classes\nstudents_multiple_classes &lt;- df_StudentRecord %&gt;%\n  group_by(student_ID) %&gt;%\n  summarise(unique_classes = n_distinct(class)) %&gt;%\n  filter(unique_classes &gt; 1)\n\nstudents_multiple_classes_entries &lt;- df_StudentRecord %&gt;%\n  filter(student_ID %in% students_multiple_classes$student_ID) %&gt;%\n  group_by(student_ID, class) %&gt;%\n  summarise(count = n()) %&gt;%\n  arrange(desc(count)) %&gt;%\n  arrange(desc(student_ID))\n\n# Display the results\nprint(students_multiple_classes_entries)\n\n# A tibble: 12 × 3\n# Groups:   student_ID [6]\n   student_ID           class   count\n   &lt;chr&gt;                &lt;chr&gt;   &lt;int&gt;\n 1 r9m46ndmmmzeeehft96z Class15   140\n 2 r9m46ndmmmzeeehft96z class       1\n 3 qz6jjynwbd3szlp0rj04 Class1    136\n 4 qz6jjynwbd3szlp0rj04 class       1\n 5 nd9xpohv0s4ttw0o7fts Class8    143\n 6 nd9xpohv0s4ttw0o7fts class       1\n 7 lqm8jh0uggps7yd0lx2x Class8    132\n 8 lqm8jh0uggps7yd0lx2x class       1\n 9 isa355t9q5rut5fm8aml Class1    142\n10 isa355t9q5rut5fm8aml class       1\n11 ezdogkk0jqt4nvvvbnxp Class7    125\n12 ezdogkk0jqt4nvvvbnxp class       1\n\n\nBased on the output above, it is apparent that the 2nd class for each of the student above is an erroneous value. Hence this inconsistency will be cleaned in the following code chunk\n\n# Step 1: Identify the correct class for each student (the class with the highest frequency)\ncorrect_classes &lt;- df_StudentRecord %&gt;%\n  filter(student_ID %in% students_multiple_classes$student_ID) %&gt;%\n  group_by(student_ID, class) %&gt;%\n  summarise(count = n()) %&gt;%\n  arrange(desc(count)) %&gt;%\n  slice(1) %&gt;%\n  select(student_ID, correct_class = class)\n\n# Step 2: Replace wrong class values\ndf_StudentRecord &lt;- df_StudentRecord %&gt;%\n  left_join(correct_classes, by = \"student_ID\") %&gt;%\n  mutate(class = ifelse(!is.na(correct_class), correct_class, class)) %&gt;%\n  select(-correct_class)\n\nFor completeness, a check is done for existence of other students with class that has no class number in the following code chunk.\n\nMissingClassNo &lt;- df_StudentRecord %&gt;%\n  filter(class == \"class\")\nMissingClassNo\n\n# A tibble: 0 × 10\n# ℹ 10 variables: index &lt;dbl&gt;, class &lt;chr&gt;, time &lt;dbl&gt;, state &lt;chr&gt;,\n#   score &lt;dbl&gt;, title_ID &lt;chr&gt;, method &lt;chr&gt;, memory &lt;dbl&gt;, timeconsume &lt;chr&gt;,\n#   student_ID &lt;chr&gt;\n\n\nBased on the output above, there are no further students with class without number.\n\n\nIdentifying Other Unexpected and/or Missing Values\n\nMissing Student_ID and title_ID in StudentRecord are also identified.\n\n\nmissing_students &lt;- anti_join(df_StudentRecord, df_StudentInfo, by = \"student_ID\")\n\n# Display the missing student IDs\nmissing_student_ids &lt;- missing_students %&gt;% select(student_ID) %&gt;% distinct()\nprint(missing_student_ids)\n\n# A tibble: 1 × 1\n  student_ID           \n  &lt;chr&gt;                \n1 44c7cf3881ae07f7fb3eD\n\n\n\nmissing_questions &lt;- anti_join(df_StudentRecord, df_TitleInfo, by = \"title_ID\")\n\n# Display the missing title IDs\nmissing_questions &lt;- missing_questions %&gt;% select(title_ID) %&gt;% distinct()\nprint(missing_questions)\n\n# A tibble: 0 × 1\n# ℹ 1 variable: title_ID &lt;chr&gt;\n\n\nThere is 1 missing student between either StudentRecord or StudentInfo, but no missing questions. Since there is partial missing info on this student, it isn’t meaningful to include in this analysis, hence the student_ID will be excluded in the following code chunk.\n\ndf_StudentInfo &lt;- df_StudentInfo %&gt;%\n  filter (student_ID != '44c7cf3881ae07f7fb3eD')\ndf_StudentRecord &lt;- df_StudentRecord %&gt;%\n  filter (student_ID != '44c7cf3881ae07f7fb3eD')\n\n\nOther unexpected values\n\nThe unique values for each column is queried to check for unexpected values in the following code chunk, wherein Index, time, class, title_ID and student_ID are excluded since they will be dealt with separately\n\nunique(df_StudentRecord$state)\n\n [1] \"Absolutely_Correct\" \"Error1\"             \"Absolutely_Error\"  \n [4] \"Error6\"             \"Error4\"             \"Partially_Correct\" \n [7] \"Error2\"             \"Error3\"             \"Error5\"            \n[10] \"Error7\"             \"Error8\"             \"Error9\"            \n[13] \"�������\"           \n\nunique(df_StudentRecord$score)\n\n[1] 3 4 0 1 2\n\nunique(df_StudentRecord$method)\n\n[1] \"Method_Cj9Ya2R7fZd6xs1q5mNQ\" \"Method_gj1NLb4Jn7URf9K2kQPd\"\n[3] \"Method_5Q4KoXthUuYz3bvrTDFm\" \"Method_m8vwGkEZc3TSW2xqYUoR\"\n[5] \"Method_BXr9AIsPQhwNvyGdZL57\"\n\nunique(df_StudentRecord$memory)\n\n  [1]   320   356   196   308     0   312   328   512   324   188   316   344\n [13]   444   192   332   484   360   200   340   184   476   492   180   448\n [25]   464  8544   204   496   364   460   508   456   352   480   348   488\n [37]   468   400   616   472   384   376   452   336   588   604   440   600\n [49]   580   500   640   520   436   368   612   504   736   632  8448   220\n [61]   372   208   828   256   568   576   628   756   620   700   212   592\n [73]   380   396   432   404   644   564   748   216   264   708   768   304\n [85]   420   624  8516  8644   288  8632  8640  8512   408   260   292   608\n [97]  8580   636   536   424   596   272   388   300   280   268   176   160\n[109]   296   416   240   284   248   172  8388   832  4164  4284   428   168\n[121]   572   164   276   528   392   412  8668  8500  8540  8664  8536  8576\n[133]  8628  8504  8800  8524  8392  8548   692   952  8508  8648  9664  9536\n[145]  9564 49852 59616  1332   948   824   724  2876  3024 24668 25208 26712\n[157] 23968   732 25248 22740   712  8520   720 18264   224  4984  8696 20272\n[169] 19576   516  8976  9028  9532   544   584   552   524  5624 29688   688\n[181] 30940 44020   740   556 51376 14656 65536   680 30440 30284 23128 28112\n[193]   760 15060 25660 23356 31796   804 24768 24232 12792 14720 26172 29020\n[205] 32992 28492 10568  8460  8404   908   652   540  8620 34268 11348 11640\n[217] 13124   532 12608 15028  1400 32544 39612 27272 28852 29248  8452  8616\n[229]  8480  8528   560 13576  8436   548  2012 24896   232 21728 21148  4424\n[241]  7640 43512 39912 19936 12580  2412  2436 24224  4296  4332  6392 25912\n[253] 21332 20128   668 35948  2360  8612  8384  5560 26548 25532 13112 15288\n[265] 13992 49336 53216 15040 13780  8496  8424 37184  8476  8400  8408 30656\n[277]  8156  8140  8064  8136 11236  5616  8160  4192 23116 19784 22908 21176\n[289] 18276 20708 19868 16348 18716 17208 19588 14824 20780 20204 24932 21084\n[301] 24992 21884 18764 26624 24368 13240 22988  3740 43532 26084 26320 13340\n[313] 11372 46460 49464 13356  8144  8564  1720 13892 14488 10580 23576  8396\n[325] 15212 15340   872 25648 25920 27028 24356 23544  7416  6560  4852  8556\n[337] 32088 32716 44216  4292   228 33212 33736 27228 27288 11764 10540 11560\n[349] 10456 11384 10708 32932 25940 17800 16764 46908 30512  9368  9472 19156\n[361]  2348 36136  8132  4708 39048 21152 30632 27200   656   252 47096  8552\n[373]  8464 14040 36984  2384  1792  6084  5844  2456  2440 26452 27364   648\n[385]   244 23168 24324  8420 41460 40568 34316   896  1472  7156 23740  6444\n[397]  6972  6200  6060  7488  6700  6580  5184  4948  5052  5820  6120  5404\n[409]  5028  5180  5100  5068  5020  5204  5976  5176  5048  5884  5824  5828\n[421]  5060  5072  5056  6076  6328  5076  8492  8428   236  7340  6668  7492\n[433]  8412  8652 17176  6852  6616  6032 45288 50140 40348 16848 21820 20856\n[445] 26296 28128 31560 17272 17656 37548 34476 38428 30456 41624 34224 18148\n[457] 20816   128   808   156   844   728   716   696   836   676  4324   860\n[469]  1980  8812   660  8636   684  8756   704  8532  8572  1920  1972  2332\n[481]  2172  2296  2280 13908 63088 15432 15680 15624 15824 15956 15724 15292\n[493]  8796  1880  1996  1992 11256 11268 11264 29240 29144 28752 27988  6068\n[505]  1180 28536 11032 39216 35632 28600  2104  8656 36028 38432 12456 30164\n[517]  1268  1328  1316  1240 50220  4540 35888  1976  4440 14336 14384 45680\n[529] 39080 28484 39104 53732  8680  8692  8660 14136  4564  4480 28848 29112\n[541] 18856  8792  8600  8592 41404 37052 36532 37804 33084 37368 30820 50620\n[553] 26248 22264 26616 25900   752 47040 14644 40636 43128 33568 36248 33088\n[565] 28140 28084 30532 30572 48376 47640 17400 20288 28724 20216 12664 12204\n[577] 11960 27188 15700 15664  4580  4584 28036 28732 34004 33508 31808  1528\n[589]  1716 13752  9592  9520  9784  9208  8828 28716 27536 28584  1704  1620\n[601] 13096 14132 14584 57528 45500  7096  2168  2236 12984 20412 31172 29296\n[613] 54356 54336 47548 41664 41812 13624  1336  1348 13496 55524  1352  1356\n[625] 42052   744   996   984   940  1016 29012 28080 26036  7344  7232  7476\n[637]  7828 13956 43452  1456  1324  1364 43196 27964 10812   972  1340  4692\n[649] 27248 44592 44860 46576 20464 52656 52996 48964 49516  6904  6592  6584\n[661]  8672 46852 40364 14500 14712 17740 17620 52584  8488 36488 44204 44500\n[673] 42300 45228 17980 37460 28240 28988 53288 58424  9540  9524  6936  6204\n[685] 54596 28604 29528 42804 12856 13776 15720  4156 12472  8704  8688 29300\n[697] 18612 12976 32376  8776 13548 26456  1884  1752   764  4172 53316 52160\n[709] 47036 45632 53396 51320 12468 11496 53604\n\nunique(df_StudentRecord$timeconsume)\n\n  [1] \"3\"   \"2\"   \"5\"   \"4\"   \"1\"   \"9\"   \"6\"   \"--\"  \"18\"  \"61\"  \"7\"   \"59\" \n [13] \"10\"  \"8\"   \"12\"  \"13\"  \"16\"  \"15\"  \"183\" \"68\"  \"314\" \"64\"  \"60\"  \"11\" \n [25] \"96\"  \"94\"  \"58\"  \"67\"  \"54\"  \"17\"  \"122\" \"19\"  \"126\" \"14\"  \"91\"  \"50\" \n [37] \"21\"  \"40\"  \"23\"  \"20\"  \"80\"  \"31\"  \"118\" \"400\" \"63\"  \"25\"  \"27\"  \"29\" \n [49] \"24\"  \"26\"  \"62\"  \"152\" \"39\"  \"22\"  \"117\" \"30\"  \"28\"  \"48\"  \"309\" \"331\"\n [61] \"36\"  \"65\"  \"47\"  \"46\"  \"45\"  \"52\"  \"32\"  \"42\"  \"34\"  \"38\"  \"187\" \"37\" \n [73] \"190\" \"163\" \"41\"  \"53\"  \"51\"  \"307\" \"201\" \"184\" \"44\"  \"43\"  \"109\" \"33\" \n [85] \"66\"  \"326\" \"73\"  \"49\"  \"77\"  \"82\"  \"70\"  \"71\"  \"81\"  \"35\"  \"57\"  \"75\" \n [97] \"394\" \"385\" \"164\" \"78\"  \"220\" \"217\" \"115\" \"86\"  \"72\"  \"88\"  \"76\"  \"134\"\n[109] \"55\"  \"84\"  \"56\"  \"106\" \"166\" \"124\" \"373\" \"289\" \"-\"   \"135\" \"103\" \"114\"\n[121] \"258\" \"254\" \"85\"  \"69\"  \"90\"  \"132\" \"173\" \"272\" \"113\" \"116\" \"215\" \"123\"\n[133] \"246\" \"146\" \"89\"  \"245\" \"285\" \"205\" \"162\" \"165\" \"266\" \"172\" \"143\" \"377\"\n[145] \"160\" \"159\" \"182\" \"74\"  \"264\" \"153\" \"83\"  \"286\" \"275\" \"280\" \"274\" \"269\"\n[157] \"288\" \"271\" \"136\" \"276\" \"277\" \"356\" \"79\"  \"147\" \"350\" \"315\" \"321\" \"302\"\n\n\n\nunique(df_StudentInfo$sex)\n\n[1] \"female\" \"male\"  \n\nunique(df_StudentInfo$age)\n\n[1] 24 21 23 22 19 18 20\n\nunique(df_StudentInfo$major)\n\n[1] \"J23517\" \"J87654\" \"J78901\" \"J40192\" \"J57489\"\n\n\n\nunique(df_TitleInfo$score)\n\n[1] 1 2 3 4\n\nunique(df_TitleInfo$knowledge)\n\n[1] \"r8S3g\" \"t5V9e\" \"m3D1v\" \"y9W5d\" \"k4W1c\" \"s8Y2f\" \"g7R2j\" \"b3C9s\"\n\nunique(df_TitleInfo$sub_knowledge)\n\n [1] \"r8S3g_l0p5viby\" \"r8S3g_n0m9rsw4\" \"t5V9e_e1k6cixp\" \"m3D1v_r1d7fr3l\"\n [5] \"m3D1v_v3d9is1x\" \"m3D1v_t0v5ts9h\" \"y9W5d_c0w4mj5h\" \"k4W1c_h5r6nux7\"\n [9] \"s8Y2f_v4x8by9j\" \"y9W5d_p8g6dgtv\" \"y9W5d_e2j7p95s\" \"g7R2j_e0v1yls8\"\n[13] \"g7R2j_j1g8gd3v\" \"b3C9s_l4z6od7y\" \"b3C9s_j0v1yls8\"\n\n\nFrom the outputs above, there is an unexpected value for state and timeconsume in StudentRecord.\nStarting with state, the rows with unexpected value(s) are queried in the following code chunk to better understand the number of affected rows.\n\nOutlier_state &lt;- df_StudentRecord %&gt;%\n  filter (state == '�������')\nOutlier_state\n\n# A tibble: 6 × 10\n  index class     time state score title_ID method memory timeconsume student_ID\n  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;     \n1  6344 Class10 1.70e9 ����…     0 Questio… Metho…  65536 309         c681117f7…\n2  6346 Class10 1.70e9 ����…     0 Questio… Metho…  65536 331         c681117f7…\n3  6347 Class10 1.70e9 ����…     0 Questio… Metho…  65536 331         c681117f7…\n4 10138 Class8  1.69e9 ����…     0 Questio… Metho…  65536 356         1883af270…\n5 16420 Class8  1.69e9 ����…     0 Questio… Metho…  65536 356         hpb03ydul…\n6 16458 Class8  1.69e9 ����…     0 Questio… Metho…  65536 356         ljylby8in…\n\n\nFrom the output above, there are only 6 rows that are affected. Further cross-validation with the data description document found that there should only be 12 unique values for this variable, and including this outlier state value will give 13. Hence this is likely a wrong entry, and so it will be excluded from the analysis in the following code chunk.\n\ndf_StudentRecord &lt;- df_StudentRecord %&gt;%\n  filter (state != '�������')\n\nFor timeconsume, the rows with unexpected value(s) are queried in the following code chunk to better understand the number of affected rows.\n\nOutlier_timeconsume &lt;- df_StudentRecord %&gt;%\n  filter (timeconsume %in% c('-', '--'))\nOutlier_timeconsume\n\n# A tibble: 2,612 × 10\n   index class    time state score title_ID method memory timeconsume student_ID\n   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;     \n 1   191 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          9417c1b4c…\n 2   321 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          8b1fbc973…\n 3   322 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          8b1fbc973…\n 4   366 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          9ea29e4a7…\n 5   396 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          9ea29e4a7…\n 6   397 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          9ea29e4a7…\n 7   422 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          f06c3ddb1…\n 8   423 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          f06c3ddb1…\n 9   424 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          f06c3ddb1…\n10   425 Class1 1.70e9 Erro…     0 Questio… Metho…      0 --          f06c3ddb1…\n# ℹ 2,602 more rows\n\n\nBased on the output, there is a significant number of 2,612 rows with the unexpected value. Hence these rows will be kept in the analysis and replaced with 0 (since there is no existing values of 0 too), however subsequent analysis in this exercise involving the timeconsume variable will treat these values as missing values. This is done in the following code chunk\n\ndf_StudentRecord &lt;- df_StudentRecord %&gt;%\n  mutate(timeconsume = ifelse(timeconsume %in% c(\"-\", \"--\"), 0, timeconsume))\nunique(df_StudentRecord$timeconsume)\n\n  [1] \"3\"   \"2\"   \"5\"   \"4\"   \"1\"   \"9\"   \"6\"   \"0\"   \"18\"  \"61\"  \"7\"   \"59\" \n [13] \"10\"  \"8\"   \"12\"  \"13\"  \"16\"  \"15\"  \"183\" \"68\"  \"314\" \"64\"  \"60\"  \"11\" \n [25] \"96\"  \"94\"  \"58\"  \"67\"  \"54\"  \"17\"  \"122\" \"19\"  \"126\" \"14\"  \"91\"  \"50\" \n [37] \"21\"  \"40\"  \"23\"  \"20\"  \"80\"  \"31\"  \"118\" \"400\" \"63\"  \"25\"  \"27\"  \"29\" \n [49] \"24\"  \"26\"  \"62\"  \"152\" \"39\"  \"22\"  \"117\" \"30\"  \"28\"  \"48\"  \"36\"  \"65\" \n [61] \"47\"  \"46\"  \"45\"  \"52\"  \"32\"  \"42\"  \"34\"  \"38\"  \"187\" \"37\"  \"190\" \"163\"\n [73] \"41\"  \"53\"  \"51\"  \"307\" \"201\" \"184\" \"44\"  \"43\"  \"109\" \"33\"  \"66\"  \"326\"\n [85] \"73\"  \"49\"  \"77\"  \"82\"  \"70\"  \"71\"  \"81\"  \"35\"  \"57\"  \"75\"  \"394\" \"385\"\n [97] \"164\" \"78\"  \"220\" \"217\" \"115\" \"86\"  \"72\"  \"88\"  \"76\"  \"134\" \"55\"  \"84\" \n[109] \"56\"  \"106\" \"166\" \"124\" \"373\" \"289\" \"135\" \"103\" \"114\" \"258\" \"254\" \"85\" \n[121] \"69\"  \"90\"  \"132\" \"173\" \"272\" \"113\" \"116\" \"215\" \"123\" \"246\" \"146\" \"89\" \n[133] \"245\" \"285\" \"205\" \"162\" \"165\" \"266\" \"172\" \"143\" \"377\" \"160\" \"159\" \"182\"\n[145] \"74\"  \"264\" \"153\" \"83\"  \"286\" \"275\" \"331\" \"280\" \"274\" \"269\" \"288\" \"271\"\n[157] \"136\" \"276\" \"277\" \"79\"  \"147\" \"350\" \"315\" \"321\" \"302\"\n\n\n\n\nRemoving Index Col\nEach data set contains an index column, which is possibly to keep track of the original order and the total number of rows. This is no longer required and relevant in the analysis, hence it will be excluded.\n\n#remove index column\ndf_StudentRecord &lt;- df_StudentRecord %&gt;% select(-1)\ndf_TitleInfo &lt;- df_TitleInfo %&gt;% select(-1)\ndf_StudentInfo &lt;- df_StudentInfo %&gt;% select(-1)\n\n\n\nCorrecting Data Types\nBased on the glimpse() function, the time variable of the StudentRecord is currently in numerical format. This will be corrected to date time format with the following steps.\nStep 1: From the data description document, the data collection period spans 148 days from 31/8/2023 to 25/1/2024, and the time variable of the StudentRecord in this data set is in seconds. This is compared against the min and max values of the time variable converted to days and deducted from the given start and end date of the collection period given, in the following code chunk.\n\n# Get the min and max values of the time column\nmin_time &lt;- min(df_StudentRecord$time, na.rm = TRUE)\nmax_time &lt;- max(df_StudentRecord$time, na.rm = TRUE)\n\n# Display the min & max values\ndate_adjustment1 &lt;- as.numeric(as.Date(\"2023-08-31\")) - (min_time / 24 / 60 / 60)\ndate_adjustment2 &lt;- as.numeric(as.Date(\"2024-01-25\")) - (max_time / 24 / 60 / 60)\ndate_adjustmentavg &lt;- as.Date((date_adjustment1 + date_adjustment2)/2, origin = \"1970-01-01\")\ndate_adjustmentavg\n\n[1] \"1969-12-31\"\n\n\nStep 2: Apply date_adjustmentavg to the time variable to amend the data type to date time format in the folloiwing code chunk\n\n# Convert time from timestamp to POSIXct\ndf_StudentRecord$time_change &lt;- as.POSIXct(df_StudentRecord$time, origin=date_adjustmentavg, tz=\"UTC\")\n\nglimpse(df_StudentRecord)\n\nRows: 232,811\nColumns: 10\n$ class       &lt;chr&gt; \"Class1\", \"Class1\", \"Class1\", \"Class1\", \"Class1\", \"Class1\"…\n$ time        &lt;dbl&gt; 1704209872, 1704209852, 1704209838, 1704208923, 1704208359…\n$ state       &lt;chr&gt; \"Absolutely_Correct\", \"Absolutely_Correct\", \"Absolutely_Co…\n$ score       &lt;dbl&gt; 3, 3, 3, 3, 4, 0, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 4, 0, 0…\n$ title_ID    &lt;chr&gt; \"Question_bumGRTJ0c8p4v5D6eHZa\", \"Question_62XbhBvJ8NUSnAp…\n$ method      &lt;chr&gt; \"Method_Cj9Ya2R7fZd6xs1q5mNQ\", \"Method_gj1NLb4Jn7URf9K2kQP…\n$ memory      &lt;dbl&gt; 320, 356, 196, 308, 320, 0, 308, 312, 312, 328, 512, 324, …\n$ timeconsume &lt;chr&gt; \"3\", \"3\", \"2\", \"2\", \"3\", \"5\", \"2\", \"2\", \"3\", \"2\", \"3\", \"2\"…\n$ student_ID  &lt;chr&gt; \"8b6d1125760bd3939b6e\", \"8b6d1125760bd3939b6e\", \"8b6d11257…\n$ time_change &lt;dttm&gt; 2024-01-02 08:45:17, 2024-01-02 08:44:57, 2024-01-02 08:4…\n\n\nFurther, the timeconsume variable will be converted to numeric, wherein since the ‘-’ and ‘–’ values found earlier had taken the value of 0 and there will not be an issue of NA values affecting subsequent analysis.\n\ndf_StudentRecord &lt;- df_StudentRecord %&gt;%\n  mutate(timeconsume = as.numeric(timeconsume))\n\nglimpse(df_StudentRecord)\n\nRows: 232,811\nColumns: 10\n$ class       &lt;chr&gt; \"Class1\", \"Class1\", \"Class1\", \"Class1\", \"Class1\", \"Class1\"…\n$ time        &lt;dbl&gt; 1704209872, 1704209852, 1704209838, 1704208923, 1704208359…\n$ state       &lt;chr&gt; \"Absolutely_Correct\", \"Absolutely_Correct\", \"Absolutely_Co…\n$ score       &lt;dbl&gt; 3, 3, 3, 3, 4, 0, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 4, 0, 0…\n$ title_ID    &lt;chr&gt; \"Question_bumGRTJ0c8p4v5D6eHZa\", \"Question_62XbhBvJ8NUSnAp…\n$ method      &lt;chr&gt; \"Method_Cj9Ya2R7fZd6xs1q5mNQ\", \"Method_gj1NLb4Jn7URf9K2kQP…\n$ memory      &lt;dbl&gt; 320, 356, 196, 308, 320, 0, 308, 312, 312, 328, 512, 324, …\n$ timeconsume &lt;dbl&gt; 3, 3, 2, 2, 3, 5, 2, 2, 3, 2, 3, 2, 2, 3, 3, 3, 2, 3, 3, 5…\n$ student_ID  &lt;chr&gt; \"8b6d1125760bd3939b6e\", \"8b6d1125760bd3939b6e\", \"8b6d11257…\n$ time_change &lt;dttm&gt; 2024-01-02 08:45:17, 2024-01-02 08:44:57, 2024-01-02 08:4…\n\n\n\n\n\nCreate Merged Dataset\nTo prepare for cross dataset visualisation and analysis of variables, the 3 data sets are joined on title_id and student_id variables in the following code chunks.\n\n# Merge StudentInfo with SubmitRecord based on student_ID\nmerged_data &lt;- merge(df_StudentRecord, df_StudentInfo, by = \"student_ID\")\n\n# Merge TitleInfo with the already merged data based on title_ID\nmerged_data &lt;- merge(merged_data, df_TitleInfo_gp, by = \"title_ID\")\n\nmerged_data &lt;- merged_data %&gt;%\n  rename(\n    actual_score = score.x,\n    question_score = score.y\n  )\n\n\nsaveRDS(merged_data, \"merged_data_df.rds\")\n\n\nsummary (merged_data)\n\n   title_ID          student_ID           class                time          \n Length:232811      Length:232811      Length:232811      Min.   :1.693e+09  \n Class :character   Class :character   Class :character   1st Qu.:1.697e+09  \n Mode  :character   Mode  :character   Mode  :character   Median :1.699e+09  \n                                                          Mean   :1.699e+09  \n                                                          3rd Qu.:1.701e+09  \n                                                          Max.   :1.706e+09  \n    state            actual_score       method              memory       \n Length:232811      Min.   :0.0000   Length:232811      Min.   :    0.0  \n Class :character   1st Qu.:0.0000   Class :character   1st Qu.:  188.0  \n Mode  :character   Median :0.0000   Mode  :character   Median :  324.0  \n                    Mean   :0.8992                      Mean   :  345.7  \n                    3rd Qu.:2.0000                      3rd Qu.:  356.0  \n                    Max.   :4.0000                      Max.   :65536.0  \n  timeconsume       time_change                         sex           \n Min.   :  0.000   Min.   :2023-08-31 01:53:48.50   Length:232811     \n 1st Qu.:  3.000   1st Qu.:2023-10-08 05:16:53.50   Class :character  \n Median :  4.000   Median :2023-10-29 04:48:53.50   Mode  :character  \n Mean   :  8.991   Mean   :2023-10-31 19:22:50.95                     \n 3rd Qu.:  5.000   3rd Qu.:2023-11-25 08:34:41.00                     \n Max.   :400.000   Max.   :2024-01-24 22:06:11.50                     \n      age           major            knowledge         sub_knowledge     \n Min.   :18.00   Length:232811      Length:232811      Length:232811     \n 1st Qu.:19.00   Class :character   Class :character   Class :character  \n Median :21.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :21.06                                                           \n 3rd Qu.:23.00                                                           \n Max.   :24.00                                                           \n     b3C9s             g7R2j            k4W1c       m3D1v       \n Min.   :0.00000   Min.   :0.0000   Min.   :0   Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0   1st Qu.:0.0000  \n Median :0.00000   Median :0.0000   Median :0   Median :0.0000  \n Mean   :0.06348   Mean   :0.1419   Mean   :0   Mean   :0.2051  \n 3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0   3rd Qu.:0.0000  \n Max.   :1.00000   Max.   :1.0000   Max.   :0   Max.   :1.0000  \n     r8S3g            s8Y2f       t5V9e            y9W5d        question_score \n Min.   :0.0000   Min.   :0   Min.   :0.0000   Min.   :0.0000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:0   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:2.000  \n Median :0.0000   Median :0   Median :0.0000   Median :0.0000   Median :3.000  \n Mean   :0.1578   Mean   :0   Mean   :0.1631   Mean   :0.2686   Mean   :2.549  \n 3rd Qu.:0.0000   3rd Qu.:0   3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:3.000  \n Max.   :1.0000   Max.   :0   Max.   :1.0000   Max.   :1.0000   Max.   :4.000  \n b3C9s_j0v1yls8   b3C9s_l4z6od7y    g7R2j_e0v1yls8  g7R2j_j1g8gd3v   \n Min.   :0.0000   Min.   :0.00000   Min.   :0.000   Min.   :0.00000  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.000   1st Qu.:0.00000  \n Median :0.0000   Median :0.00000   Median :0.000   Median :0.00000  \n Mean   :0.0272   Mean   :0.03629   Mean   :0.116   Mean   :0.02595  \n 3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:0.000   3rd Qu.:0.00000  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.000   Max.   :1.00000  \n k4W1c_h5r6nux7 m3D1v_r1d7fr3l  m3D1v_t0v5ts9h    m3D1v_v3d9is1x   \n Min.   :0      Min.   :0.000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:0      1st Qu.:0.000   1st Qu.:0.00000   1st Qu.:0.00000  \n Median :0      Median :0.000   Median :0.00000   Median :0.00000  \n Mean   :0      Mean   :0.147   Mean   :0.02589   Mean   :0.03214  \n 3rd Qu.:0      3rd Qu.:0.000   3rd Qu.:0.00000   3rd Qu.:0.00000  \n Max.   :0      Max.   :1.000   Max.   :1.00000   Max.   :1.00000  \n r8S3g_l0p5viby    r8S3g_n0m9rsw4   s8Y2f_v4x8by9j t5V9e_e1k6cixp  \n Min.   :0.00000   Min.   :0.0000   Min.   :0      Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0      1st Qu.:0.0000  \n Median :0.00000   Median :0.0000   Median :0      Median :0.0000  \n Mean   :0.02526   Mean   :0.1325   Mean   :0      Mean   :0.1631  \n 3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0      3rd Qu.:0.0000  \n Max.   :1.00000   Max.   :1.0000   Max.   :0      Max.   :1.0000  \n y9W5d_c0w4mj5h   y9W5d_e2j7p95s    y9W5d_p8g6dgtv   \n Min.   :0.0000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.00000  \n Median :0.0000   Median :0.00000   Median :0.00000  \n Mean   :0.1983   Mean   :0.02861   Mean   :0.04174  \n 3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:0.00000  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.00000"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#learning-modes",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#learning-modes",
    "title": "Take-home Ex3",
    "section": "Learning modes",
    "text": "Learning modes\nBased on the given data, the relevant features that best defines a learner’s learning mode is assessed to be as follows:\n\nPeak answering hours determined by (a) day of the week and (b) time of the day\nVariety of question types attempted determined by (a) total number of different questions attempted, (b) total number of different knowledge and sub knowledge areas covered,\nDepth of question types and answers determined by (a) mean question scores, (b) mean memory size of file submissions\nLevel of learning effort determined by (a) total number of answering attempts, (b) total number of different answering methods, (C) total memory size of file submission\n\n\nFeature engineering\n\nPeak answering hours Boolean Integer Variables\nSplitting Date and time up from the earlier created time_change date-time variable, and adding 2 derived variables for boolean integer values for weekday (Mon to Fri) and working hours (8am to 8pm) with the following code chunk.\n\nmerged_data_lm &lt;- merged_data %&gt;%\n  mutate(\n    date = as.Date(time_change),\n    time = as_hms(format(time_change, \"%H:%M:%S\")),\n    is_weekday = as.numeric(wday(date) %in% 2:6),  # Monday to Friday 1, else 0\n    is_working_hours = as.numeric(hour(time) &gt;= 8 & hour(time) &lt; 20)  # 8am to 8pm 1, else 0\n  )\n\nglimpse(merged_data_lm)\n\nRows: 232,811\nColumns: 42\n$ title_ID         &lt;chr&gt; \"Question_3MwAFlmNO8EKrpY5zjUd\", \"Question_3MwAFlmNO8…\n$ student_ID       &lt;chr&gt; \"d554e419f820fa5cb0ca\", \"b92448e12093e45dc6ff\", \"6b22…\n$ class            &lt;chr&gt; \"Class9\", \"Class8\", \"Class12\", \"Class7\", \"Class1\", \"C…\n$ time             &lt;time&gt; 04:09:22, 07:11:39, 01:22:28, 22:25:49, 08:11:04, 02…\n$ state            &lt;chr&gt; \"Partially_Correct\", \"Partially_Correct\", \"Error1\", \"…\n$ actual_score     &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 1, 0, 2, 1, 1, 0, 1, 1, 0, 1, 0, 1,…\n$ method           &lt;chr&gt; \"Method_BXr9AIsPQhwNvyGdZL57\", \"Method_BXr9AIsPQhwNvy…\n$ memory           &lt;dbl&gt; 196, 332, 0, 196, 0, 0, 336, 320, 324, 204, 340, 320,…\n$ timeconsume      &lt;dbl&gt; 2, 6, 2, 3, 4, 3, 4, 4, 2, 2, 3, 5, 3, 4, 3, 5, 1, 3,…\n$ time_change      &lt;dttm&gt; 2023-10-03 04:09:22, 2023-11-10 07:11:39, 2023-10-16…\n$ sex              &lt;chr&gt; \"male\", \"female\", \"female\", \"male\", \"male\", \"male\", \"…\n$ age              &lt;dbl&gt; 19, 21, 23, 20, 21, 20, 19, 20, 21, 21, 21, 21, 21, 2…\n$ major            &lt;chr&gt; \"J40192\", \"J23517\", \"J87654\", \"J87654\", \"J40192\", \"J4…\n$ knowledge        &lt;chr&gt; \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\", \"t5V9e\",…\n$ sub_knowledge    &lt;chr&gt; \"t5V9e_e1k6cixp\", \"t5V9e_e1k6cixp\", \"t5V9e_e1k6cixp\",…\n$ b3C9s            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ g7R2j            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ k4W1c            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ m3D1v            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ r8S3g            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ s8Y2f            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ t5V9e            &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ y9W5d            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ question_score   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ b3C9s_j0v1yls8   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ b3C9s_l4z6od7y   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ g7R2j_e0v1yls8   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ g7R2j_j1g8gd3v   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ k4W1c_h5r6nux7   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ m3D1v_r1d7fr3l   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ m3D1v_t0v5ts9h   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ m3D1v_v3d9is1x   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ r8S3g_l0p5viby   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ r8S3g_n0m9rsw4   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ s8Y2f_v4x8by9j   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ t5V9e_e1k6cixp   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ y9W5d_c0w4mj5h   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ y9W5d_e2j7p95s   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ y9W5d_p8g6dgtv   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ date             &lt;date&gt; 2023-10-03, 2023-11-10, 2023-10-16, 2023-09-28, 2023…\n$ is_weekday       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,…\n$ is_working_hours &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,…\n\n\n\n\nGroup By Student ID\nThe following variables will be obtained with the code chunk below in preparation for clustering analysis\n\nPeak answering hours\n\n\npercentage of answers on weekdays,\npercentage of answers during working hours\n\n\nVariety of question types attempted\n\n\ntotal number of different questions attempted,\ntotal number of different knowledge and sub knowledge areas covered,\n\n\nDepth of question types and answers\n\n\nmean question scores,\nmean memory size of file submissions\n\n\nLevel of learning effort\n\n\ntotal number of answering attempts,\ntotal number of different answering methods,\n\n\n\ntotal memory size of file submission\n\n\nStudentLM_data &lt;- merged_data_lm %&gt;%\n  group_by(student_ID) %&gt;%\n  summarize(\n    pct_answers_weekdays = sum(is_weekday, na.rm = TRUE) / n() * 100,\n    pct_answers_working_hours = sum(is_working_hours, na.rm = TRUE) / n() * 100,\n    total_diff_questions_attempted = n_distinct(title_ID, na.rm = TRUE),\n    total_diff_knowledge_areas = sum(colSums(across(16:23, as.numeric)) &gt; 0),\n    total_diff_sub_knowledge_areas = sum(colSums(across(25:39, as.numeric)) &gt; 0),\n    mean_question_scores = mean(question_score, na.rm = TRUE),\n    mean_memory_size = mean(memory, na.rm = TRUE),\n    total_answering_attempts = n(),\n    total_diff_answering_methods = n_distinct(method, na.rm = TRUE),\n    total_memory_size = sum(memory, na.rm = TRUE)\n  )\n\nglimpse(StudentLM_data)\n\nRows: 1,364\nColumns: 11\n$ student_ID                     &lt;chr&gt; \"0088dc183f73c83f763e\", \"00cbf05221bb47…\n$ pct_answers_weekdays           &lt;dbl&gt; 94.88372, 88.75000, 78.03347, 65.54622,…\n$ pct_answers_working_hours      &lt;dbl&gt; 14.883721, 10.416667, 15.899582, 21.008…\n$ total_diff_questions_attempted &lt;int&gt; 38, 38, 38, 38, 38, 38, 38, 38, 38, 35,…\n$ total_diff_knowledge_areas     &lt;int&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 2, 6, 6, …\n$ total_diff_sub_knowledge_areas &lt;int&gt; 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,…\n$ mean_question_scores           &lt;dbl&gt; 2.339535, 2.266667, 2.744770, 2.546218,…\n$ mean_memory_size               &lt;dbl&gt; 224.0186, 407.5833, 285.5565, 273.8151,…\n$ total_answering_attempts       &lt;int&gt; 215, 240, 478, 119, 204, 145, 253, 543,…\n$ total_diff_answering_methods   &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 5, 5, …\n$ total_memory_size              &lt;dbl&gt; 48164, 97820, 136496, 32584, 35260, 424…\n\n\n\n\nRemoving highly skewed columns\nInspecting the data frame, 3 variables were found to be highly skewed and concentrated within a small range of values, hence they are removed for more meaningful analysis, with the following code chunk.\n\nStudentLM_data &lt;- StudentLM_data %&gt;%\n  select(-total_diff_knowledge_areas, -total_diff_sub_knowledge_areas, -total_diff_answering_methods)\n\nglimpse(StudentLM_data)\n\nRows: 1,364\nColumns: 8\n$ student_ID                     &lt;chr&gt; \"0088dc183f73c83f763e\", \"00cbf05221bb47…\n$ pct_answers_weekdays           &lt;dbl&gt; 94.88372, 88.75000, 78.03347, 65.54622,…\n$ pct_answers_working_hours      &lt;dbl&gt; 14.883721, 10.416667, 15.899582, 21.008…\n$ total_diff_questions_attempted &lt;int&gt; 38, 38, 38, 38, 38, 38, 38, 38, 38, 35,…\n$ mean_question_scores           &lt;dbl&gt; 2.339535, 2.266667, 2.744770, 2.546218,…\n$ mean_memory_size               &lt;dbl&gt; 224.0186, 407.5833, 285.5565, 273.8151,…\n$ total_answering_attempts       &lt;int&gt; 215, 240, 478, 119, 204, 145, 253, 543,…\n$ total_memory_size              &lt;dbl&gt; 48164, 97820, 136496, 32584, 35260, 424…\n\n\n\n\n\nDetermine number of K-Means clusters\nTo determine the ideal number of clusters for K-means clustering on the recompiled learners’ learning mode features, a silhouette analysis and SSE elbow method are performed in the following code chunks.\n\nSilhouette analysis\n\n# Exclude non-numeric columns\nStudentLM_data_numeric &lt;- StudentLM_data %&gt;%\n  select(-student_ID)\n\n# Function to compute silhouette widths\nsilhouette_analysis &lt;- function(data, max_clusters) {\n  avg_sil_widths &lt;- numeric(max_clusters)\n  \n  for (k in 2:max_clusters) {\n    # Perform k-means clustering\n    kmeans_result &lt;- kmeans(data, centers = k, nstart = 25)\n    \n    # Compute silhouette widths\n    sil &lt;- silhouette(kmeans_result$cluster, dist(data))\n    \n    # Calculate average silhouette width\n    avg_sil_widths[k] &lt;- mean(sil[, 3])\n  }\n  \n  return(avg_sil_widths)\n}\n\n# Determine the maximum number of clusters to test\nmax_clusters &lt;- 10\n\n# Perform silhouette analysis\navg_sil_widths &lt;- silhouette_analysis(StudentLM_data_numeric, max_clusters)\n\n# Plot the average silhouette widths\nplot(1:max_clusters, avg_sil_widths, type = \"b\", pch = 19, frame = FALSE,\n     xlab = \"Number of clusters\", ylab = \"Average silhouette width\",\n     main = \"Silhouette Analysis for Determining Optimal Number of Clusters\")\n\n# Highlight the optimal number of clusters\noptimal_clusters &lt;- which.max(avg_sil_widths)\npoints(optimal_clusters, avg_sil_widths[optimal_clusters], col = \"red\", pch = 19)\n\n\n\n\n\n\nSSE-Elbow method\n\n# Function to compute SSE for different numbers of clusters\ncompute_sse &lt;- function(data, max_clusters) {\n  sse &lt;- numeric(max_clusters)\n  \n  for (k in 1:max_clusters) {\n    # Perform k-means clustering\n    kmeans_result &lt;- kmeans(data, centers = k, nstart = 25)\n    \n    # Compute SSE\n    sse[k] &lt;- kmeans_result$tot.withinss\n  }\n  \n  return(sse)\n}\n\n# Determine the maximum number of clusters to test\nmax_clusters &lt;- 10\n\n# Compute SSE for each number of clusters\nsse_values &lt;- compute_sse(StudentLM_data_numeric, max_clusters)\n\n# Plot SSE against number of clusters\nplot(1:max_clusters, sse_values, type = \"b\", pch = 19, frame = FALSE,\n     xlab = \"Number of clusters\", ylab = \"SSE\",\n     main = \"Elbow Method for Optimal Number of Clusters\")\n\n# Add text for elbow point\nelbow_point &lt;- which.min(diff(sse_values)) + 1\ntext(elbow_point, sse_values[elbow_point], labels = paste(\"Elbow Point:\", elbow_point), pos = 4, col = \"red\")\n\n\n\n\n\n\n\nK Means clustering\nK Means clustering is then performed on the recompiled learners’ learning mode features with the number of clusters set as 2 based on the above results, in the following code chunk\n\n# Drop the student_ID column\nclustering_data &lt;- StudentLM_data %&gt;%\n  select(-student_ID)\n\n# Standardize the data\nclustering_data_scaled &lt;- scale(clustering_data)\n\n# Perform k-means clustering\nset.seed(123)  # For reproducibility\nkmeans_result &lt;- kmeans(clustering_data_scaled, centers = 2, nstart = 25)\n\n# Add the cluster assignments to the original data\nStudentLM_data$cluster &lt;- kmeans_result$cluster\n\n\n\nVisualisation of K Means clusters\nThe first plot for visualisation of the K means cluster is the Principal Component Analysis (PCA) Plot, which gives an initial sensing of the separation of the clusters based on first 2 PCA components that rank the highest in distinctness amongst the features used. This is plotted with the following code chunk.\n\n# Perform PCA\npca_result &lt;- prcomp(StudentLM_data[-1], scale. = TRUE)\n\n# Get PCA scores\npca_scores &lt;- as.data.frame(predict(pca_result))\n\n# Add cluster information to PCA scores\npca_scores$cluster &lt;- factor(StudentLM_data$cluster)\n\n# Plot PCA results with cluster color coding\npca_plot &lt;- ggplot(pca_scores, aes(PC1, PC2, color = cluster)) +\n  geom_point(size = 3) +\n  scale_color_discrete(name = \"Cluster\") +\n  labs(x = \"Principal Component 1\", y = \"Principal Component 2\",\n       title = \"PCA Plot of Clusters\") +\n  theme_minimal()\n\n# Display the plot\npca_plot\n\n\n\n\nBased on the PCA plot, the clusters are visually clearly separated, suggesting that the clusters are distinct, especially in relation to the top 2 PCA components in the x and y-axis.\nNext to visualise the distribution of the 2 clusters across all the features used for the K Means clustering, a parallel coordinate plot is used, with the following code chunk.\n\n#| fig-width: 15\n#| fig-height: 12\n\nStudentLM_data_factor &lt;- StudentLM_data\nStudentLM_data_factor$cluster &lt;- as.character(StudentLM_data_factor$cluster)\n\nggparcoord(data = StudentLM_data_factor, \n           columns = c(2:8), \n           groupColumn = 9,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of Students' learning modes\")+\n   theme(axis.text.x = element_text(angle = 30))\n\n\n\n\nBased on the plot, there is varying degree of separation and distinctness between the 2 clusters across different variables. The more distinct separation are in variables such as total-memory size of answers, total answering attempt and mean memory size, where cluster 2 tends to fare better in these metrics suggesting that perhaps cluster 2 is the more hardworking learning mode among the 2.\n\n\nHierarchical Clustering and Visualisation with heatmap\nAs an alternative to K means, hierarchical clustering is also considered, and initiates with mapping the data frame into a data matrix, and thereby using the dend_expend function to determine the best clustering method.\n\nrow.names(StudentLM_data) &lt;- StudentLM_data$student_ID\nStudentLM_data1 &lt;- select(StudentLM_data, c(1, 2:8))\nStudentLM_data_matrix &lt;- data.matrix(StudentLM_data1)\n\nStudentLM_data_d &lt;- dist(normalize(StudentLM_data_matrix[, -c(1)]), method = \"euclidean\")\ndend_expend(StudentLM_data_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.4338557\n2      unknown        ward.D2 0.4397626\n3      unknown         single 0.8240479\n4      unknown       complete 0.6816024\n5      unknown        average 0.8551659\n6      unknown       mcquitty 0.7179679\n7      unknown         median 0.6816340\n8      unknown       centroid 0.8361293\n\n\nBased on the output above, the average method will be the most optimal.\nA silhoutte plot in the same approach as before is also done with the following code chunk to give an initial sensing of the number of clusters to achieve higher cluster separation and distinctness. This is done in the following code chunk.\n\nStudentLM_data_clust &lt;- hclust(StudentLM_data_d, method = \"average\")\nnum_k &lt;- find_k(StudentLM_data_clust)\nplot(num_k)\n\n\n\n\nBased on the output above, the same conclusion is arrived at, with 2 clusters being the idea number.\nNow using the 2 parameters, the hierarchical clustering using an interactive heatmap for visualisation is plot with the following code chunk.\n\n#| fig-width: 15\n#| fig-height: 12\nheatmaply(normalize(StudentLM_data_matrix[, -c(1)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 2,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,          \n          main=\"Students' Learning Mode Clustering \\nDataTransformation using Normalise Method\",\n          xlab = \"Student_IDs\",\n          ylab = \"Learning Mode Features\"\n)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#knowledge-acquisition",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#knowledge-acquisition",
    "title": "Take-home Ex3",
    "section": "Knowledge Acquisition",
    "text": "Knowledge Acquisition\nBased on the given data, the relevant features that best defines a learner’s knowledge acquisition is assessed to be as follows:\n\nknowledge mastery determined by (a) overall sum of highest actual score for each question attempted and (b) sum of highest actual score of each question by knowledge area\ncorrect answering rate determined by (a) percentage of answers absolutely correct, (b) total number of questions with answers absolutely correct and partially correct\n\n\nFeature engineering\n\nGroup By Student ID\nThe following variables will be obtained with the code chunk below in preparation for visualisation and analysis of Knowledge Acquisition with respect to the various learning modes.\n\nknowledge mastery\n\n\noverall sum of highest actual score for each question attempted and\nsum of highest actual score of each question by knowledge area\n\n\ncorrect answering rate\n\n\npercentage of answers absolutely correct,\ntotal number of questions with answers absolutely correct and partially correct\n\n\nStudentKA_data &lt;- merged_data %&gt;%\n  group_by(student_ID) %&gt;%\n  summarize(\n    # Part (a): Sum of highest actual score for each question attempted\n    sum_highest_actual_score = sum(sapply(unique(title_ID), function(x) {\n      max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, \"actual_score\"])\n    })),\n    \n    # Part (b): Sum of highest actual score for each knowledge area\n    sum_highest_actual_score_b3C9s = sum(sapply(unique(title_ID), function(x) {\n      ifelse(any(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 16] == 1),\n             max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID & merged_data[, 16] == 1, \"actual_score\"]),\n             0)\n    })),\n    sum_highest_actual_score_g7R2j = sum(sapply(unique(title_ID), function(x) {\n      ifelse(any(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 17] == 1),\n             max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID & merged_data[, 17] == 1, \"actual_score\"]),\n             0)\n    })),\n    sum_highest_actual_score_k4W1c = sum(sapply(unique(title_ID), function(x) {\n      ifelse(any(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 18] == 1),\n             max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID & merged_data[, 18] == 1, \"actual_score\"]),\n             0)\n    })),\n    sum_highest_actual_score_m3D1v = sum(sapply(unique(title_ID), function(x) {\n      ifelse(any(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 19] == 1),\n             max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID & merged_data[, 19] == 1, \"actual_score\"]),\n             0)\n    })),\n    sum_highest_actual_score_r8S3g = sum(sapply(unique(title_ID), function(x) {\n      ifelse(any(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 20] == 1),\n             max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID & merged_data[, 20] == 1, \"actual_score\"]),\n             0)\n    })),\n    sum_highest_actual_score_s8Y2f = sum(sapply(unique(title_ID), function(x) {\n      ifelse(any(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 21] == 1),\n             max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID & merged_data[, 21] == 1, \"actual_score\"]),\n             0)\n    })),\n    sum_highest_actual_score_t5V9e = sum(sapply(unique(title_ID), function(x) {\n      ifelse(any(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 22] == 1),\n             max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID & merged_data[, 22] == 1, \"actual_score\"]),\n             0)\n    })),\n    sum_highest_actual_score_y9W5d = sum(sapply(unique(title_ID), function(x) {\n      ifelse(any(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID, 23] == 1),\n             max(merged_data[merged_data$title_ID == x & merged_data$student_ID == cur_group()$student_ID & merged_data[, 23] == 1, \"actual_score\"]),\n             0)\n    })),\n    \n    # Part (c): Percentage of answers absolutely correct\n    pct_answers_absolutely_correct = (sum(state == \"Absolutely_Correct\") / n()) * 100,\n    \n    # Part (d): Total number of questions with answers absolutely correct and partially correct\n    total_questions_correct_or_partial = length(unique(title_ID[state %in% c(\"Partially_Correct\", \"Absolutely_Correct\")]))\n  )\n\n\nglimpse(StudentKA_data)\n\nRows: 1,364\nColumns: 12\n$ student_ID                         &lt;chr&gt; \"0088dc183f73c83f763e\", \"00cbf05221…\n$ sum_highest_actual_score           &lt;dbl&gt; 100, 100, 100, 100, 100, 100, 100, …\n$ sum_highest_actual_score_b3C9s     &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10,…\n$ sum_highest_actual_score_g7R2j     &lt;dbl&gt; 15, 15, 15, 15, 15, 15, 15, 15, 15,…\n$ sum_highest_actual_score_k4W1c     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ sum_highest_actual_score_m3D1v     &lt;dbl&gt; 27, 27, 27, 27, 27, 27, 27, 27, 27,…\n$ sum_highest_actual_score_r8S3g     &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 0, 5,…\n$ sum_highest_actual_score_s8Y2f     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ sum_highest_actual_score_t5V9e     &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10,…\n$ sum_highest_actual_score_y9W5d     &lt;dbl&gt; 33, 33, 33, 33, 33, 33, 33, 33, 33,…\n$ pct_answers_absolutely_correct     &lt;dbl&gt; 20.930233, 19.166667, 11.087866, 32…\n$ total_questions_correct_or_partial &lt;int&gt; 38, 38, 38, 38, 38, 38, 38, 38, 38,…\n\n\n\n\nRemoving highly skewed columns\nInspecting the data frame, 2 variables were found to be highly skewed and concentrated within a small range of values, hence they are removed for more meaningful analysis, with the following code chunk.\n\nStudentKA_data &lt;- StudentKA_data %&gt;%\n  select(-sum_highest_actual_score_k4W1c, -sum_highest_actual_score_s8Y2f)\n\nglimpse(StudentKA_data)\n\nRows: 1,364\nColumns: 10\n$ student_ID                         &lt;chr&gt; \"0088dc183f73c83f763e\", \"00cbf05221…\n$ sum_highest_actual_score           &lt;dbl&gt; 100, 100, 100, 100, 100, 100, 100, …\n$ sum_highest_actual_score_b3C9s     &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10,…\n$ sum_highest_actual_score_g7R2j     &lt;dbl&gt; 15, 15, 15, 15, 15, 15, 15, 15, 15,…\n$ sum_highest_actual_score_m3D1v     &lt;dbl&gt; 27, 27, 27, 27, 27, 27, 27, 27, 27,…\n$ sum_highest_actual_score_r8S3g     &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 0, 5,…\n$ sum_highest_actual_score_t5V9e     &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10,…\n$ sum_highest_actual_score_y9W5d     &lt;dbl&gt; 33, 33, 33, 33, 33, 33, 33, 33, 33,…\n$ pct_answers_absolutely_correct     &lt;dbl&gt; 20.930233, 19.166667, 11.087866, 32…\n$ total_questions_correct_or_partial &lt;int&gt; 38, 38, 38, 38, 38, 38, 38, 38, 38,…\n\n\n\n\n\nMerging Students’ Learning Modes with Knowledge Acqusition features\nWith the both data frames prepared, they will now be merged for next sub-task which involves comparison of learners’ knowledge acqusition in respect to learning mode, and subsequently to identify patterns and relationship\n\n# Join the two dataframes on the column student_ID\nStudentLMKA_data &lt;- left_join(StudentLM_data, StudentKA_data, by = \"student_ID\")\n\nglimpse(StudentLMKA_data)\n\nRows: 1,364\nColumns: 18\n$ student_ID                         &lt;chr&gt; \"0088dc183f73c83f763e\", \"00cbf05221…\n$ pct_answers_weekdays               &lt;dbl&gt; 94.88372, 88.75000, 78.03347, 65.54…\n$ pct_answers_working_hours          &lt;dbl&gt; 14.883721, 10.416667, 15.899582, 21…\n$ total_diff_questions_attempted     &lt;int&gt; 38, 38, 38, 38, 38, 38, 38, 38, 38,…\n$ mean_question_scores               &lt;dbl&gt; 2.339535, 2.266667, 2.744770, 2.546…\n$ mean_memory_size                   &lt;dbl&gt; 224.0186, 407.5833, 285.5565, 273.8…\n$ total_answering_attempts           &lt;int&gt; 215, 240, 478, 119, 204, 145, 253, …\n$ total_memory_size                  &lt;dbl&gt; 48164, 97820, 136496, 32584, 35260,…\n$ cluster                            &lt;int&gt; 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2,…\n$ sum_highest_actual_score           &lt;dbl&gt; 100, 100, 100, 100, 100, 100, 100, …\n$ sum_highest_actual_score_b3C9s     &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10,…\n$ sum_highest_actual_score_g7R2j     &lt;dbl&gt; 15, 15, 15, 15, 15, 15, 15, 15, 15,…\n$ sum_highest_actual_score_m3D1v     &lt;dbl&gt; 27, 27, 27, 27, 27, 27, 27, 27, 27,…\n$ sum_highest_actual_score_r8S3g     &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 0, 5,…\n$ sum_highest_actual_score_t5V9e     &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10,…\n$ sum_highest_actual_score_y9W5d     &lt;dbl&gt; 33, 33, 33, 33, 33, 33, 33, 33, 33,…\n$ pct_answers_absolutely_correct     &lt;dbl&gt; 20.930233, 19.166667, 11.087866, 32…\n$ total_questions_correct_or_partial &lt;int&gt; 38, 38, 38, 38, 38, 38, 38, 38, 38,…\n\n\n\n\nVisualisation of Knowledge Aquisition by learning mode clusters\nTo visualise differences in the performance in total number of questions that had correct or partially correct answers, a ridgeline plot to compare the shape of distribution of students in both clusters in the same axis, using the following code chunk.\n\nStudentLMKA_data$cluster &lt;- as.factor(StudentLMKA_data$cluster)\n\n# Plot\nggplot(StudentLMKA_data, \n       aes(x = total_questions_correct_or_partial, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\nCluster 2 has a sharper peak and more packed to the right which suggests that students in this cluster generally performed better, while for cluster 1 there is a 2nd smaller group of that performs even worse.\nA multi faceted plot to compare the distribution of answering performance in respect to the 2 clusters across 6 knowledge areas is plot with the following code chunk.\n\n#| fig-width: 16\n#| fig-height: 20\n\na &lt;- ggplot(StudentLMKA_data, \n       aes(x = sum_highest_actual_score_b3C9s, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\nb &lt;- ggplot(StudentLMKA_data, \n       aes(x = sum_highest_actual_score_g7R2j,\n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\nc &lt;- ggplot(StudentLMKA_data, \n       aes(x = sum_highest_actual_score_m3D1v, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\nd &lt;- ggplot(StudentLMKA_data, \n       aes(x = sum_highest_actual_score_r8S3g, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\ne &lt;- ggplot(StudentLMKA_data, \n       aes(x = sum_highest_actual_score_t5V9e, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\nf &lt;- ggplot(StudentLMKA_data, \n       aes(x = sum_highest_actual_score_y9W5d, \n           y = cluster,\n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n(a + b) / (c + d) / (e + f)\n\n\n\n\nThe findings are highly congruent with the earlier ridge plot, which found that cluster 2 had performed better with a sharper peak and more concentration of learners to the right, where as cluster 1 had small pockets of learners to the left instead.\nA statistical violin plot to perform both a mathematical 2 sample mean test in tandem with a visual analysis of the difference in the distribution of the students’ total actual score in the answering records in respect of the 2 clusters is plot with the following code chunk.\n\nggbetweenstats(\n  data = StudentLMKA_data,\n  x = cluster, \n  y = sum_highest_actual_score,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\nBased on the figures, the p-value is extremely small which suggest that there is strong statistical significance between the 2 clusters in the performance of total actual score of students in each cluster, wherein cluster 2 fared better than cluster 1, it also shows that cluster 2 is much smaller than cluster 1.\nLastly a similar statistical violin plot to analyse the differences in percentage of answers that were absolutely correct in respect of the 2 clusters is plot in the following code chunk.\n\nggbetweenstats(\n  data = StudentLMKA_data,\n  x = cluster, \n  y = pct_answers_absolutely_correct,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\nBased on the figures, the p-value is extremely small which suggest that there is strong statistical significance between the 2 clusters in the performance of total actual score of students in each cluster, wherein surprisingly, cluster 1 had fared better than cluster 2, cluster 2 had a smaller spread and more concentrated compared to cluster 1."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#conclusion",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#conclusion",
    "title": "Take-home Ex3",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, the visual analysis of learning modes clustering found that 2 substantially distinct clusters can be formed using the selected students’ learning mode features, whereby cluster 2 tends to be the more earnest learning mode cluster.\nUsing these clusters to draw a relationship with indicators of students’ knowledge acquistion found that cluster 2 also had a better knowledge acquisition although cluster 1 students seems to have submit lesser answers in general, hence having a higher percentage of correct answers.\nAnd therefore, the consensus in the analysis found that there is a statistically significant relationship between the selected indicators for more hardworking learning modes with better knowledge aquisition."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex9/In-class_Ex9.html",
    "href": "In-class_Ex/In-class_Ex9/In-class_Ex9.html",
    "title": "In-class Ex9",
    "section": "",
    "text": "The key learning objective of this hands-on exercise is to plot functional and truthful choropleth maps using appropriate R packages."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex9/In-class_Ex9.html#overview",
    "href": "In-class_Ex/In-class_Ex9/In-class_Ex9.html#overview",
    "title": "In-class Ex9",
    "section": "",
    "text": "The key learning objective of this hands-on exercise is to plot functional and truthful choropleth maps using appropriate R packages."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex9/In-class_Ex9.html#getting-started",
    "href": "In-class_Ex/In-class_Ex9/In-class_Ex9.html#getting-started",
    "title": "In-class Ex9",
    "section": "Getting Started",
    "text": "Getting Started\nThe code chunk below installs and loads various required packages into R environment\n\npacman:: p_load(scatterPlotMatrix, parallelPlot, cluster, factoextra, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex9/In-class_Ex9.html#importing-data",
    "href": "In-class_Ex/In-class_Ex9/In-class_Ex9.html#importing-data",
    "title": "In-class Ex9",
    "section": "Importing Data",
    "text": "Importing Data\n\nwine &lt;- read_csv(\"data/wine_quality.csv\")\n\n\nggplot(data = wine,\n       aes(x = type)) +\n  geom_bar()\n\n\n\n\n\nwhitewine &lt;- wine %&gt;%\n  filter(type == \"white\") %&gt;%\n  select(c(1:11))\n\n\nscatterPlotMatrix(whitewine, \n                  corrPlotType = \"Text\",\n                  distribType = 1,\n                  rotateTitle = TRUE,\n                  width = 900,\n                  height = 900\n                  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9a.html",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9a.html",
    "title": "Hands-on Ex9a",
    "section": "",
    "text": "Ternary plots are a way of displaying the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.) It’s display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nThe key learning objective of this hands-on exercise is to build ternary plot programmatically using R for visualising and analysing population structure of Singapore..\nThe hands-on exercise consists of 4 steps:\n\nInstall and launch tidyverse and ggtern packages.\nDerive 3 new measures using mutate() function of dplyr package.\nBuild a static ternary plot using ggtern() function of ggtern package.\nBuild an interactive ternary plot using plot-ly() function of Plotly R package.\n\n\n\n\nFor this exercise, two main R packages will be used in this hands-on exercise, they are:\n\nggtern, a ggplot extension specially designed to plot ternary diagrams. The package will be used to plot static ternary plots.\nPlotly R, an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\n\nSelected tidyverse family packages namely: readr, dplyr and tidyr also needs to be installed and loaded.\nIn this exercise, version 3.2.1 of ggplot2 will be installed instead of the latest version of ggplot2. This is because the current version of ggtern package is not compatible to the latest version of ggplot2.\nThe code chunks below will accomplish the task.\n\npacman:: p_load(plotly, ggtern, tidyverse)\n\n\n\n\n\n\nFor the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used. The data set has been downloaded and included in the data sub-folder of the hands-on exercise folder. It is called respopagsex2000to2018_tidy.csv and is in csv file format.\n\n\n\nTo import respopagsex2000to2018_tidy.csv into R, read_csv() function of readr package will be used.\n\n#Reading the data into R environment\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\n\n\nNext, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\n#Deriving the young, economy active and old measures\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)\n\n\n\n\n\n\n\nUse ggtern() function of ggtern package to create a simple ternary plot.\n\n#Building the static ternary plot\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\n\n#Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2015\") +\n  theme_rgbw()\n\n\n\n\n\n\n\nThe code below create an interactive ternary plot using plot_ly() function of Plotly R.\n\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9a.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9a.html#overview",
    "title": "Hands-on Ex9a",
    "section": "",
    "text": "Ternary plots are a way of displaying the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.) It’s display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nThe key learning objective of this hands-on exercise is to build ternary plot programmatically using R for visualising and analysing population structure of Singapore..\nThe hands-on exercise consists of 4 steps:\n\nInstall and launch tidyverse and ggtern packages.\nDerive 3 new measures using mutate() function of dplyr package.\nBuild a static ternary plot using ggtern() function of ggtern package.\nBuild an interactive ternary plot using plot-ly() function of Plotly R package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9a.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9a.html#getting-started",
    "title": "Hands-on Ex9a",
    "section": "",
    "text": "For this exercise, two main R packages will be used in this hands-on exercise, they are:\n\nggtern, a ggplot extension specially designed to plot ternary diagrams. The package will be used to plot static ternary plots.\nPlotly R, an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\n\nSelected tidyverse family packages namely: readr, dplyr and tidyr also needs to be installed and loaded.\nIn this exercise, version 3.2.1 of ggplot2 will be installed instead of the latest version of ggplot2. This is because the current version of ggtern package is not compatible to the latest version of ggplot2.\nThe code chunks below will accomplish the task.\n\npacman:: p_load(plotly, ggtern, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9a.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9a.html#data-preparation",
    "title": "Hands-on Ex9a",
    "section": "",
    "text": "For the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used. The data set has been downloaded and included in the data sub-folder of the hands-on exercise folder. It is called respopagsex2000to2018_tidy.csv and is in csv file format.\n\n\n\nTo import respopagsex2000to2018_tidy.csv into R, read_csv() function of readr package will be used.\n\n#Reading the data into R environment\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\n\n\nNext, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\n#Deriving the young, economy active and old measures\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9a.html#plotting-ternary-diagram-with-r",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9a.html#plotting-ternary-diagram-with-r",
    "title": "Hands-on Ex9a",
    "section": "",
    "text": "Use ggtern() function of ggtern package to create a simple ternary plot.\n\n#Building the static ternary plot\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\n\n#Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2015\") +\n  theme_rgbw()\n\n\n\n\n\n\n\nThe code below create an interactive ternary plot using plot_ly() function of Plotly R.\n\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9b.html",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9b.html",
    "title": "Hands-on Ex9b",
    "section": "",
    "text": "Correlation coefficient is a popular statistic that use to measure the type and strength of the relationship between two variables. The values of a correlation coefficient ranges between -1.0 and 1.0. A correlation coefficient of 1 shows a perfect linear relationship between the two variables, while a -1.0 shows a perfect inverse relationship between the two variables. A correlation coefficient of 0.0 shows no linear relationship between the two variables.\nWhen multivariate data are used, the correlation coefficeints of the pair comparisons are displayed in a table form known as correlation matrix or scatterplot matrix.\nThere are three broad reasons for computing a correlation matrix.\n\nTo reveal the relationship between high-dimensional variables pair-wisely.\nTo input into other analyses. For example, people commonly use correlation matrices as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise.\nAs a diagnostic when checking other analyses. For example, with linear regression a high amount of correlations suggests that the linear regression’s estimates will be unreliable.\n\nWhen the data is large, both in terms of the number of observations and the number of variables, Corrgram tend to be used to visually explore and analyse the structure and the patterns of relations among variables. It is designed based on two main schemes:\n\nRendering the value of a correlation to depict its sign and magnitude, and\nReordering the variables in a correlation matrix so that “similar” variables are positioned adjacently, facilitating perception.\n\nThe key learning objective is to plot data visualisation for visualising correlation matrix with R. It consists of three main sections. First, you will learn how to create correlation matrix using pairs() of R Graphics. Next, to plot corrgram using corrplot package of R. And lastly, to create an interactive correlation matrix using plotly R.\n\n\n\n\n\nThe code chunk below installs and loads corrplot, ggpubr, plotly and tidyverse packages into R environment.\n\npacman:: p_load(corrplot, ggstatsplot, tidyverse)\n\n\n\n\nIn this hands-on exercise, the Wine Quality Data Set of UCI Machine Learning Repository will be used. The data set consists of 13 variables and 6497 observations. For the purpose of this exercise, we have combined the red wine and white wine data into one data file. It is called wine_quality and is in csv file format.\nThe data is imported into R by using read_csv() of readr package.\n\nwine &lt;- read_csv(\"data/wine_quality.csv\")\n\nNotice that beside quality and type, the rest of the variables are numerical and continuous data type.\n\n\n\n\nThere are more than one way to build scatterplot matrix with R. In this section, the learning objective is to create a scatterplot matrix using the pairs function of R Graphics.\nIt is important to first read the syntax description of [pairsfunction] (https://stat.ethz.ch/R-manual/R-devel/library/graphics/html/pairs.html).\n\n\nFigure below shows the scatter plot matrix of Wine Quality Data. It is a 11 by 11 matrix.\n\npairs(wine[,1:11])\n\n\n\n\nThe required input of pairs() can be a matrix or data frame. The code chunk used to create the scatterplot matrix is relatively simple. It uses the default pairs function. Columns 2 to 12 of wine dataframe is used to build the scatterplot matrix. The variables are: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates and alcohol.\n\npairs(wine[,2:12])\n\n\n\n\n\n\n\npairs() function of R Graphics provided many customisation arguments. For example, it is a common practice to show either the upper half or lower half of the correlation matrix instead of both. This is because a correlation matrix is symmetric.\nTo show the lower half of the correlation matrix, the upper.panel argument will be used as shown in the code chunk below.\n\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\nSimilarly, the upper half of the correlation matrix can be displayed using the code chunk below.\n\npairs(wine[,2:12], lower.panel = NULL)\n\n\n\n\n\n\n\nTo show the correlation coefficient of each pair of variables instead of a scatter plot, panel.cor function will be used. This will also show higher correlations in a larger font.\n\npanel.cor &lt;- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr &lt;- par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr &lt;- abs(cor(x, y, use=\"complete.obs\"))\ntxt &lt;- format(c(r, 0.123456789), digits=digits)[1]\ntxt &lt;- paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)\n\n\n\n\n\n\n\n\nOne of the major limitation of the correlation matrix is that the scatter plots appear very cluttered when the number of observations is relatively large (i.e. more than 500 observations). To overcome this, Corrgram data visualisation technique suggested by D. J. Murdoch and E. D. Chow (1996) and Friendly, M (2002) and will be used.\nThe are at least three R packages that contain functions to plot corrgram, they are:\n\ncorrgram\nellipse\ncorrplot\n\nOn top that, some R package like ggstatsplot package also provides functions for building corrgram.\nIn this section, the learning objective is to visualise the correlation matrix by using ggcorrmat() of ggstatsplot package.\nThe basic plot On of the advantage of using ggcorrmat() over many other methods to visualise a correlation matrix is it’s ability to provide a comprehensive and yet professional statistical report as shown in the figure below.\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11)\n\n\n\n\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\"\n)\n\n\n\n\nThings to learn from the code chunk above:\n\ncor.vars argument is used to compute the correlation matrix needed to build the corrgram.\nggcorrplot.args argument provide additional (mostly aesthetic) arguments that will be passed to ggcorrplot::ggcorrplot function. The list should avoid any of the following arguments since they are already internally being used: corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, digits.\n\nThe sample sub-code chunk can be used to control specific component of the plot such as the font size of the x-axis, y-axis, and the statistical report.\n\nggplot.component = list(\n    theme(text=element_text(size=5),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))\n\n\n\n\nSince ggstasplot is an extension of ggplot2, it also supports faceting. However the feature is not available in ggcorrmat() but in the grouped_ggcorrmat() of ggstatsplot.\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)\n\n\n\n\nThings to learn from the code chunk above:\n\nto build a facet plot, the only argument needed is grouping.var.\nBehind group_ggcorrmat(), patchwork package is used to create the multiplot. plotgrid.args argument provides a list of additional arguments passed to patchwork::wrap_plots, except for guides argument which is already separately specified earlier.\nLikewise, annotation.args argument is calling plot annotation arguments of patchwork package.\n\n\n\n\nIn this section the focus will be on corrplot.\nBefore getting started, it is required to read [An Introduction to corrplot Package] (https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html) in order to gain a basic understanding of corrplot package.\n\n\nBefore plotting a corrgram using corrplot(), need to compute the correlation matrix of wine data frame.\nIn the code chunk below, cor() of R Stats is used to compute the correlation matrix of wine data frame.\n\nwine.cor &lt;- cor(wine[, 1:11])\n\nNext, corrplot() is used to plot the corrgram by using all the default setting as shown in the code chunk below.\n\ncorrplot(wine.cor)\n\n\n\n\nNotice that the default visual object used to plot the corrgram is circle. The default layout of the corrgram is a symmetric matrix. The default colour scheme is diverging blue-red. Blue colours are used to represent pair variables with positive correlation coefficients and red colours are used to represent pair variables with negative correlation coefficients. The intensity of the colour or also know as saturation is used to represent the strength of the correlation coefficient. Darker colours indicate relatively stronger linear relationship between the paired variables. On the other hand, lighter colours indicates relatively weaker linear relationship.\n\n\n\nIn corrplot package, there are seven visual geometrics (parameter method) can be used to encode the attribute values. They are: circle, square, ellipse, number, shade, color and pie. The default is circle. As shown in the previous section, the default visual geometric of corrplot matrix is circle. However, this default setting can be changed by using the method argument as shown in the code chunk below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\nFeel free to change the method argument to other supported visual geometrics.\n\n\n\ncorrplor() supports three layout types, namely: “full”, “upper” or “lower”. The default is “full” which display full matrix. The default setting can be changed by using the type argument of corrplot().\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\")\n\n\n\n\nThe default layout of the corrgram can be further customised. For example, arguments diag and tl.col are used to turn off the diagonal cells and to change the axis text label colour to black colour respectively as shown in the code chunk and figure below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\nPlease feel free to experiment with other layout design argument such as tl.pos, tl.cex, tl.offset, cl.pos, cl.cex and cl.offset, just to mention a few of them.\n\n\n\nWith corrplot package, it is possible to design corrgram with mixed visual matrix of one half and numerical matrix on the other half. In order to create a coorgram with mixed layout, the corrplot.mixed(), a wrapped function for mixed visualisation style will be used.\nFigure below shows a mixed layout corrgram plotted using wine quality data.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\nNotice that argument lower and upper are used to define the visualisation method used. In this case ellipse is used to map the lower half of the corrgram and numerical matrix (i.e. number) is used to map the upper half of the corrgram. The argument tl.pos, on the other, is used to specify the placement of the axis label. Lastly, the diag argument is used to specify the glyph on the principal diagonal of the corrgram.\n\n\n\nIn statistical analysis, we are also interested to know which pair of variables their correlation coefficients are statistically significant.\nFigure below shows a corrgram combined with the significant test. The corrgram reveals that not all correlation pairs are statistically significant. For example the correlation between total sulfur dioxide and free surfur dioxide is statistically significant at significant level of 0.1 but not the pair between total sulfur dioxide and citric acid.\nWith corrplot package, we can use the cor.mtest() to compute the p-values and confidence interval for each pair of variables.\n\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\nWe can then use the p.mat argument of corrplot function as shown in the code chunk below.\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and pattern in a corrgram. By default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e. “original”). The default setting can be over-write by using the order argument of corrplot(). Currently, corrplot package support four sorting methods, they are:\n\n“AOE” is for the angular order of the eigenvectors. See Michael Friendly (2002) for details.\n“FPC” for the first principal component order.\n“hclust” for hierarchical clustering order, and “hclust.method” for the agglomeration method to be used.\n“hclust.method” should be one of “ward”, “single”, “complete”, “average”, “mcquitty”, “median” or “centroid”.\n“alphabet” for alphabetical order.\n\n“AOE”, “FPC”, “hclust”, “alphabet”. More algorithms can be found in seriation package.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\nIf using hclust, corrplot() can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9b.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9b.html#overview",
    "title": "Hands-on Ex9b",
    "section": "",
    "text": "Correlation coefficient is a popular statistic that use to measure the type and strength of the relationship between two variables. The values of a correlation coefficient ranges between -1.0 and 1.0. A correlation coefficient of 1 shows a perfect linear relationship between the two variables, while a -1.0 shows a perfect inverse relationship between the two variables. A correlation coefficient of 0.0 shows no linear relationship between the two variables.\nWhen multivariate data are used, the correlation coefficeints of the pair comparisons are displayed in a table form known as correlation matrix or scatterplot matrix.\nThere are three broad reasons for computing a correlation matrix.\n\nTo reveal the relationship between high-dimensional variables pair-wisely.\nTo input into other analyses. For example, people commonly use correlation matrices as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise.\nAs a diagnostic when checking other analyses. For example, with linear regression a high amount of correlations suggests that the linear regression’s estimates will be unreliable.\n\nWhen the data is large, both in terms of the number of observations and the number of variables, Corrgram tend to be used to visually explore and analyse the structure and the patterns of relations among variables. It is designed based on two main schemes:\n\nRendering the value of a correlation to depict its sign and magnitude, and\nReordering the variables in a correlation matrix so that “similar” variables are positioned adjacently, facilitating perception.\n\nThe key learning objective is to plot data visualisation for visualising correlation matrix with R. It consists of three main sections. First, you will learn how to create correlation matrix using pairs() of R Graphics. Next, to plot corrgram using corrplot package of R. And lastly, to create an interactive correlation matrix using plotly R."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9b.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9b.html#getting-started",
    "title": "Hands-on Ex9b",
    "section": "",
    "text": "The code chunk below installs and loads corrplot, ggpubr, plotly and tidyverse packages into R environment.\n\npacman:: p_load(corrplot, ggstatsplot, tidyverse)\n\n\n\n\nIn this hands-on exercise, the Wine Quality Data Set of UCI Machine Learning Repository will be used. The data set consists of 13 variables and 6497 observations. For the purpose of this exercise, we have combined the red wine and white wine data into one data file. It is called wine_quality and is in csv file format.\nThe data is imported into R by using read_csv() of readr package.\n\nwine &lt;- read_csv(\"data/wine_quality.csv\")\n\nNotice that beside quality and type, the rest of the variables are numerical and continuous data type."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9b.html#building-correlation-matrix-pairs-method",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9b.html#building-correlation-matrix-pairs-method",
    "title": "Hands-on Ex9b",
    "section": "",
    "text": "There are more than one way to build scatterplot matrix with R. In this section, the learning objective is to create a scatterplot matrix using the pairs function of R Graphics.\nIt is important to first read the syntax description of [pairsfunction] (https://stat.ethz.ch/R-manual/R-devel/library/graphics/html/pairs.html).\n\n\nFigure below shows the scatter plot matrix of Wine Quality Data. It is a 11 by 11 matrix.\n\npairs(wine[,1:11])\n\n\n\n\nThe required input of pairs() can be a matrix or data frame. The code chunk used to create the scatterplot matrix is relatively simple. It uses the default pairs function. Columns 2 to 12 of wine dataframe is used to build the scatterplot matrix. The variables are: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates and alcohol.\n\npairs(wine[,2:12])\n\n\n\n\n\n\n\npairs() function of R Graphics provided many customisation arguments. For example, it is a common practice to show either the upper half or lower half of the correlation matrix instead of both. This is because a correlation matrix is symmetric.\nTo show the lower half of the correlation matrix, the upper.panel argument will be used as shown in the code chunk below.\n\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\nSimilarly, the upper half of the correlation matrix can be displayed using the code chunk below.\n\npairs(wine[,2:12], lower.panel = NULL)\n\n\n\n\n\n\n\nTo show the correlation coefficient of each pair of variables instead of a scatter plot, panel.cor function will be used. This will also show higher correlations in a larger font.\n\npanel.cor &lt;- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr &lt;- par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr &lt;- abs(cor(x, y, use=\"complete.obs\"))\ntxt &lt;- format(c(r, 0.123456789), digits=digits)[1]\ntxt &lt;- paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9b.html#visualising-correlation-matrix-ggcormat",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9b.html#visualising-correlation-matrix-ggcormat",
    "title": "Hands-on Ex9b",
    "section": "",
    "text": "One of the major limitation of the correlation matrix is that the scatter plots appear very cluttered when the number of observations is relatively large (i.e. more than 500 observations). To overcome this, Corrgram data visualisation technique suggested by D. J. Murdoch and E. D. Chow (1996) and Friendly, M (2002) and will be used.\nThe are at least three R packages that contain functions to plot corrgram, they are:\n\ncorrgram\nellipse\ncorrplot\n\nOn top that, some R package like ggstatsplot package also provides functions for building corrgram.\nIn this section, the learning objective is to visualise the correlation matrix by using ggcorrmat() of ggstatsplot package.\nThe basic plot On of the advantage of using ggcorrmat() over many other methods to visualise a correlation matrix is it’s ability to provide a comprehensive and yet professional statistical report as shown in the figure below.\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11)\n\n\n\n\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\"\n)\n\n\n\n\nThings to learn from the code chunk above:\n\ncor.vars argument is used to compute the correlation matrix needed to build the corrgram.\nggcorrplot.args argument provide additional (mostly aesthetic) arguments that will be passed to ggcorrplot::ggcorrplot function. The list should avoid any of the following arguments since they are already internally being used: corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, digits.\n\nThe sample sub-code chunk can be used to control specific component of the plot such as the font size of the x-axis, y-axis, and the statistical report.\n\nggplot.component = list(\n    theme(text=element_text(size=5),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9b.html#building-multiple-plots",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9b.html#building-multiple-plots",
    "title": "Hands-on Ex9b",
    "section": "",
    "text": "Since ggstasplot is an extension of ggplot2, it also supports faceting. However the feature is not available in ggcorrmat() but in the grouped_ggcorrmat() of ggstatsplot.\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)\n\n\n\n\nThings to learn from the code chunk above:\n\nto build a facet plot, the only argument needed is grouping.var.\nBehind group_ggcorrmat(), patchwork package is used to create the multiplot. plotgrid.args argument provides a list of additional arguments passed to patchwork::wrap_plots, except for guides argument which is already separately specified earlier.\nLikewise, annotation.args argument is calling plot annotation arguments of patchwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9b.html#visualising-correlation-matrix-using-corrplot-package",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9b.html#visualising-correlation-matrix-using-corrplot-package",
    "title": "Hands-on Ex9b",
    "section": "",
    "text": "In this section the focus will be on corrplot.\nBefore getting started, it is required to read [An Introduction to corrplot Package] (https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html) in order to gain a basic understanding of corrplot package.\n\n\nBefore plotting a corrgram using corrplot(), need to compute the correlation matrix of wine data frame.\nIn the code chunk below, cor() of R Stats is used to compute the correlation matrix of wine data frame.\n\nwine.cor &lt;- cor(wine[, 1:11])\n\nNext, corrplot() is used to plot the corrgram by using all the default setting as shown in the code chunk below.\n\ncorrplot(wine.cor)\n\n\n\n\nNotice that the default visual object used to plot the corrgram is circle. The default layout of the corrgram is a symmetric matrix. The default colour scheme is diverging blue-red. Blue colours are used to represent pair variables with positive correlation coefficients and red colours are used to represent pair variables with negative correlation coefficients. The intensity of the colour or also know as saturation is used to represent the strength of the correlation coefficient. Darker colours indicate relatively stronger linear relationship between the paired variables. On the other hand, lighter colours indicates relatively weaker linear relationship.\n\n\n\nIn corrplot package, there are seven visual geometrics (parameter method) can be used to encode the attribute values. They are: circle, square, ellipse, number, shade, color and pie. The default is circle. As shown in the previous section, the default visual geometric of corrplot matrix is circle. However, this default setting can be changed by using the method argument as shown in the code chunk below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\nFeel free to change the method argument to other supported visual geometrics.\n\n\n\ncorrplor() supports three layout types, namely: “full”, “upper” or “lower”. The default is “full” which display full matrix. The default setting can be changed by using the type argument of corrplot().\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\")\n\n\n\n\nThe default layout of the corrgram can be further customised. For example, arguments diag and tl.col are used to turn off the diagonal cells and to change the axis text label colour to black colour respectively as shown in the code chunk and figure below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\nPlease feel free to experiment with other layout design argument such as tl.pos, tl.cex, tl.offset, cl.pos, cl.cex and cl.offset, just to mention a few of them.\n\n\n\nWith corrplot package, it is possible to design corrgram with mixed visual matrix of one half and numerical matrix on the other half. In order to create a coorgram with mixed layout, the corrplot.mixed(), a wrapped function for mixed visualisation style will be used.\nFigure below shows a mixed layout corrgram plotted using wine quality data.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\nNotice that argument lower and upper are used to define the visualisation method used. In this case ellipse is used to map the lower half of the corrgram and numerical matrix (i.e. number) is used to map the upper half of the corrgram. The argument tl.pos, on the other, is used to specify the placement of the axis label. Lastly, the diag argument is used to specify the glyph on the principal diagonal of the corrgram.\n\n\n\nIn statistical analysis, we are also interested to know which pair of variables their correlation coefficients are statistically significant.\nFigure below shows a corrgram combined with the significant test. The corrgram reveals that not all correlation pairs are statistically significant. For example the correlation between total sulfur dioxide and free surfur dioxide is statistically significant at significant level of 0.1 but not the pair between total sulfur dioxide and citric acid.\nWith corrplot package, we can use the cor.mtest() to compute the p-values and confidence interval for each pair of variables.\n\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\nWe can then use the p.mat argument of corrplot function as shown in the code chunk below.\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and pattern in a corrgram. By default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e. “original”). The default setting can be over-write by using the order argument of corrplot(). Currently, corrplot package support four sorting methods, they are:\n\n“AOE” is for the angular order of the eigenvectors. See Michael Friendly (2002) for details.\n“FPC” for the first principal component order.\n“hclust” for hierarchical clustering order, and “hclust.method” for the agglomeration method to be used.\n“hclust.method” should be one of “ward”, “single”, “complete”, “average”, “mcquitty”, “median” or “centroid”.\n“alphabet” for alphabetical order.\n\n“AOE”, “FPC”, “hclust”, “alphabet”. More algorithms can be found in seriation package.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\nIf using hclust, corrplot() can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9c.html",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9c.html",
    "title": "Hands-on Ex9c",
    "section": "",
    "text": "Heatmaps visualise data through variations in colouring. When applied to a tabular format, heatmaps are useful for cross-examining multivariate data, through placing variables in the columns and observation (or records) in rowa and colouring the cells within the table. Heatmaps are good for showing variance across multiple variables, revealing any patterns, displaying whether any variables are similar to each other, and for detecting if any correlations exist in-between them.\nIn this hands-on exercise, the key learning objective is to plot static and interactive heatmap for visualising and analysing multivariate data.\n\n\n\n\n\nThe code chunk below will install and launch seriation, heatmaply, dendextend and tidyverse in RStudio.\n\npacman::p_load(seriation, dendextend, heatmaply, tidyverse)\n\n\n\n\nIn this hands-on exercise, the data of World Happines 2018 report will be used. The data set is downloaded from here. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\nThe output tibbled data frame is called wh.\n\n\n\nNext, we need to change the rows by country name instead of row number by using the code chunk below\n\nrow.names(wh) &lt;- wh$Country\n\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform wh data frame into a data matrix.\n\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)\n\nNotice that wh_matrix is in R matrix format.\n\n\n\n\nThere are many R packages and functions can be used to drawing static heatmaps, they are:\n\nheatmap()of R stats package. It draws a simple heatmap.\nheatmap.2() of gplots R package. It draws an enhanced heatmap compared to the R base function.\npheatmap() of pheatmap R package. pheatmap package also known as Pretty Heatmap. The package provides functions to draws pretty heatmaps and provides more control to change the appearance of heatmaps.\nComplexHeatmap package of R/Bioconductor package. The package draws, annotates and arranges complex heatmaps (very useful for genomic data analysis). The full reference guide of the package is available here.\nsuperheat package: A Graphical Tool for Exploring Complex Datasets Using Heatmaps. A system for generating extendable and customizable heatmaps for exploring complex datasets, including big data and data with multiple data types. The full reference guide of the package is available here.\n\nIn this section, you will learn how to plot static heatmaps by using heatmap() of R Stats package.\n\n\nIn this sub-section, we will plot a heatmap by using heatmap() of Base Stats. The code chunk is given below.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\nNote:\n\nBy default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\n\nTo plot a cluster heatmap, we just have to use the default as shown in the code chunk below.\n\nwh_heatmap &lt;- heatmap(wh_matrix)\n\n\n\n\nNote:\n\nThe order of both rows and columns is different compare to the native wh_matrix. This is because heatmap do a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\n\nHere, red cells denotes small values, and red small ones. This heatmap is not really informative. Indeed, the Happiness Score variable have relatively higher values, what makes that the other variables with small values all look the same. Thus, we need to normalize this matrix. This is done using the scale argument. It can be applied to rows or to columns following your needs.\nThe code chunk below normalises the matrix column-wise.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\nNotice that the values are scaled now. Also note that margins argument is used to ensure that the entire x-axis labels are displayed completely and, cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively.\n\n\n\n\nheatmaply is an R package for building interactive cluster heatmap that can be shared online as a stand-alone HTML file. It is designed and maintained by Tal Galili.\nBefore we get started, you should review the Introduction to Heatmaply to have an overall understanding of the features and functions of Heatmaply package. You are also required to have the user manual of the package handy with you for reference purposes.\nIn this section, you will gain hands-on experience on using heatmaply to design an interactive cluster heatmap. We will still use the wh_matrix as the input data.\n\n\n\nheatmaply(mtcars)\n\n\n\n\n\nThe code chunk below shows the basic syntax needed to create n interactive heatmap by using heatmaply package.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\nNote that:\n\nDifferent from heatmap(), for heatmaply() the default horizontal dendrogram is placed on the left hand side of the heatmap.\nThe text label of each raw, on the other hand, is placed on the right hand side of the heat map.\nWhen the x-axis marker labels are too long, they will be rotated by 135 degree from the north.\n\n\n\n\nWhen analysing multivariate data set, it is very common that the variables in the data sets includes values that reflect different types of measurement. In general, these variables’ values have their own range. In order to ensure that all the variables have comparable values, data transformation are commonly used before clustering.\nThree main data transformation methods are supported by heatmaply(), namely: scale, normalise and percentilse.\n\n\n\nWhen all variables are came from or assumed to come from some normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution.\nIn such a case, each value would reflect the distance from the mean in units of standard deviation.\nThe scale argument in heatmaply() supports column and row scaling.\n\nThe code chunk below is used to scale variable values columewise.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\n\n\n\nWhen variables in the data comes from possibly different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations.\nThis preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”.\n\nDifferent from Scaling, the normalise method is performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\nThis is similar to ranking the variables, but instead of keeping the rank values, divide them by the maximal rank.\nThis is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile.\nThe benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it.\n\nSimilar to Normalize method, the Percentize method is also performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\nheatmaply supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\ndistfun: function used to compute the distance (dissimilarity) between both rows and columns. Defaults to dist. The options “pearson”, “spearman” and “kendall” can be used to use correlation-based clustering, which uses as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method).\nhclustfun: function used to compute the hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust.\ndist_method default is NULL, which results in “euclidean” to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is “dist”” hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\nhclust_method default is NULL, which results in “complete” method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\n\n\nIn the code chunk below, the heatmap is plotted by using hierachical clustering algorithm with “Euclidean distance” and “ward.D” method.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\n\nIn order to determine the best clustering method and number of cluster the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\nwh_d &lt;- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\nwh_clust &lt;- hclust(wh_d, method = \"average\")\nnum_k &lt;- find_k(wh_clust)\nplot(num_k)\n\n\n\n\nFigure above shows that k=3 would be good.\nWith reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\n\n\n\nOne of the problems with hierarchical clustering is that it doesn’t actually place the rows in a definite order, it merely constrains the space of possible orderings. Take three items A, B and C. If you ignore reflections, there are three possible orderings: ABC, ACB, BAC. If clustering them gives you ((A+B)+C) as a tree, you know that C can’t end up between A and B, but it doesn’t tell you which way to flip the A+B cluster. It doesn’t tell you if the ABC ordering will lead to a clearer-looking heatmap than the BAC ordering.\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimize the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized. This is related to a restricted version of the travelling salesman problem.\nHere we meet our first seriation algorithm: Optimal Leaf Ordering (OLO). This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimize the sum of dissimilarities between adjacent leaves. Here is the result of applying Optimal Leaf Ordering to the same clustering result as the heatmap above.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\nThe default options is “OLO” (Optimal leaf ordering) which optimizes the above criterion (in O(n^4)). Another option is “GW” (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\n\nThe option “mean” gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\nThe option “none” gives us the dendrograms without any rotation that is based on the data matrix.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\n\nThe default colour palette uses by heatmaply is viridis. heatmaply users, however, can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap.\nIn the code chunk below, the Blues colour palette of rColorBrewer is used\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\n\n\nBeside providing a wide collection of arguments for meeting the statistical analysis needs, heatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the code chunk below the following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsize_row and fontsize_col are used to change the font size for row and column labels to 4.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively.\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9c.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9c.html#overview",
    "title": "Hands-on Ex9c",
    "section": "",
    "text": "Heatmaps visualise data through variations in colouring. When applied to a tabular format, heatmaps are useful for cross-examining multivariate data, through placing variables in the columns and observation (or records) in rowa and colouring the cells within the table. Heatmaps are good for showing variance across multiple variables, revealing any patterns, displaying whether any variables are similar to each other, and for detecting if any correlations exist in-between them.\nIn this hands-on exercise, the key learning objective is to plot static and interactive heatmap for visualising and analysing multivariate data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9c.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9c.html#getting-started",
    "title": "Hands-on Ex9c",
    "section": "",
    "text": "The code chunk below will install and launch seriation, heatmaply, dendextend and tidyverse in RStudio.\n\npacman::p_load(seriation, dendextend, heatmaply, tidyverse)\n\n\n\n\nIn this hands-on exercise, the data of World Happines 2018 report will be used. The data set is downloaded from here. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\nThe output tibbled data frame is called wh.\n\n\n\nNext, we need to change the rows by country name instead of row number by using the code chunk below\n\nrow.names(wh) &lt;- wh$Country\n\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform wh data frame into a data matrix.\n\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)\n\nNotice that wh_matrix is in R matrix format."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9c.html#static-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9c.html#static-heatmap",
    "title": "Hands-on Ex9c",
    "section": "",
    "text": "There are many R packages and functions can be used to drawing static heatmaps, they are:\n\nheatmap()of R stats package. It draws a simple heatmap.\nheatmap.2() of gplots R package. It draws an enhanced heatmap compared to the R base function.\npheatmap() of pheatmap R package. pheatmap package also known as Pretty Heatmap. The package provides functions to draws pretty heatmaps and provides more control to change the appearance of heatmaps.\nComplexHeatmap package of R/Bioconductor package. The package draws, annotates and arranges complex heatmaps (very useful for genomic data analysis). The full reference guide of the package is available here.\nsuperheat package: A Graphical Tool for Exploring Complex Datasets Using Heatmaps. A system for generating extendable and customizable heatmaps for exploring complex datasets, including big data and data with multiple data types. The full reference guide of the package is available here.\n\nIn this section, you will learn how to plot static heatmaps by using heatmap() of R Stats package.\n\n\nIn this sub-section, we will plot a heatmap by using heatmap() of Base Stats. The code chunk is given below.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\nNote:\n\nBy default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\n\nTo plot a cluster heatmap, we just have to use the default as shown in the code chunk below.\n\nwh_heatmap &lt;- heatmap(wh_matrix)\n\n\n\n\nNote:\n\nThe order of both rows and columns is different compare to the native wh_matrix. This is because heatmap do a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\n\nHere, red cells denotes small values, and red small ones. This heatmap is not really informative. Indeed, the Happiness Score variable have relatively higher values, what makes that the other variables with small values all look the same. Thus, we need to normalize this matrix. This is done using the scale argument. It can be applied to rows or to columns following your needs.\nThe code chunk below normalises the matrix column-wise.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\nNotice that the values are scaled now. Also note that margins argument is used to ensure that the entire x-axis labels are displayed completely and, cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9c.html#creating-interactive-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9c.html#creating-interactive-heatmap",
    "title": "Hands-on Ex9c",
    "section": "",
    "text": "heatmaply is an R package for building interactive cluster heatmap that can be shared online as a stand-alone HTML file. It is designed and maintained by Tal Galili.\nBefore we get started, you should review the Introduction to Heatmaply to have an overall understanding of the features and functions of Heatmaply package. You are also required to have the user manual of the package handy with you for reference purposes.\nIn this section, you will gain hands-on experience on using heatmaply to design an interactive cluster heatmap. We will still use the wh_matrix as the input data.\n\n\n\nheatmaply(mtcars)\n\n\n\n\n\nThe code chunk below shows the basic syntax needed to create n interactive heatmap by using heatmaply package.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\nNote that:\n\nDifferent from heatmap(), for heatmaply() the default horizontal dendrogram is placed on the left hand side of the heatmap.\nThe text label of each raw, on the other hand, is placed on the right hand side of the heat map.\nWhen the x-axis marker labels are too long, they will be rotated by 135 degree from the north.\n\n\n\n\nWhen analysing multivariate data set, it is very common that the variables in the data sets includes values that reflect different types of measurement. In general, these variables’ values have their own range. In order to ensure that all the variables have comparable values, data transformation are commonly used before clustering.\nThree main data transformation methods are supported by heatmaply(), namely: scale, normalise and percentilse.\n\n\n\nWhen all variables are came from or assumed to come from some normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution.\nIn such a case, each value would reflect the distance from the mean in units of standard deviation.\nThe scale argument in heatmaply() supports column and row scaling.\n\nThe code chunk below is used to scale variable values columewise.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\n\n\n\nWhen variables in the data comes from possibly different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations.\nThis preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”.\n\nDifferent from Scaling, the normalise method is performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\nThis is similar to ranking the variables, but instead of keeping the rank values, divide them by the maximal rank.\nThis is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile.\nThe benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it.\n\nSimilar to Normalize method, the Percentize method is also performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\nheatmaply supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\ndistfun: function used to compute the distance (dissimilarity) between both rows and columns. Defaults to dist. The options “pearson”, “spearman” and “kendall” can be used to use correlation-based clustering, which uses as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method).\nhclustfun: function used to compute the hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust.\ndist_method default is NULL, which results in “euclidean” to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is “dist”” hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\nhclust_method default is NULL, which results in “complete” method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\n\n\nIn the code chunk below, the heatmap is plotted by using hierachical clustering algorithm with “Euclidean distance” and “ward.D” method.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\n\nIn order to determine the best clustering method and number of cluster the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\nwh_d &lt;- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\nwh_clust &lt;- hclust(wh_d, method = \"average\")\nnum_k &lt;- find_k(wh_clust)\nplot(num_k)\n\n\n\n\nFigure above shows that k=3 would be good.\nWith reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9c.html#seriation",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9c.html#seriation",
    "title": "Hands-on Ex9c",
    "section": "",
    "text": "One of the problems with hierarchical clustering is that it doesn’t actually place the rows in a definite order, it merely constrains the space of possible orderings. Take three items A, B and C. If you ignore reflections, there are three possible orderings: ABC, ACB, BAC. If clustering them gives you ((A+B)+C) as a tree, you know that C can’t end up between A and B, but it doesn’t tell you which way to flip the A+B cluster. It doesn’t tell you if the ABC ordering will lead to a clearer-looking heatmap than the BAC ordering.\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimize the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized. This is related to a restricted version of the travelling salesman problem.\nHere we meet our first seriation algorithm: Optimal Leaf Ordering (OLO). This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimize the sum of dissimilarities between adjacent leaves. Here is the result of applying Optimal Leaf Ordering to the same clustering result as the heatmap above.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\nThe default options is “OLO” (Optimal leaf ordering) which optimizes the above criterion (in O(n^4)). Another option is “GW” (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\n\nThe option “mean” gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\nThe option “none” gives us the dendrograms without any rotation that is based on the data matrix.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\n\nThe default colour palette uses by heatmaply is viridis. heatmaply users, however, can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap.\nIn the code chunk below, the Blues colour palette of rColorBrewer is used\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\n\n\nBeside providing a wide collection of arguments for meeting the statistical analysis needs, heatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the code chunk below the following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsize_row and fontsize_col are used to change the font size for row and column labels to 4.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively.\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9d.html",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9d.html",
    "title": "Hands-on Ex9d",
    "section": "",
    "text": "Parallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data. It is ideal for comparing multiple variables together and seeing the relationships between them. For example, the variables contribute to Happiness Index. Parallel coordinates was invented by Alfred Inselberg in the 1970s as a way to visualize high-dimensional data. This data visualisation technique is more often found in academic and scientific communities than in business and consumer data visualizations. As pointed out by Stephen Few(2006), “This certainly isn’t a chart that you would present to the board of directors or place on your Web site for the general public. In fact, the strength of parallel coordinates isn’t in their ability to communicate some truth in the data to others, but rather in their ability to bring meaningful multivariate patterns and comparisons to light when used interactively for analysis.” For example, parallel coordinates plot can be used to characterise clusters detected during customer segmentation.\nThe key learning objective of this exercise are:\n\nplotting statistic parallel coordinates plots by using ggparcoord() of GGally package,\nplotting interactive parallel coordinates plots by using parcoords package, and\nplotting interactive parallel coordinates plots by using parallelPlot package.\n\n\n\n\n\n\nFor this exercise, the GGally, parcoords, parallelPlot and tidyverse packages will be used.\nThe code chunks below are used to install and load the packages in R.\n\npacman::p_load(GGally, parallelPlot, tidyverse)\n\n\n\n\nIn this hands-on exercise, the data of World Happines 2018 report will be used. The data set is downloaded from here. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\n\n\n\n\nIn this section, you will learn how to plot static parallel coordinates plot by using ggparcoord() of GGally package. Before getting started, it is a good practice to read the function description in detail.\n\n\nCode chunk below shows a typical syntax used to plot a basic static parallel coordinates plot by using ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12))\n\n\n\n\nNotice that only two argument namely data and columns is used. Data argument is used to map the data object (i.e. wh) and columns is used to select the columns for preparing the parallel coordinates plot.\n\n\n\nThe basic parallel coordinates failed to reveal any meaning understanding of the World Happiness measures. In this section, you will learn how to makeover the plot by using a collection of arguments provided by ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\nThings to learn from the code chunk above.\n\ngroupColumn argument is used to group the observations (i.e. parallel lines) by using a single variable (i.e. Region) and colour the parallel coordinates lines by region name.\nscale argument is used to scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\nalphaLines argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nboxplot argument is used to turn on the boxplot by using logical TRUE. The default is FALSE.\ntitle argument is used to provide the parallel coordinates plot a title.\n\n\n\n\nSince ggparcoord() is developed by extending ggplot2 package, we can combination use some of the ggplot2 function when plotting a parallel coordinates plot.\nIn the code chunk below, facet_wrap() of ggplot2 is used to plot 10 small multiple parallel coordinates plots. Each plot represent one geographical region such as East Asia.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\nOne of the aesthetic defect of the current design is that some of the variable names overlap on x-axis.\n\n\n\nTo make the x-axis text label easy to read, let us rotate the labels by 30 degrees. We can rotate axis text labels using theme() function in ggplot2 as shown in the code chunk below\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\nThings to learn from the code chunk above:\n\nTo rotate x-axis text labels, we use axis.text.x as argument to theme() function. And we specify element_text(angle = 30) to rotate the x-axis text by an angle 30 degree.\n\n\n\n\nRotating x-axis text labels to 30 degrees makes the label overlap with the plot and we can avoid this by adjusting the text location using hjust argument to theme’s text element with element_text(). We use axis.text.x as we want to change the look of x-axis text.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))\n\n\n\n\n\n\n\n\nparallelPlot is an R package specially designed to plot a parallel coordinates plot by using ‘htmlwidgets’ package and d3.js. In this section, you will learn how to use functions provided in parallelPlot package to build interactive parallel coordinates plot.\n\n\nThe code chunk below plot an interactive parallel coordinates plot by using parallelPlot().\n\nwh &lt;- wh %&gt;%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\nNotice that some of the axis labels are too long. You will learn how to overcome this problem in the next step.\n\n\n\nIn the code chunk below, rotateTitle argument is used to avoid overlapping axis labels.\n\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\n\nOne of the useful interactive feature of parallelPlot is we can click on a variable of interest, for example Happiness score, the monotonous blue colour (default) will change a blues with different intensity colour scheme will be used.\n\n\n\nWe can change the default blue colour scheme by using continousCS argument as shown in the code chunk below.\n\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n\nIn the code chunk below, histoVisibility argument is used to plot histogram along the axis of each variables.\n\nhistoVisibility &lt;- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9d.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9d.html#overview",
    "title": "Hands-on Ex9d",
    "section": "",
    "text": "Parallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data. It is ideal for comparing multiple variables together and seeing the relationships between them. For example, the variables contribute to Happiness Index. Parallel coordinates was invented by Alfred Inselberg in the 1970s as a way to visualize high-dimensional data. This data visualisation technique is more often found in academic and scientific communities than in business and consumer data visualizations. As pointed out by Stephen Few(2006), “This certainly isn’t a chart that you would present to the board of directors or place on your Web site for the general public. In fact, the strength of parallel coordinates isn’t in their ability to communicate some truth in the data to others, but rather in their ability to bring meaningful multivariate patterns and comparisons to light when used interactively for analysis.” For example, parallel coordinates plot can be used to characterise clusters detected during customer segmentation.\nThe key learning objective of this exercise are:\n\nplotting statistic parallel coordinates plots by using ggparcoord() of GGally package,\nplotting interactive parallel coordinates plots by using parcoords package, and\nplotting interactive parallel coordinates plots by using parallelPlot package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9d.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9d.html#getting-started",
    "title": "Hands-on Ex9d",
    "section": "",
    "text": "For this exercise, the GGally, parcoords, parallelPlot and tidyverse packages will be used.\nThe code chunks below are used to install and load the packages in R.\n\npacman::p_load(GGally, parallelPlot, tidyverse)\n\n\n\n\nIn this hands-on exercise, the data of World Happines 2018 report will be used. The data set is downloaded from here. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9d.html#plotting-static-parallel-coordinates-plot",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9d.html#plotting-static-parallel-coordinates-plot",
    "title": "Hands-on Ex9d",
    "section": "",
    "text": "In this section, you will learn how to plot static parallel coordinates plot by using ggparcoord() of GGally package. Before getting started, it is a good practice to read the function description in detail.\n\n\nCode chunk below shows a typical syntax used to plot a basic static parallel coordinates plot by using ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12))\n\n\n\n\nNotice that only two argument namely data and columns is used. Data argument is used to map the data object (i.e. wh) and columns is used to select the columns for preparing the parallel coordinates plot.\n\n\n\nThe basic parallel coordinates failed to reveal any meaning understanding of the World Happiness measures. In this section, you will learn how to makeover the plot by using a collection of arguments provided by ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\nThings to learn from the code chunk above.\n\ngroupColumn argument is used to group the observations (i.e. parallel lines) by using a single variable (i.e. Region) and colour the parallel coordinates lines by region name.\nscale argument is used to scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\nalphaLines argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nboxplot argument is used to turn on the boxplot by using logical TRUE. The default is FALSE.\ntitle argument is used to provide the parallel coordinates plot a title.\n\n\n\n\nSince ggparcoord() is developed by extending ggplot2 package, we can combination use some of the ggplot2 function when plotting a parallel coordinates plot.\nIn the code chunk below, facet_wrap() of ggplot2 is used to plot 10 small multiple parallel coordinates plots. Each plot represent one geographical region such as East Asia.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\nOne of the aesthetic defect of the current design is that some of the variable names overlap on x-axis.\n\n\n\nTo make the x-axis text label easy to read, let us rotate the labels by 30 degrees. We can rotate axis text labels using theme() function in ggplot2 as shown in the code chunk below\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\nThings to learn from the code chunk above:\n\nTo rotate x-axis text labels, we use axis.text.x as argument to theme() function. And we specify element_text(angle = 30) to rotate the x-axis text by an angle 30 degree.\n\n\n\n\nRotating x-axis text labels to 30 degrees makes the label overlap with the plot and we can avoid this by adjusting the text location using hjust argument to theme’s text element with element_text(). We use axis.text.x as we want to change the look of x-axis text.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9d.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9d.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "title": "Hands-on Ex9d",
    "section": "",
    "text": "parallelPlot is an R package specially designed to plot a parallel coordinates plot by using ‘htmlwidgets’ package and d3.js. In this section, you will learn how to use functions provided in parallelPlot package to build interactive parallel coordinates plot.\n\n\nThe code chunk below plot an interactive parallel coordinates plot by using parallelPlot().\n\nwh &lt;- wh %&gt;%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\nNotice that some of the axis labels are too long. You will learn how to overcome this problem in the next step.\n\n\n\nIn the code chunk below, rotateTitle argument is used to avoid overlapping axis labels.\n\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\n\nOne of the useful interactive feature of parallelPlot is we can click on a variable of interest, for example Happiness score, the monotonous blue colour (default) will change a blues with different intensity colour scheme will be used.\n\n\n\nWe can change the default blue colour scheme by using continousCS argument as shown in the code chunk below.\n\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n\nIn the code chunk below, histoVisibility argument is used to plot histogram along the axis of each variables.\n\nhistoVisibility &lt;- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9e.html",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9e.html",
    "title": "Hands-on Ex9e",
    "section": "",
    "text": "In this hands-on exercise, the key learning objective is designing treemap using appropriate R packages. The hands-on exercise consists of three main section. 1st sub-objective is to manipulate transaction data into a treemap strcuture by using selected functions provided in dplyr package. Then, 2nd sub-objective is to plot static treemap by using treemap package. 3rd sub-objective is to design interactive treemap by using d3treeR package.\n\n\n\n\n\nThe code chunk below will install and launch treemap and tidyverse in RStudio.\n\npacman::p_load(treemap, treemapify, tidyverse)\n\n\n\n\nIn this exercise, REALIS2018.csv data will be used. This dataset provides information of private property transaction records in 2018. The dataset is extracted from REALIS portal of Urban Redevelopment Authority (URA).\nIn the code chunk below, read_csv() of readr is used to import realis2018.csv into R and parsed it into tibble R data.frame format.\n\nrealis2018 &lt;- read_csv(\"data/realis2018.csv\")\n\nThe output tibble data.frame is called realis2018.\n\n\n\nThe data.frame realis2018 is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap. In this section, we will perform the following steps to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\n\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and\ncompute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No. of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively.\n\nTwo key verbs of dplyr package, namely: group_by() and summarize() will be used to perform these steps.\ngroup_by() breaks down a data.frame into specified groups of rows. When you then apply the verbs above on the resulting object they’ll be automatically applied “by group”.\nGrouping affects the verbs as follows:\n\ngrouped select() is the same as ungrouped select(), except that grouping variables are always retained.\ngrouped arrange() is the same as ungrouped; unless you set .by_group = TRUE, in which case it orders first by the grouping variables.\nmutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x). They are described in detail in vignette(“window-functions”).\nsample_n() and sample_frac() sample the specified number/fraction of rows in each group.\nsummarise() computes the summary for each group.\n\nIn our case, group_by() will used together with summarise() to derive the summarised data.frame.\n::: tab-set “Recommendation” Students who are new to dplyr methods should consult Introduction to dplyr before moving on to the next section. :::\n\n\n\nThe code chank below shows a typical two lines code approach to perform the steps.\n\nrealis2018_grouped &lt;- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\nrealis2018_summarised &lt;- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n::: tab-set “Note” Aggregation functions such as sum() and meadian() obey the usual rule of missing values: if there’s any missing value in the input, the output will be a missing value. The argument na.rm = TRUE removes the missing values prior to computation. :::\nThe code chunk above is not very efficient because we have to give each intermediate data.frame a name, even though we don’t have to care about it.\n\n\nThe code chunk below shows a more efficient way to tackle the same processes by using the pipe, %&gt;%:\n\nTo learn more about pipe, visit this excellent article: Pipes in R Tutorial For Beginners.\n\n\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\n\n\n\n\ntreemap package is a R package specially designed to offer great flexibility in drawing treemaps. The core function, namely: treemap() offers at least 43 arguments. In this section, we will only explore the major arguments for designing elegent and yet truthful treemaps.\n\n\nIn this section, treemap() of Treemap package is used to plot a treemap showing the distribution of median unit prices and total unit sold of resale condominium by geographic hierarchy in 2017.\nFirst, select records belonging to resale condominium property type from realis2018_selected data frame.\n\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\n\n\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely: index, vSize and vColor.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\nThings to learn from the three arguments used:\n\nindex\nThe index vector must consist of at least two column names or else no hierarchy treemap will be plotted.\nIf multiple column names are provided, such as the code chunk above, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on.\nvSize\nThe column must not contain negative values. This is because it’s vaues will be used to map the sizes of the rectangles of the treemaps.\n\nWarning:\nThe treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectagles should be in different intensity showing, in our case, median unit prices.\nFor treemap(), vColor is used in combination with the argument type to determines the colours of the rectangles. Without defining type, like the code chunk above, treemap() assumes type = index, in our case, the hierarchy of planning areas.\n\n\n\nIn the code chunk below, type argument is define as value.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\nThinking to learn from the conde chunk above.\n\nThe rectangles are coloured with different intensity of green, reflecting their respective median unit prices.\nThe legend reveals that the values are binned into ten bins, i.e. 0-5000, 5000-10000, etc. with an equal interval of 5000.\n\n\n\n\nThere are two arguments that determine the mapping to color palettes: mapping and palette. The only difference between “value” and “manual” is the default value for mapping. The “value” treemap considers palette to be a diverging color palette (say ColorBrewer’s “RdYlBu”), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color. The “manual” treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color.\n\n\n\nThe code chunk below shows a value type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\nThing to learn from the code chunk above:\n\nalthough the colour palette used is RdYlBu but there are no red rectangles in the treemap above. This is because all the median unit prices are positive.\nThe reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\n\n\n\nThe “manual” type does not interpret the values as the “value” type does. Instead, the value range is mapped linearly to the colour palette.\nThe code chunk below shows a manual type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\nThings to learn from the code chunk above:\n\nThe colour scheme used is very copnfusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative To overcome this problem, a single colour palette such as Blues should be used.\n\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\ntreemap() supports two popular treemap layouts, namely: “squarified” and “pivotSize”. The default is “pivotSize”.\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID). The ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable.\n\n\n\nThe code chunk below plots a squarified treemap by changing the algorithm argument.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\nWhen “pivotSize” algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\ntreemapify is a R package specially developed to draw treemaps in ggplot2. In this section, you will learn how to designing treemps closely resemble treemaps designing in previous section by using treemapify. Before you getting started, you should read Introduction to “treemapify” its user guide.\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n\n\n\nGroup by Planning Region\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\nGroup by Planning Area\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\nAdding boundary line\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")\n\n\n\n\n\n\n\n\n\n\nThis slide shows you how to install a R package which is not available in cran.\n\nIf this is the first time you install a package from github, you should install devtools package by using the code below or else you can skip this step.\n\ninstall.packages(\"devtools\")\n\nNext, you will load the devtools library and install the package found in github by using the codes below.\n\nlibrary(devtools)\ninstall_github(\"timelyportfolio/d3treeR\")\n\nNow you are ready to launch d3treeR package\n\n\nlibrary(d3treeR)\n\n\n\n\nThe codes below perform two processes.\n\ntreemap() is used to build a treemap by using selected variables in condominium data.frame. The treemap created is save as object called tm.\n\n\ntm &lt;- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThen d3tree() is used to build an interactive treemap.\n\n\nd3tree(tm,rootname = \"Singapore\" )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9e.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9e.html#overview",
    "title": "Hands-on Ex9e",
    "section": "",
    "text": "In this hands-on exercise, the key learning objective is designing treemap using appropriate R packages. The hands-on exercise consists of three main section. 1st sub-objective is to manipulate transaction data into a treemap strcuture by using selected functions provided in dplyr package. Then, 2nd sub-objective is to plot static treemap by using treemap package. 3rd sub-objective is to design interactive treemap by using d3treeR package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9e.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9e.html#getting-started",
    "title": "Hands-on Ex9e",
    "section": "",
    "text": "The code chunk below will install and launch treemap and tidyverse in RStudio.\n\npacman::p_load(treemap, treemapify, tidyverse)\n\n\n\n\nIn this exercise, REALIS2018.csv data will be used. This dataset provides information of private property transaction records in 2018. The dataset is extracted from REALIS portal of Urban Redevelopment Authority (URA).\nIn the code chunk below, read_csv() of readr is used to import realis2018.csv into R and parsed it into tibble R data.frame format.\n\nrealis2018 &lt;- read_csv(\"data/realis2018.csv\")\n\nThe output tibble data.frame is called realis2018.\n\n\n\nThe data.frame realis2018 is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap. In this section, we will perform the following steps to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\n\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and\ncompute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No. of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively.\n\nTwo key verbs of dplyr package, namely: group_by() and summarize() will be used to perform these steps.\ngroup_by() breaks down a data.frame into specified groups of rows. When you then apply the verbs above on the resulting object they’ll be automatically applied “by group”.\nGrouping affects the verbs as follows:\n\ngrouped select() is the same as ungrouped select(), except that grouping variables are always retained.\ngrouped arrange() is the same as ungrouped; unless you set .by_group = TRUE, in which case it orders first by the grouping variables.\nmutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x). They are described in detail in vignette(“window-functions”).\nsample_n() and sample_frac() sample the specified number/fraction of rows in each group.\nsummarise() computes the summary for each group.\n\nIn our case, group_by() will used together with summarise() to derive the summarised data.frame.\n::: tab-set “Recommendation” Students who are new to dplyr methods should consult Introduction to dplyr before moving on to the next section. :::\n\n\n\nThe code chank below shows a typical two lines code approach to perform the steps.\n\nrealis2018_grouped &lt;- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\nrealis2018_summarised &lt;- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n::: tab-set “Note” Aggregation functions such as sum() and meadian() obey the usual rule of missing values: if there’s any missing value in the input, the output will be a missing value. The argument na.rm = TRUE removes the missing values prior to computation. :::\nThe code chunk above is not very efficient because we have to give each intermediate data.frame a name, even though we don’t have to care about it.\n\n\nThe code chunk below shows a more efficient way to tackle the same processes by using the pipe, %&gt;%:\n\nTo learn more about pipe, visit this excellent article: Pipes in R Tutorial For Beginners.\n\n\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9e.html#designing-treemap-with-treemap-package",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9e.html#designing-treemap-with-treemap-package",
    "title": "Hands-on Ex9e",
    "section": "",
    "text": "treemap package is a R package specially designed to offer great flexibility in drawing treemaps. The core function, namely: treemap() offers at least 43 arguments. In this section, we will only explore the major arguments for designing elegent and yet truthful treemaps.\n\n\nIn this section, treemap() of Treemap package is used to plot a treemap showing the distribution of median unit prices and total unit sold of resale condominium by geographic hierarchy in 2017.\nFirst, select records belonging to resale condominium property type from realis2018_selected data frame.\n\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\n\n\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely: index, vSize and vColor.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\nThings to learn from the three arguments used:\n\nindex\nThe index vector must consist of at least two column names or else no hierarchy treemap will be plotted.\nIf multiple column names are provided, such as the code chunk above, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on.\nvSize\nThe column must not contain negative values. This is because it’s vaues will be used to map the sizes of the rectangles of the treemaps.\n\nWarning:\nThe treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectagles should be in different intensity showing, in our case, median unit prices.\nFor treemap(), vColor is used in combination with the argument type to determines the colours of the rectangles. Without defining type, like the code chunk above, treemap() assumes type = index, in our case, the hierarchy of planning areas.\n\n\n\nIn the code chunk below, type argument is define as value.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\nThinking to learn from the conde chunk above.\n\nThe rectangles are coloured with different intensity of green, reflecting their respective median unit prices.\nThe legend reveals that the values are binned into ten bins, i.e. 0-5000, 5000-10000, etc. with an equal interval of 5000.\n\n\n\n\nThere are two arguments that determine the mapping to color palettes: mapping and palette. The only difference between “value” and “manual” is the default value for mapping. The “value” treemap considers palette to be a diverging color palette (say ColorBrewer’s “RdYlBu”), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color. The “manual” treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color.\n\n\n\nThe code chunk below shows a value type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\nThing to learn from the code chunk above:\n\nalthough the colour palette used is RdYlBu but there are no red rectangles in the treemap above. This is because all the median unit prices are positive.\nThe reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\n\n\n\nThe “manual” type does not interpret the values as the “value” type does. Instead, the value range is mapped linearly to the colour palette.\nThe code chunk below shows a manual type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\nThings to learn from the code chunk above:\n\nThe colour scheme used is very copnfusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative To overcome this problem, a single colour palette such as Blues should be used.\n\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\ntreemap() supports two popular treemap layouts, namely: “squarified” and “pivotSize”. The default is “pivotSize”.\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID). The ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable.\n\n\n\nThe code chunk below plots a squarified treemap by changing the algorithm argument.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\nWhen “pivotSize” algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9e.html#designing-treemap-using-treemapify-package",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9e.html#designing-treemap-using-treemapify-package",
    "title": "Hands-on Ex9e",
    "section": "",
    "text": "treemapify is a R package specially developed to draw treemaps in ggplot2. In this section, you will learn how to designing treemps closely resemble treemaps designing in previous section by using treemapify. Before you getting started, you should read Introduction to “treemapify” its user guide.\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n\n\n\nGroup by Planning Region\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\nGroup by Planning Area\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\nAdding boundary line\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9e.html#designing-interactive-treemap-using-d3treer",
    "href": "Hands-on_Ex/Hands-on_Ex9/Hands-on_Ex9e.html#designing-interactive-treemap-using-d3treer",
    "title": "Hands-on Ex9e",
    "section": "",
    "text": "This slide shows you how to install a R package which is not available in cran.\n\nIf this is the first time you install a package from github, you should install devtools package by using the code below or else you can skip this step.\n\ninstall.packages(\"devtools\")\n\nNext, you will load the devtools library and install the package found in github by using the codes below.\n\nlibrary(devtools)\ninstall_github(\"timelyportfolio/d3treeR\")\n\nNow you are ready to launch d3treeR package\n\n\nlibrary(d3treeR)\n\n\n\n\nThe codes below perform two processes.\n\ntreemap() is used to build a treemap by using selected variables in condominium data.frame. The treemap created is save as object called tm.\n\n\ntm &lt;- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThen d3tree() is used to build an interactive treemap.\n\n\nd3tree(tm,rootname = \"Singapore\" )"
  }
]